---
title: DQN으로 AI가 달 착륙을 배우다 - 강화학습 입문 프로젝트
date: 2025-08-05 10:44:23 +0900
categories: [machine learning, deep learning]
tags: [machine learning, deep learning, DQN]     # TAG names should always be lowercase
---

# DQN으로 AI가 달 착륙을 배우다 - 강화학습 입문 프로젝트

> "한 걸음은 작지만, 인류에게는 거대한 도약이다" - 닐 암스트롱

1969년 인류 최초의 달 착륙 이후 50여 년이 지난 지금, AI는 스스로 달 착륙 방법을 학습하고 있습니다. 이번 포스트에서는 **Deep Q-Network(DQN)**을 활용해 AI가 달 착륙선을 조종하는 방법을 익히는 프로젝트를 상세히 살펴보겠습니다.

## 🚀 왜 달 착륙선인가?

달 착륙은 강화학습을 배우기에 완벽한 문제입니다. 복잡한 물리법칙이 적용되고, 연속적인 의사결정이 필요하며, 명확한 성공/실패 기준이 있죠. 무엇보다 학습 과정을 시각적으로 확인할 수 있어 AI의 사고 과정을 직관적으로 이해할 수 있습니다.

OpenAI Gymnasium의 LunarLander-v2 환경에서는 AI가 중력, 관성, 연료 소모를 모두 고려하여 안전하고 효율적인 착륙을 학습해야 합니다. 이는 실제 우주선 제어와 매우 유사한 도전입니다.

## 🧠 DQN: 게임을 바꾼 알고리즘

### Q-Learning의 한계와 DQN의 혁신

전통적인 Q-Learning은 상태-행동 테이블을 사용해 최적 정책을 학습합니다:

```
Q(s,a) = Q(s,a) + α[r + γ max Q(s',a') - Q(s,a)]
```

하지만 달 착륙선처럼 연속적인 상태 공간을 가진 문제에서는 테이블 방식으로는 한계가 있습니다. DQN은 이 문제를 **신경망으로 Q함수를 근사**하여 해결했습니다.

### DQN의 핵심 혁신 세 가지

**1. 신경망 Q함수 근사**
```python
class DQN(nn.Module):
    def __init__(self, state_size=8, action_size=4, hidden_size=64):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(state_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, action_size)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)
```

**2. Experience Replay**
과거 경험을 메모리에 저장하고 랜덤하게 샘플링하여 학습합니다. 이는 데이터 간 상관관계를 제거하고 학습 안정성을 크게 향상시킵니다.

**3. Target Network**
별도의 타겟 네트워크를 사용해 학습 목표를 안정화합니다. 이는 "움직이는 목표" 문제를 해결하여 수렴성을 보장합니다.

## 🎯 달 착륙선 환경 분석

### 상태 공간 (8차원)
- **위치**: x, y 좌표
- **속도**: x, y 방향 속도
- **자세**: 각도, 각속도  
- **접촉**: 왼쪽/오른쪽 다리 지면 접촉 여부

### 행동 공간 (4가지)
- `0`: 아무것도 하지 않음
- `1`: 왼쪽 조향 엔진 점화
- `2`: 메인 엔진 점화  
- `3`: 오른쪽 조향 엔진 점화

### 보상 체계
- **성공적 착륙**: +100~140점
- **다리 지면 접촉**: 각각 +10점
- **엔진 사용**: 프레임당 -0.3점
- **추락**: -100점

**목표**: 100회 연속 에피소드 평균 점수 200점 이상 달성

## 💻 핵심 구현 살펴보기

### 1. DQN 에이전트 클래스

```python
class DQNAgent:
    def __init__(self, state_size=8, action_size=4, lr=5e-4):
        self.state_size = state_size
        self.action_size = action_size
        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE)
        
        # 메인 네트워크와 타겟 네트워크
        self.qnetwork_local = DQN(state_size, action_size).to(device)
        self.qnetwork_target = DQN(state_size, action_size).to(device)
        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=lr)
        
        # 탐험-활용 균형을 위한 epsilon
        self.epsilon = EPS_START
```

### 2. Experience Replay 메커니즘

```python
class ReplayBuffer:
    def __init__(self, action_size, buffer_size, batch_size):
        self.action_size = action_size
        self.memory = deque(maxlen=buffer_size)
        self.batch_size = batch_size
        self.experience = namedtuple("Experience", 
                                   field_names=["state", "action", "reward", 
                                              "next_state", "done"])
    
    def add(self, state, action, reward, next_state, done):
        """경험을 메모리에 추가"""
        e = self.experience(state, action, reward, next_state, done)
        self.memory.append(e)
    
    def sample(self):
        """랜덤하게 배치 샘플링"""
        experiences = random.sample(self.memory, k=self.batch_size)
        # 텐서로 변환하여 반환
        states = torch.from_numpy(np.vstack([e.state for e in experiences])).float()
        actions = torch.from_numpy(np.vstack([e.action for e in experiences])).long()
        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences])).float()
        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences])).float()
        dones = torch.from_numpy(np.vstack([e.done for e in experiences]).astype(np.uint8)).float()
        
        return (states, actions, rewards, next_states, dones)
```

### 3. 학습 핵심 로직

```python
def learn(self, experiences, gamma):
    """경험으로부터 Q-네트워크 학습"""
    states, actions, rewards, next_states, dones = experiences
    
    # 현재 Q값 계산
    Q_expected = self.qnetwork_local(states).gather(1, actions)
    
    # 타겟 Q값 계산 (타겟 네트워크 사용)
    Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)
    Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))
    
    # 손실 계산 및 역전파
    loss = F.mse_loss(Q_expected, Q_targets)
    self.optimizer.zero_grad()
    loss.backward()
    self.optimizer.step()
    
    # 타겟 네트워크 소프트 업데이트
    self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)
```

## 📈 학습 여정: 추락에서 완벽한 착륙까지

### 학습 단계별 분석

**🔴 초기 단계 (에피소드 1-500): 혼돈의 시기**
- ε = 1.0 (100% 랜덤 행동)
- 대부분 추락하거나 착륙 패드를 벗어남
- 평균 점수: -200 ~ -50점
- **의미**: 환경의 물리 법칙과 행동 결과 학습

**🟡 중간 단계 (에피소드 500-1500): 깨달음의 순간**
- ε = 0.5 → 0.1 (점진적 감소)
- 가끔 성공적인 착륙 달성
- 평균 점수: -50 ~ 150점
- **의미**: 기본적인 제어 방법 습득

**🟢 최종 단계 (에피소드 1500-2000): 마스터리 달성**
- ε = 0.05 (거의 학습된 정책 사용)
- 안정적이고 연료 효율적인 착륙
- 평균 점수: 200점 이상
- **의미**: 최적 정책 수렴

### 성능 지표
- **수렴 시점**: 약 1,500 에피소드
- **학습 시간**: 30분~2시간 (GPU 사용 시)
- **최종 성능**: 연속 100회 평균 220점
- **성공률**: 95% 이상

## 🛠️ 직접 실행해보기

### 환경 설정 (원클릭 실행)

```bash
# 저장소 클론
git clone https://github.com/BanHun28/dqn_study
cd dqn_study

# 자동 설정 및 실행
chmod +x setup_and_run.sh
./setup_and_run.sh
```

### 수동 실행
```bash
# 가상환경 생성 및 활성화
python3 -m venv dqn_lunar_env
source dqn_lunar_env/bin/activate

# 의존성 설치
pip install torch numpy gymnasium[box2d] imageio

# 학습 시작
python3 deep_q_learning_for_lunar_landing_complete_code.py
```

### 하이퍼파라미터 튜닝 가이드

```python
# 핵심 하이퍼파라미터
LEARNING_RATE = 5e-4      # 학습률: 너무 크면 불안정, 너무 작으면 느림
BATCH_SIZE = 64           # 배치 크기: GPU 메모리와 학습 안정성 균형
BUFFER_SIZE = int(1e5)    # 메모리 크기: 클수록 다양한 경험 저장
GAMMA = 0.99              # 할인 인수: 미래 보상의 중요도
TAU = 1e-3                # 타겟 네트워크 업데이트 비율
UPDATE_EVERY = 4          # 학습 주기: 경험 수집과 학습의 균형

# ε-greedy 파라미터
EPS_START = 1.0           # 초기 탐험 확률
EPS_END = 0.01            # 최종 탐험 확률  
EPS_DECAY = 0.995         # 탐험 확률 감소율
```

## 🎓 핵심 인사이트와 배운 점

### 1. Experience Replay의 마법
가장 인상적인 발견은 Experience Replay의 효과였습니다. 동일한 네트워크 구조에서도 Experience Replay를 사용하지 않으면 학습이 매우 불안정했습니다. 과거 경험을 랜덤하게 재학습함으로써:

- **상관관계 제거**: 연속된 경험 간 상관관계 해소
- **데이터 효율성**: 한 번의 경험을 여러 번 재활용
- **학습 안정성**: 급격한 정책 변화 방지

### 2. 탐험-활용 균형의 예술
ε-greedy 전략에서 탐험과 활용의 균형이 성능에 미치는 영향을 직접 확인했습니다:

- **초기 충분한 탐험**: 다양한 상황 경험으로 robust한 정책 학습
- **점진적 활용 증가**: 학습된 지식을 바탕으로 성능 향상
- **최소 탐험 유지**: 예상치 못한 상황에 대한 적응성 보존

### 3. Target Network의 필요성
Target Network 없이 학습하면 "움직이는 목표" 문제로 인해 학습이 발산했습니다. 안정적인 타겟 제공이 수렴에 얼마나 중요한지 실감했습니다.

## 🔮 발전 방향과 확장 가능성

### 알고리즘 개선 방향

**1. Double DQN (DDQN)**
```python
# Double DQN의 핵심: 행동 선택과 가치 평가 분리
next_actions = self.qnetwork_local(next_states).detach().argmax(1).unsqueeze(1)
Q_targets_next = self.qnetwork_target(next_states).gather(1, next_actions)
```
- **문제 해결**: Q값 과대추정 편향 완화
- **성능 향상**: 더 정확한 가치 추정

**2. Dueling DQN**
```python
class DuelingDQN(nn.Module):
    def __init__(self, state_size, action_size, hidden_size=64):
        super(DuelingDQN, self).__init__()
        self.feature = nn.Sequential(
            nn.Linear(state_size, hidden_size),
            nn.ReLU()
        )
        self.advantage = nn.Linear(hidden_size, action_size)
        self.value = nn.Linear(hidden_size, 1)
    
    def forward(self, x):
        features = self.feature(x)
        advantage = self.advantage(features)
        value = self.value(features)
        return value + advantage - advantage.mean(dim=1, keepdim=True)
```
- **핵심 아이디어**: 상태 가치와 행동 우위 분리
- **효과**: 상태 가치 학습 효율성 향상

**3. Prioritized Experience Replay**
- **개념**: 중요한 경험에 높은 우선순위 부여
- **구현**: TD 오차 기반 우선순위 계산
- **효과**: 학습 효율성 대폭 향상

### 실무 적용 영역

**자율주행 시스템**
- 복잡한 교통 상황에서의 의사결정
- 안전성과 효율성의 균형
- 연속적인 제어 최적화

**로봇 제어**
- 정밀한 조작 작업 학습
- 환경 변화에 대한 적응
- 다중 목표 최적화

**금융 알고리즘 트레이딩**
- 시장 상황에 따른 포트폴리오 조정
- 위험 관리와 수익 최대화
- 실시간 의사결정

## 💡 프로젝트의 교육적 가치

이 프로젝트는 다음과 같은 이유로 강화학습 입문에 최적입니다:

### 1. 직관적 이해
달 착륙이라는 구체적이고 시각적인 작업을 통해 추상적인 강화학습 개념을 직관적으로 이해할 수 있습니다.

### 2. 완전한 구현
이론부터 구현, 실행, 결과 분석까지 전체 파이프라인을 경험할 수 있습니다.

### 3. 상세한 한국어 주석
코드의 모든 라인에 한국어 주석이 달려 있어 초심자도 쉽게 따라할 수 있습니다.

### 4. 확장 가능성
기본 DQN을 이해한 후 다양한 개선 기법을 적용해볼 수 있는 견고한 기반을 제공합니다.

## 🎬 마무리: AI의 무한한 가능성

50년 전 인간이 달에 첫 발을 내딛었을 때, 그 누구도 AI가 스스로 달 착륙을 학습할 수 있을 것이라고 상상하지 못했을 것입니다. 이 프로젝트를 통해 우리는 AI가 복잡한 물리 환경에서 인간의 개입 없이 최적의 전략을 스스로 발견할 수 있음을 확인했습니다.

DQN은 단순히 게임을 플레이하는 AI를 넘어, 실제 세계의 복잡한 제어 문제를 해결할 수 있는 강력한 도구입니다. 자율주행차, 로봇 제어, 자원 관리 등 다양한 분야에서 DQN과 그 발전된 형태들이 실제로 사용되고 있습니다.

### 다음 단계 제안

1. **코드 직접 실행**: GitHub에서 프로젝트를 클론하여 직접 실행해보세요
2. **하이퍼파라미터 실험**: 다양한 설정으로 학습 성능 비교
3. **알고리즘 확장**: Double DQN, Dueling DQN 구현
4. **다른 환경 적용**: CartPole, Breakout 등 다른 강화학습 환경에 적용

강화학습의 세계는 무궁무진합니다. 이 달 착륙선 프로젝트가 여러분의 AI 여정에 작은 첫걸음이 되기를 바랍니다. 

---

### 📚 참고 자료

- **프로젝트 저장소**: [BanHun28/dqn_study](https://github.com/BanHun28/dqn_study)
- **원논문**: "Human-level control through deep reinforcement learning" (Nature, 2015)
- **OpenAI Gymnasium**: [LunarLander-v2 환경 문서](https://gymnasium.farama.org/environments/box2d/lunar_lander/)
- **PyTorch 강화학습 튜토리얼**: [공식 문서](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)

*"The important thing is not to stop questioning." - Albert Einstein*

AI와 함께하는 무한한 탐험은 계속됩니다. 🚀

