---
title: LLaMA Factory íŒŒì¸íŠœë‹ ì˜µì…˜ ê°€ì´ë“œ
date: 2025-08-14 10:29:57 +0900
categories: [artificial intelligence, machine learning]
tags: [machine learning, deep learning, llm, finetuning, nlp, llamafactory, pytorch, transformers, huggingface, lora, qlora, tutorial, guide, ai]     # TAG names should always be lowercase
---

# LLaMA Factory íŒŒì¸íŠœë‹ ì˜µì…˜ ê°€ì´ë“œ

## ğŸ“‹ ëª©ì°¨
1. [ê¸°ë³¸ ì„¤ì • ì˜µì…˜](#-ê¸°ë³¸-ì„¤ì •-ì˜µì…˜)
2. [ëª¨ë¸ ê´€ë ¨ ì˜µì…˜](#-ëª¨ë¸-ê´€ë ¨-ì˜µì…˜)
3. [ë°ì´í„°ì…‹ ê´€ë ¨ ì˜µì…˜](#-ë°ì´í„°ì…‹-ê´€ë ¨-ì˜µì…˜)
4. [í›ˆë ¨ ë°©ë²• ì˜µì…˜](#-í›ˆë ¨-ë°©ë²•-ì˜µì…˜)
5. [í•˜ì´í¼íŒŒë¼ë¯¸í„° ì˜µì…˜](#ï¸-í•˜ì´í¼íŒŒë¼ë¯¸í„°-ì˜µì…˜)
6. [ì–‘ìí™” ì˜µì…˜](#-ì–‘ìí™”-ì˜µì…˜)
7. [LoRA ê´€ë ¨ ì˜µì…˜](#-lora-ê´€ë ¨-ì˜µì…˜)
8. [ë¶„ì‚° í›ˆë ¨ ì˜µì…˜](#-ë¶„ì‚°-í›ˆë ¨-ì˜µì…˜)
9. [ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§ ì˜µì…˜](#-ë¡œê¹…-ë°-ëª¨ë‹ˆí„°ë§-ì˜µì…˜)
10. [ìµœì í™” ê´€ë ¨ ì˜µì…˜](#-ìµœì í™”-ê´€ë ¨-ì˜µì…˜)

---

## ğŸ”§ ê¸°ë³¸ ì„¤ì • ì˜µì…˜

### stage (í›ˆë ¨ ë‹¨ê³„)
**ì„¤ëª…**: ì‹¤í–‰í•  í›ˆë ¨ ë‹¨ê³„ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.
```yaml
stage: sft  # í•„ìˆ˜ ì˜µì…˜
```

**ê°€ëŠ¥í•œ ê°’**:
- `pt`: (Continued) Pre-training - ì‚¬ì „í›ˆë ¨ ë˜ëŠ” ì§€ì†ì  ì‚¬ì „í›ˆë ¨
- `sft`: Supervised Fine-Tuning - ì§€ë„ íŒŒì¸íŠœë‹
- `rm`: Reward Modeling - ë³´ìƒ ëª¨ë¸ í›ˆë ¨ 
- `ppo`: PPO Training - ê°•í™”í•™ìŠµ PPO í›ˆë ¨
- `dpo`: DPO Training - Direct Preference Optimization
- `kto`: KTO Training - Knowledge Transfer from Oracle
- `orpo`: ORPO Training - Odds Ratio Preference Optimization

**ì‹¤ë¬´ ê¶Œì¥ì‚¬í•­**:
- ëŒ€ë¶€ë¶„ì˜ ì»¤ìŠ¤í…€ ì‘ì—…: `sft`
- ê°•í™”í•™ìŠµ ê¸°ë°˜ ì •ë ¬: `dpo` â†’ `ppo` ìˆœì„œë¡œ ì§„í–‰

### do_train
**ì„¤ëª…**: í›ˆë ¨ ëª¨ë“œ í™œì„±í™” ì—¬ë¶€
```yaml
do_train: true  # ê¸°ë³¸ê°’: false
```

### do_eval
**ì„¤ëª…**: í‰ê°€ ëª¨ë“œ í™œì„±í™” ì—¬ë¶€
```yaml
do_eval: true  # ê¸°ë³¸ê°’: false
```

### do_predict
**ì„¤ëª…**: ì˜ˆì¸¡/ì¶”ë¡  ëª¨ë“œ í™œì„±í™” ì—¬ë¶€
```yaml
do_predict: true  # ê¸°ë³¸ê°’: false
```

---

## ğŸ¤– ëª¨ë¸ ê´€ë ¨ ì˜µì…˜

### model_name_or_path
**ì„¤ëª…**: ì‚¬ìš©í•  ëª¨ë¸ì˜ ê²½ë¡œ ë˜ëŠ” Hugging Face ëª¨ë¸ ì´ë¦„
```yaml
model_name_or_path: meta-llama/Meta-Llama-3-8B-Instruct
# ë˜ëŠ” ë¡œì»¬ ê²½ë¡œ
model_name_or_path: /path/to/local/model
```

**ì§€ì› ëª¨ë¸ ì˜ˆì‹œ**:
- `meta-llama/Meta-Llama-3-8B-Instruct`
- `Qwen/Qwen2.5-7B-Instruct` 
- `microsoft/Phi-3-mini-4k-instruct`
- `google/gemma-2-9b-it`

### model_revision
**ì„¤ëª…**: íŠ¹ì • ëª¨ë¸ ë²„ì „/ë¦¬ë¹„ì „ ì§€ì •
```yaml
model_revision: main  # ê¸°ë³¸ê°’: main
```

### quantization_bit
**ì„¤ëª…**: ì–‘ìí™” ë¹„íŠ¸ ìˆ˜ ì„¤ì •
```yaml
quantization_bit: 4  # 4, 8, ë˜ëŠ” None
```

### template
**ì„¤ëª…**: ëª¨ë¸ì— ë§ëŠ” ëŒ€í™” í…œí”Œë¦¿ ì§€ì •
```yaml
template: llama3  # ëª¨ë¸ì— ë”°ë¼ í•„ìˆ˜
```

**ì£¼ìš” í…œí”Œë¦¿**:
- `llama3`: Llama 3 ëª¨ë¸ìš©
- `qwen`: Qwen ëª¨ë¸ìš©
- `chatglm3`: ChatGLM ëª¨ë¸ìš©
- `phi`: Phi ëª¨ë¸ìš©
- `gemma`: Gemma ëª¨ë¸ìš©

---

## ğŸ“Š ë°ì´í„°ì…‹ ê´€ë ¨ ì˜µì…˜

### dataset
**ì„¤ëª…**: ì‚¬ìš©í•  ë°ì´í„°ì…‹ ì´ë¦„(ë“¤)
```yaml
dataset: alpaca_en_demo  # ë‹¨ì¼ ë°ì´í„°ì…‹
# ë˜ëŠ”
dataset: alpaca_en_demo,alpaca_zh_demo  # ë‹¤ì¤‘ ë°ì´í„°ì…‹
```

### dataset_dir
**ì„¤ëª…**: ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬ ê²½ë¡œ
```yaml
dataset_dir: data  # ê¸°ë³¸ê°’: data
```

### cutoff_len
**ì„¤ëª…**: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
```yaml
cutoff_len: 1024  # ê¸°ë³¸ê°’: 1024
```

**ì‹¤ë¬´ ê°€ì´ë“œ**:
- ì¼ë°˜ì ì¸ ëŒ€í™”: 1024-2048
- ê¸´ ë¬¸ì„œ ìš”ì•½: 4096-8192
- ì½”ë“œ ìƒì„±: 2048-4096

### max_samples
**ì„¤ëª…**: ì‚¬ìš©í•  ìµœëŒ€ ìƒ˜í”Œ ìˆ˜
```yaml
max_samples: 100000  # ì „ì²´ ë°ì´í„°ì…‹ í¬ê¸° ì œí•œ
```

### preprocessing_num_workers
**ì„¤ëª…**: ë°ì´í„° ì „ì²˜ë¦¬ ì›Œì»¤ ìˆ˜
```yaml
preprocessing_num_workers: 16  # CPU ì½”ì–´ ìˆ˜ì— ë§ê²Œ ì¡°ì •
```

### streaming
**ì„¤ëª…**: ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ í™œì„±í™” (ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹)
```yaml
streaming: false  # ê¸°ë³¸ê°’: false
```

---

## ğŸ¯ í›ˆë ¨ ë°©ë²• ì˜µì…˜

### finetuning_type
**ì„¤ëª…**: íŒŒì¸íŠœë‹ ë°©ë²• ì„ íƒ
```yaml
finetuning_type: lora  # í•„ìˆ˜ ì˜µì…˜
```

**ê°€ëŠ¥í•œ ê°’**:
- `lora`: LoRA (Low-Rank Adaptation)
- `qlora`: QLoRA (Quantized LoRA)
- `full`: Full-parameter fine-tuning
- `freeze`: Freeze tuning

**ì„ íƒ ê°€ì´ë“œ**:
```yaml
# ë©”ëª¨ë¦¬ ì œì•½ì´ ì‹¬í•œ ê²½ìš°
finetuning_type: qlora
quantization_bit: 4

# ì„±ëŠ¥ ìš°ì„ ì¸ ê²½ìš°
finetuning_type: lora

# ìµœê³  ì„±ëŠ¥ì´ í•„ìš”í•œ ê²½ìš° (ëŒ€ìš©ëŸ‰ ë©”ëª¨ë¦¬ í•„ìš”)
finetuning_type: full
```

---

## âš™ï¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì˜µì…˜

### í•™ìŠµë¥  ê´€ë ¨

#### learning_rate
**ì„¤ëª…**: ê¸°ë³¸ í•™ìŠµë¥ 
```yaml
learning_rate: 5e-5  # ê¸°ë³¸ê°’: 5e-5
```

**ê¶Œì¥ ê°’**:
- LoRA: `5e-4` ~ `1e-4`
- Full fine-tuning: `5e-6` ~ `5e-5`
- QLoRA: `1e-4` ~ `2e-4`

#### lr_scheduler_type
**ì„¤ëª…**: í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ íƒ€ì…
```yaml
lr_scheduler_type: cosine  # ê¸°ë³¸ê°’: linear
```

**ì˜µì…˜**:
- `linear`: ì„ í˜• ê°ì†Œ
- `cosine`: ì½”ì‚¬ì¸ ìŠ¤ì¼€ì¤„ë§ (ê¶Œì¥)
- `polynomial`: ë‹¤í•­ì‹ ê°ì†Œ
- `constant`: ìƒìˆ˜ ìœ ì§€

#### warmup_steps
**ì„¤ëª…**: ì›Œë°ì—… ìŠ¤í… ìˆ˜
```yaml
warmup_steps: 100  # ê¸°ë³¸ê°’: 0
```

### ë°°ì¹˜ ê´€ë ¨

#### per_device_train_batch_size
**ì„¤ëª…**: ë””ë°”ì´ìŠ¤ë‹¹ í›ˆë ¨ ë°°ì¹˜ í¬ê¸°
```yaml
per_device_train_batch_size: 8  # ê¸°ë³¸ê°’: 8
```

**ë©”ëª¨ë¦¬ë³„ ê¶Œì¥ê°’**:
- 8GB GPU: 1-2
- 16GB GPU: 2-4
- 24GB GPU: 4-8
- 40GB+ GPU: 8-16

#### gradient_accumulation_steps
**ì„¤ëª…**: ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ìŠ¤í…
```yaml
gradient_accumulation_steps: 8  # ê¸°ë³¸ê°’: 8
```

**ê³„ì‚° ê³µì‹**:
```
ì‹¤ì œ ë°°ì¹˜ í¬ê¸° = per_device_train_batch_size Ã— gradient_accumulation_steps Ã— GPU ìˆ˜
```

#### dataloader_num_workers
**ì„¤ëª…**: ë°ì´í„° ë¡œë” ì›Œì»¤ ìˆ˜
```yaml
dataloader_num_workers: 4  # ê¸°ë³¸ê°’: 4
```

### ì—í¬í¬ ë° ìŠ¤í…

#### num_train_epochs
**ì„¤ëª…**: í›ˆë ¨ ì—í¬í¬ ìˆ˜
```yaml
num_train_epochs: 3.0  # ê¸°ë³¸ê°’: 3.0
```

#### max_steps
**ì„¤ëª…**: ìµœëŒ€ í›ˆë ¨ ìŠ¤í… (ì—í¬í¬ ëŒ€ì‹  ì‚¬ìš©)
```yaml
max_steps: 1000  # -1: ë¬´ì œí•œ
```

---

## ğŸ”§ ì–‘ìí™” ì˜µì…˜

### ê¸°ë³¸ ì–‘ìí™”

#### quantization_bit
```yaml
quantization_bit: 4  # None, 4, 8
```

#### double_quantization
**ì„¤ëª…**: ì´ì¤‘ ì–‘ìí™” í™œì„±í™” (ë©”ëª¨ë¦¬ ì¶”ê°€ ì ˆì•½)
```yaml
double_quantization: true  # ê¸°ë³¸ê°’: true
```

#### quant_type
**ì„¤ëª…**: ì–‘ìí™” íƒ€ì…
```yaml
quant_type: nf4  # nf4, fp4
```

### ê³ ê¸‰ ì–‘ìí™” ë°©ë²•

#### GPTQ ì–‘ìí™”
```yaml
# GPTQ ì„¤ì • ì˜ˆì‹œ
gptq_bits: 4
gptq_group_size: 128
gptq_desc_act: false
```

#### AWQ ì–‘ìí™”
```yaml
# AWQ ì„¤ì • ì˜ˆì‹œ  
awq_bits: 4
awq_group_size: 128
```

---

## ğŸ”— LoRA ê´€ë ¨ ì˜µì…˜

### ê¸°ë³¸ LoRA ì„¤ì •

#### lora_rank
**ì„¤ëª…**: LoRA ì–´ëŒ‘í„°ì˜ ë­í¬ (ì°¨ì›)
```yaml
lora_rank: 8  # ê¸°ë³¸ê°’: 8
```

**ê°€ì´ë“œë¼ì¸**:
- ì‘ì€ ëª¨ë¸ (7B ì´í•˜): 4-8
- ì¤‘ê°„ ëª¨ë¸ (7B-30B): 8-16  
- í° ëª¨ë¸ (30B+): 16-32

#### lora_alpha
**ì„¤ëª…**: LoRA ìŠ¤ì¼€ì¼ë§ íŒŒë¼ë¯¸í„°
```yaml
lora_alpha: 16  # ì¼ë°˜ì ìœ¼ë¡œ rankì˜ 2ë°°
```

#### lora_dropout
**ì„¤ëª…**: LoRA ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
```yaml
lora_dropout: 0.1  # ê¸°ë³¸ê°’: 0.0
```

#### lora_target
**ì„¤ëª…**: LoRAë¥¼ ì ìš©í•  íƒ€ê²Ÿ ëª¨ë“ˆ
```yaml
lora_target: all  # ê¸°ë³¸ê°’: all
# ë˜ëŠ” íŠ¹ì • ëª¨ë“ˆ ì§€ì •
lora_target: q_proj,v_proj,k_proj,o_proj
```

**ëª¨ë“ˆ ì˜µì…˜**:
- `all`: ëª¨ë“  ì„ í˜• ë ˆì´ì–´
- `q_proj,v_proj`: Attentionì˜ Q, V í”„ë¡œì ì…˜
- `q_proj,v_proj,k_proj,o_proj`: ì „ì²´ Attention
- `gate_proj,up_proj,down_proj`: MLP ë ˆì´ì–´

### LoRA ê³ ê¸‰ ì˜µì…˜

#### use_rslora
**ì„¤ëª…**: RS-LoRA ì‚¬ìš© ì—¬ë¶€
```yaml
use_rslora: false  # ê¸°ë³¸ê°’: false
```

#### loraplus_lr_ratio
**ì„¤ëª…**: LoRA+ í•™ìŠµë¥  ë¹„ìœ¨
```yaml
loraplus_lr_ratio: 16.0  # LoRA+ ì‚¬ìš© ì‹œ
```

#### pissa_init
**ì„¤ëª…**: PiSSA ì´ˆê¸°í™” ì‚¬ìš©
```yaml
pissa_init: false  # ê¸°ë³¸ê°’: false  
```

---

## ğŸŒ ë¶„ì‚° í›ˆë ¨ ì˜µì…˜

### ë©€í‹° GPU ì„¤ì •

#### ddp_backend
**ì„¤ëª…**: ë¶„ì‚° ë°±ì—”ë“œ
```yaml
ddp_backend: nccl  # nccl, gloo, mpi
```

#### ddp_timeout
**ì„¤ëª…**: DDP íƒ€ì„ì•„ì›ƒ (ì´ˆ)
```yaml
ddp_timeout: 1800  # ê¸°ë³¸ê°’: 1800
```

#### fsdp
**ì„¤ëª…**: FSDP (Fully Sharded Data Parallel) ì„¤ì •
```yaml
fsdp: full_shard  # "", "full_shard", "shard_grad_op"
```

### DeepSpeed ì„¤ì •

#### deepspeed
**ì„¤ëª…**: DeepSpeed ì„¤ì • íŒŒì¼
```yaml
deepspeed: examples/deepspeed/ds_z3_config.json
```

**ZeRO ë‹¨ê³„**:
- ZeRO-1: ì˜µí‹°ë§ˆì´ì € ìƒíƒœ ë¶„í• 
- ZeRO-2: + ê·¸ë˜ë””ì–¸íŠ¸ ë¶„í•   
- ZeRO-3: + ëª¨ë¸ íŒŒë¼ë¯¸í„° ë¶„í• 

---

## ğŸ“Š ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§ ì˜µì…˜

### ê¸°ë³¸ ë¡œê¹…

#### output_dir
**ì„¤ëª…**: ì¶œë ¥ ë””ë ‰í† ë¦¬
```yaml
output_dir: saves/llama3-8b/lora/test
```

#### logging_steps
**ì„¤ëª…**: ë¡œê¹… ì£¼ê¸° (ìŠ¤í…)
```yaml
logging_steps: 10  # ê¸°ë³¸ê°’: 500
```

#### save_steps
**ì„¤ëª…**: ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì£¼ê¸°
```yaml
save_steps: 500  # ê¸°ë³¸ê°’: 500
```

#### eval_steps
**ì„¤ëª…**: í‰ê°€ ì£¼ê¸°
```yaml
eval_steps: 500  # ê¸°ë³¸ê°’: 500
```

### ì™¸ë¶€ ëª¨ë‹ˆí„°ë§ ë„êµ¬

#### report_to
**ì„¤ëª…**: ë¡œê¹… ë°±ì—”ë“œ
```yaml
report_to: none  # none, wandb, tensorboard
```

#### run_name
**ì„¤ëª…**: ì‹¤í—˜ ì´ë¦„
```yaml
run_name: llama3-sft-experiment
```

### TensorBoard
```yaml
report_to: tensorboard
logging_dir: ./logs
```

### Weights & Biases
```yaml
report_to: wandb
run_name: my_experiment
```

### SwanLab (ê¶Œì¥)
```yaml
use_swanlab: true
swanlab_project: llamafactory
swanlab_run_name: test_run
swanlab_workspace: your_workspace  
swanlab_api_key: your_api_key
```

---

## âš¡ ìµœì í™” ê´€ë ¨ ì˜µì…˜

### ì˜µí‹°ë§ˆì´ì €

#### optim
**ì„¤ëª…**: ì˜µí‹°ë§ˆì´ì € íƒ€ì…
```yaml
optim: adamw_torch  # ê¸°ë³¸ê°’: adamw_torch
```

**ì˜µì…˜**:
- `adamw_torch`: AdamW (PyTorch)
- `adamw_hf`: AdamW (HuggingFace)
- `sgd`: SGD
- `adam_mini`: Adam-mini (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )

#### adam_beta1, adam_beta2
**ì„¤ëª…**: Adam ë² íƒ€ íŒŒë¼ë¯¸í„°
```yaml
adam_beta1: 0.9   # ê¸°ë³¸ê°’: 0.9
adam_beta2: 0.999 # ê¸°ë³¸ê°’: 0.999
```

#### weight_decay
**ì„¤ëª…**: ê°€ì¤‘ì¹˜ ê°ì‡ 
```yaml
weight_decay: 0.01  # ê¸°ë³¸ê°’: 0.0
```

### ê·¸ë˜ë””ì–¸íŠ¸ ê´€ë ¨

#### max_grad_norm
**ì„¤ëª…**: ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
```yaml
max_grad_norm: 1.0  # ê¸°ë³¸ê°’: 1.0
```

#### gradient_checkpointing
**ì„¤ëª…**: ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… (ë©”ëª¨ë¦¬ ì ˆì•½)
```yaml
gradient_checkpointing: true  # ê¸°ë³¸ê°’: false
```

### ì •ë°€ë„

#### fp16
**ì„¤ëª…**: 16-bit ë¶€ë™ì†Œìˆ˜ì  ì‚¬ìš©
```yaml
fp16: true  # ê¸°ë³¸ê°’: false
```

#### bf16
**ì„¤ëª…**: Brain float 16 ì‚¬ìš© (Ampere GPU ì´ìƒ)
```yaml
bf16: false  # ê¸°ë³¸ê°’: false
```

### Flash Attention

#### flash_attn
**ì„¤ëª…**: Flash Attention ì‚¬ìš©
```yaml
flash_attn: auto  # auto, true, false
```

**ê¶Œì¥**: ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ `auto` ì‚¬ìš©

---

## ğŸ“‹ ì‹¤ë¬´ ì„¤ì • ì˜ˆì‹œ

### ğŸš€ ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘ìš© ì„¤ì •
```yaml
# ê¸°ë³¸ ì„¤ì •
stage: sft
do_train: true
model_name_or_path: meta-llama/Meta-Llama-3-8B-Instruct
template: llama3

# ë°ì´í„°ì…‹
dataset: alpaca_en_demo
cutoff_len: 1024

# í›ˆë ¨ ë°©ë²•
finetuning_type: qlora
quantization_bit: 4

# í•˜ì´í¼íŒŒë¼ë¯¸í„° (ë¹ ë¥¸ ì‹¤í—˜ìš©)
learning_rate: 1e-4
num_train_epochs: 1.0
per_device_train_batch_size: 2
gradient_accumulation_steps: 4

# LoRA ì„¤ì •
lora_rank: 8
lora_alpha: 16
lora_target: all

# ë¡œê¹…
output_dir: saves/quick-test
logging_steps: 10
save_steps: 50
```

### ğŸ¯ í”„ë¡œë•ì…˜ìš© ì„¤ì •
```yaml
# ê¸°ë³¸ ì„¤ì •
stage: sft
do_train: true
do_eval: true
model_name_or_path: meta-llama/Meta-Llama-3-8B-Instruct
template: llama3

# ë°ì´í„°ì…‹
dataset: your_production_dataset
cutoff_len: 2048
max_samples: 50000
preprocessing_num_workers: 16

# í›ˆë ¨ ë°©ë²•  
finetuning_type: lora

# í•˜ì´í¼íŒŒë¼ë¯¸í„° (ì•ˆì •ì„± ì¤‘ì‹¬)
learning_rate: 5e-5
lr_scheduler_type: cosine
warmup_steps: 100
num_train_epochs: 3.0
per_device_train_batch_size: 8
gradient_accumulation_steps: 2

# LoRA ì„¤ì •
lora_rank: 16
lora_alpha: 32
lora_dropout: 0.1
lora_target: all

# ìµœì í™”
optim: adamw_torch
weight_decay: 0.01
max_grad_norm: 1.0
gradient_checkpointing: true

# ì •ë°€ë„
fp16: true
flash_attn: auto

# ë¡œê¹… ë° ì €ì¥
output_dir: saves/production-model
logging_steps: 50
save_steps: 500
eval_steps: 500
report_to: tensorboard

# ëª¨ë‹ˆí„°ë§
use_swanlab: true
swanlab_project: production
swanlab_run_name: final-model
```

### ğŸ’° ì €ë¹„ìš© GPU í™˜ê²½ìš© ì„¤ì •
```yaml
# ê¸°ë³¸ ì„¤ì •
stage: sft  
do_train: true
model_name_or_path: microsoft/Phi-3-mini-4k-instruct
template: phi

# ë°ì´í„°ì…‹ (ì‘ê²Œ)
dataset: alpaca_en_demo
cutoff_len: 512
max_samples: 1000

# ë©”ëª¨ë¦¬ ìµœì í™”
finetuning_type: qlora
quantization_bit: 4
double_quantization: true

# ì‘ì€ ë°°ì¹˜
per_device_train_batch_size: 1
gradient_accumulation_steps: 16
gradient_checkpointing: true

# LoRA (ì‘ê²Œ)
lora_rank: 4
lora_alpha: 8
lora_target: q_proj,v_proj

# ê¸°íƒ€
fp16: true
output_dir: saves/budget-model
```

## ğŸ” íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ

### ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ
1. `quantization_bit: 4` ì‚¬ìš©
2. `gradient_checkpointing: true` í™œì„±í™”
3. `per_device_train_batch_size` ì¤„ì´ê¸°
4. `gradient_accumulation_steps` ëŠ˜ë¦¬ê¸°
5. `lora_rank` ì¤„ì´ê¸°

### í•™ìŠµì´ ë¶ˆì•ˆì •í•  ë•Œ
1. `learning_rate` ì¤„ì´ê¸°
2. `warmup_steps` ëŠ˜ë¦¬ê¸°
3. `lr_scheduler_type: cosine` ì‚¬ìš©
4. `weight_decay` ì¶”ê°€
5. `lora_dropout` ì¶”ê°€

### ì†ë„ê°€ ëŠë¦´ ë•Œ
1. `flash_attn: auto` ì‚¬ìš©
2. `preprocessing_num_workers` ëŠ˜ë¦¬ê¸°
3. `dataloader_num_workers` ëŠ˜ë¦¬ê¸°
4. ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ì‚¬ìš©

ì´ ê°€ì´ë“œë¥¼ í†µí•´ LLaMA Factoryì˜ ëª¨ë“  ì£¼ìš” ì˜µì…˜ì„ ì´í•´í•˜ê³ , ìƒí™©ì— ë§ëŠ” ìµœì ì˜ ì„¤ì •ì„ êµ¬ì„±í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

