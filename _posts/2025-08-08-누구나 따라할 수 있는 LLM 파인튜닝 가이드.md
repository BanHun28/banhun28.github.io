---
title: 누구나 따라할 수 있는 LLM 파인튜닝 가이드
date: 2025-08-08 08:30:49 +0900
categories: [artificial intelligence, machine learning]
tags: [machine learning, deep learning, llm, finetuning, nlp, pytorch, transformers, huggingface, apple, silicon, mps, gpt, bert]
---

# 🚀 누구나 따라할 수 있는 LLM 파인튜닝 가이드
## 애플 실리콘 맥북으로 나만의 AI 모델 만들기

> "복잡해 보이는 AI 모델 학습, 사실은 생각보다 간단합니다!"

**📦 프로젝트 GitHub 주소:** https://github.com/BanHun28/finetune_study

안녕하세요! 오늘은 여러분과 함께 **대규모 언어모델(LLM) 파인튜닝**에 도전해보려고 합니다. 

"파인튜닝이 뭔데? 나는 개발자도 아닌데 할 수 있을까?" 하는 분들도 걱정하지 마세요. 이 가이드는 **완전 초보자**도 따라할 수 있도록 모든 과정을 단계별로 설명해드릴게요.

---

## 🎯 이 글에서 배울 수 있는 것들

- LLM 파인튜닝이 무엇인지 쉽게 이해하기
- 애플 실리콘 맥북에서 GPU 가속으로 모델 학습하기
- NVIDIA GPU가 있는 Windows/Linux에서도 실행하기
- 의료 데이터로 특화된 AI 모델 만들어보기
- 메모리 부족 문제 해결하는 방법들

---

## 🤔 LLM 파인튜닝이란?

imagine 여러분이 ChatGPT 같은 AI를 가지고 있는데, 이 AI가 **의료 분야**에 대해서는 잘 모른다고 해봅시다. 파인튜닝은 이 AI에게 **의료 전문 지식을 추가로 가르치는 과정**입니다.

### 🏠 집 짓기로 비유하면
- **사전 훈련된 모델**: 기본 골조가 완성된 집
- **파인튜닝**: 내 취향에 맞게 인테리어 하기
- **결과**: 나만의 특별한 집 완성!

### 💡 왜 파인튜닝을 해야 할까요?
1. **특정 도메인 전문성** 향상 (의료, 법률, 금융 등)
2. **회사 내부 데이터**로 맞춤형 AI 구축
3. **비용 절약** (처음부터 학습하는 것보다 훨씬 저렴)
4. **빠른 결과** (몇 시간이면 완성!)

---

## 🛠️ 시작하기 전 준비사항

### 💻 하드웨어 요구사항
| 구분         | 최소사양  | 권장사양                     | 내 상황은?     |
| ------------ | --------- | ---------------------------- | -------------- |
| **메모리**   | 8GB       | 16GB+                        | ✅ 체크해보세요 |
| **저장공간** | 10GB      | 20GB+                        | ✅ 체크해보세요 |
| **GPU**      | 없어도 OK | 애플 실리콘 맥북 또는 NVIDIA | ✅ 체크해보세요 |

### 🔍 내 맥북이 뭔지 확인하기
```bash
# 터미널에서 실행해보세요
system_profiler SPHardwareDataType | grep "Chip"
```

**결과 예시:**
- `Apple M1` → 애플 실리콘 맥북 ✅
- `Apple M2` → 애플 실리콘 맥북 ✅  
- `Apple M3` → 애플 실리콘 맥북 ✅
- `Intel` → 인텔 맥 (NVIDIA GPU 섹션 참고)

---

## 📥 1단계: 프로젝트 다운로드

### GitHub에서 프로젝트 가져오기
```bash
# 1. 터미널 열기 (맥: CMD+Space → "터미널" 검색)
# 2. 홈 디렉토리로 이동
cd ~

# 3. 프로젝트 복제
git clone https://github.com/BanHun28/finetune_study.git

# 4. 프로젝트 폴더로 이동
cd finetune_study

# 5. 파일들이 잘 다운로드 되었는지 확인
ls -la
```

**🎉 성공했다면 이런 파일들이 보일 거예요:**
```
📦 finetune_study/
├── 🐍 fine_tuning_llms_apple_silicon_optimized.py
├── 🐍 fine_tuning_llms_with_hugging_face_partial_code.py
├── ⚙️ requirements.txt
├── 🚀 run.sh
├── 🚀 setup_environment.sh
└── 📚 README.md
```

---

## ⚡ 2단계: 원클릭 실행 (가장 쉬운 방법!)

### 마법의 명령어 한 줄
```bash
# 실행 권한 부여
chmod +x *.sh

# 🎯 이것만 실행하면 끝!
./run.sh
```

### 🤖 자동으로 일어나는 일들
1. **시스템 감지**: "어? 애플 실리콘 맥북이네!" 또는 "NVIDIA GPU구나!"
2. **환경 설정**: Python 가상환경 자동 생성
3. **패키지 설치**: 필요한 라이브러리들 자동 설치
4. **최적화 적용**: 하드웨어에 맞는 최적 설정 적용
5. **학습 시작**: 의료 데이터셋으로 모델 파인튜닝 시작!

---

## 📊 3단계: 학습 과정 모니터링

### 실시간으로 진행상황 확인하기
```bash
# 새 터미널 창을 열고 실행
tail -f training_log_*.log
```

**이런 로그들이 실시간으로 보일 거예요:**
```
🚀 Starting LLM Fine-tuning...
📱 Detected: Apple Silicon MacBook
🧠 Using MPS backend for acceleration
📚 Loading medical dataset...
⚡ Training step 1/100 - Loss: 2.341
⚡ Training step 2/100 - Loss: 2.298
💾 Saving checkpoint...
```

### 💻 시스템 상태 모니터링
```bash
# 맥에서 시스템 모니터 열기
open -a "Activity Monitor"

# 또는 터미널에서 확인
top -o cpu
```

---

## 🔧 4단계: 설정 커스터마이징

### 📝 간단한 설정 변경하기

메모리가 부족하거나 더 빠르게 실행하고 싶다면:

```python
# fine_tuning_llms_apple_silicon_optimized.py 파일을 열어서
# 이 값들을 수정해보세요

# 💾 메모리 절약 설정
per_device_train_batch_size = 1  # 기본값: 4
max_seq_length = 256            # 기본값: 512

# ⚡ 빠른 테스트용 설정  
max_steps = 50                  # 기본값: 100

# 🎯 LoRA 설정 (고급자용)
r = 16                         # 기본값: 32 (작을수록 빠름)
lora_alpha = 16               # 기본값: 16
```

### 🆘 메모리 부족 시 대처법
```bash
# 방법 1: 더 작은 배치 크기로 실행
export BATCH_SIZE=1
./run.sh

# 방법 2: 환경변수로 설정 조정
export MAX_STEPS=50
export OUTPUT_DIR="./quick_test"
./run.sh
```

---

## 🎯 5단계: 결과 확인하기

### 📁 생성된 파일들 확인
```bash
# 결과 폴더 확인
ls -la ./results/

# 로그 파일 확인
ls -la training_log_*.log
```

**성공했다면 이런 파일들이 생성됩니다:**
```
📁 ./results/
├── 🤖 adapter_model.bin      # 학습된 LoRA 어댑터
├── ⚙️ adapter_config.json    # 설정 파일
├── 📊 training_args.bin      # 학습 파라미터
└── 🔤 tokenizer_config.json  # 토크나이저 설정

📄 training_log_2024_xxx.log  # 상세 학습 로그
```

### 🧪 모델 테스트해보기
```python
# 간단한 테스트 스크립트
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# 기본 모델 로드
base_model = AutoModelForCausalLM.from_pretrained("microsoft/DialoGPT-medium")
tokenizer = AutoTokenizer.from_pretrained("microsoft/DialoGPT-medium")

# 파인튜닝된 어댑터 로드
model = PeftModel.from_pretrained(base_model, "./results")

# 테스트 입력
text = "의학에서 고혈압의 증상은"
inputs = tokenizer.encode(text, return_tensors="pt")

# 결과 생성
outputs = model.generate(inputs, max_length=100, pad_token_id=tokenizer.eos_token_id)
result = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(f"입력: {text}")
print(f"출력: {result}")
```

---

## 🚨 문제해결 가이드

### ❌ 자주 발생하는 문제들

#### 1. **"MPS 백엔드를 찾을 수 없습니다"**
```bash
# 해결방법: PyTorch 업데이트
pip install --upgrade torch torchvision torchaudio
```

#### 2. **"CUDA out of memory" (NVIDIA GPU)**
```bash
# 해결방법: GPU 메모리 정리
python -c "import torch; torch.cuda.empty_cache()"

# 또는 배치 크기 줄이기
export BATCH_SIZE=1
```

#### 3. **"Permission denied" 오류**
```bash
# 해결방법: 실행 권한 부여
chmod +x *.sh
```

#### 4. **학습이 너무 느려요**
```python
# fine_tuning_llms_apple_silicon_optimized.py에서 수정
max_steps = 10          # 매우 빠른 테스트
per_device_train_batch_size = 1  # 메모리 절약
```

### 💡 성능 최적화 팁

#### 🍎 애플 실리콘 맥북 사용자
- **MPS 백엔드** 자동 활용 ✅
- **통합 메모리** 아키텍처 활용 ✅
- **bfloat16** 데이터 타입으로 메모리 절약 ✅

#### 🎮 NVIDIA GPU 사용자  
- **CUDA 가속** 자동 활용 ✅
- **bitsandbytes 양자화** 메모리 절약 ✅
- **멀티 GPU** 지원 ✅

#### 🖥️ CPU만 있는 경우
- 시간이 더 오래 걸리지만 동작함 ✅
- 배치 크기를 1로 설정 권장
- 인내심이 필요해요! ⏰

---

## 🎓 더 알아보기

### 📚 핵심 개념 정리

**LoRA (Low-Rank Adaptation)**
- 전체 모델을 다시 학습하지 않고 작은 어댑터만 학습
- 메모리 사용량 대폭 감소 (90% 절약!)
- 학습 속도 향상

**PEFT (Parameter Efficient Fine-Tuning)**
- 파라미터 효율적 파인튜닝 프레임워크
- Hugging Face에서 제공하는 표준 라이브러리

**양자화 (Quantization)**
- 모델 크기를 줄여 메모리 절약
- 4bit/8bit 정밀도로 압축

### 🚀 다음 단계 제안

1. **다른 데이터셋으로 실험해보기**
   - 법률 문서, 코드, 일반 대화 등

2. **하이퍼파라미터 튜닝**
   - 학습률, LoRA rank, 배치 크기 조정

3. **모델 서빙**
   - FastAPI로 웹 서비스 만들기
   - Docker 컨테이너화

4. **성능 평가**
   - BLEU, ROUGE 스코어 측정
   - 실제 태스크로 평가

---

## 💬 마무리

축하합니다! 🎉 여러분은 방금 **나만의 AI 모델**을 만들어 보았습니다!

이 과정을 통해 배운 것들:
- LLM 파인튜닝의 기본 개념
- 실제 코드 실행과 문제 해결
- 하드웨어별 최적화 방법
- 실무에서 활용할 수 있는 기술

---

행복한 AI 개발 되세요! 🚀✨

---
