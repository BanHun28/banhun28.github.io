---
title: λ„κµ¬λ‚ λ”°λΌν•  μ μλ” LLM νμΈνλ‹ κ°€μ΄λ“
date: 2025-08-08 08:30:49 +0900
categories: [artificial intelligence, machine learning]
tags: [machine learning, deep learning, llm, finetuning, nlp, pytorch, transformers, huggingface, apple, silicon, mps, gpt, bert]
---

# π€ λ„κµ¬λ‚ λ”°λΌν•  μ μλ” LLM νμΈνλ‹ κ°€μ΄λ“
## μ• ν” μ‹¤λ¦¬μ½ λ§¥λ¶μΌλ΅ λ‚λ§μ AI λ¨λΈ λ§λ“¤κΈ°

> "λ³µμ΅ν•΄ λ³΄μ΄λ” AI λ¨λΈ ν•™μµ, μ‚¬μ‹¤μ€ μƒκ°λ³΄λ‹¤ κ°„λ‹¨ν•©λ‹λ‹¤!"

**π“¦ ν”„λ΅μ νΈ GitHub μ£Όμ†:** https://github.com/BanHun28/finetune_study

μ•λ…•ν•μ„Έμ”! μ¤λμ€ μ—¬λ¬λ¶„κ³Ό ν•¨κ» **λ€κ·λ¨ μ–Έμ–΄λ¨λΈ(LLM) νμΈνλ‹**μ— λ„μ „ν•΄λ³΄λ ¤κ³  ν•©λ‹λ‹¤. 

"νμΈνλ‹μ΄ λ­”λ°? λ‚λ” κ°λ°μλ„ μ•„λ‹λ° ν•  μ μμ„κΉ?" ν•λ” λ¶„λ“¤λ„ κ±±μ •ν•μ§€ λ§μ„Έμ”. μ΄ κ°€μ΄λ“λ” **μ™„μ „ μ΄λ³΄μ**λ„ λ”°λΌν•  μ μλ„λ΅ λ¨λ“  κ³Όμ •μ„ λ‹¨κ³„λ³„λ΅ μ„¤λ…ν•΄λ“λ¦΄κ²μ”.

---

## π― μ΄ κΈ€μ—μ„ λ°°μΈ μ μλ” κ²ƒλ“¤

- LLM νμΈνλ‹μ΄ λ¬΄μ—‡μΈμ§€ μ‰½κ² μ΄ν•΄ν•κΈ°
- μ• ν” μ‹¤λ¦¬μ½ λ§¥λ¶μ—μ„ GPU κ°€μ†μΌλ΅ λ¨λΈ ν•™μµν•κΈ°
- NVIDIA GPUκ°€ μλ” Windows/Linuxμ—μ„λ„ μ‹¤ν–‰ν•κΈ°
- μλ£ λ°μ΄ν„°λ΅ νΉν™”λ AI λ¨λΈ λ§λ“¤μ–΄λ³΄κΈ°
- λ©”λ¨λ¦¬ λ¶€μ΅± λ¬Έμ  ν•΄κ²°ν•λ” λ°©λ²•λ“¤

---

## π¤” LLM νμΈνλ‹μ΄λ€?

imagine μ—¬λ¬λ¶„μ΄ ChatGPT κ°™μ€ AIλ¥Ό κ°€μ§€κ³  μλ”λ°, μ΄ AIκ°€ **μλ£ λ¶„μ•Ό**μ— λ€ν•΄μ„λ” μ λ¨λ¥Έλ‹¤κ³  ν•΄λ΄…μ‹λ‹¤. νμΈνλ‹μ€ μ΄ AIμ—κ² **μλ£ μ „λ¬Έ μ§€μ‹μ„ μ¶”κ°€λ΅ κ°€λ¥΄μΉλ” κ³Όμ •**μ…λ‹λ‹¤.

### π  μ§‘ μ§“κΈ°λ΅ λΉ„μ ν•λ©΄
- **μ‚¬μ „ ν›λ ¨λ λ¨λΈ**: κΈ°λ³Έ κ³¨μ΅°κ°€ μ™„μ„±λ μ§‘
- **νμΈνλ‹**: λ‚΄ μ·¨ν–¥μ— λ§κ² μΈν…λ¦¬μ–΄ ν•κΈ°
- **κ²°κ³Ό**: λ‚λ§μ νΉλ³„ν• μ§‘ μ™„μ„±!

### π’΅ μ™ νμΈνλ‹μ„ ν•΄μ•Ό ν• κΉμ”?
1. **νΉμ • λ„λ©”μΈ μ „λ¬Έμ„±** ν–¥μƒ (μλ£, λ²•λ¥ , κΈμµ λ“±)
2. **νμ‚¬ λ‚΄λ¶€ λ°μ΄ν„°**λ΅ λ§μ¶¤ν• AI κµ¬μ¶•
3. **λΉ„μ© μ μ•½** (μ²μλ¶€ν„° ν•™μµν•λ” κ²ƒλ³΄λ‹¤ ν›¨μ”¬ μ €λ ΄)
4. **λΉ λ¥Έ κ²°κ³Ό** (λ‡ μ‹κ°„μ΄λ©΄ μ™„μ„±!)

---

## π› οΈ μ‹μ‘ν•κΈ° μ „ μ¤€λΉ„μ‚¬ν•­

### π’» ν•λ“μ›¨μ–΄ μ”κµ¬μ‚¬ν•­
| κµ¬λ¶„         | μµμ†μ‚¬μ–‘  | κ¶μ¥μ‚¬μ–‘                     | λ‚΄ μƒν™©μ€?     |
| ------------ | --------- | ---------------------------- | -------------- |
| **λ©”λ¨λ¦¬**   | 8GB       | 16GB+                        | β… μ²΄ν¬ν•΄λ³΄μ„Έμ” |
| **μ €μ¥κ³µκ°„** | 10GB      | 20GB+                        | β… μ²΄ν¬ν•΄λ³΄μ„Έμ” |
| **GPU**      | μ—†μ–΄λ„ OK | μ• ν” μ‹¤λ¦¬μ½ λ§¥λ¶ λλ” NVIDIA | β… μ²΄ν¬ν•΄λ³΄μ„Έμ” |

### π” λ‚΄ λ§¥λ¶μ΄ λ­”μ§€ ν™•μΈν•κΈ°
```bash
# ν„°λ―Έλ„μ—μ„ μ‹¤ν–‰ν•΄λ³΄μ„Έμ”
system_profiler SPHardwareDataType | grep "Chip"
```

**κ²°κ³Ό μμ‹:**
- `Apple M1` β†’ μ• ν” μ‹¤λ¦¬μ½ λ§¥λ¶ β…
- `Apple M2` β†’ μ• ν” μ‹¤λ¦¬μ½ λ§¥λ¶ β…  
- `Apple M3` β†’ μ• ν” μ‹¤λ¦¬μ½ λ§¥λ¶ β…
- `Intel` β†’ μΈν…” λ§¥ (NVIDIA GPU μ„Ήμ… μ°Έκ³ )

---

## π“¥ 1λ‹¨κ³„: ν”„λ΅μ νΈ λ‹¤μ΄λ΅λ“

### GitHubμ—μ„ ν”„λ΅μ νΈ κ°€μ Έμ¤κΈ°
```bash
# 1. ν„°λ―Έλ„ μ—΄κΈ° (λ§¥: CMD+Space β†’ "ν„°λ―Έλ„" κ²€μƒ‰)
# 2. ν™ λ””λ ‰ν† λ¦¬λ΅ μ΄λ™
cd ~

# 3. ν”„λ΅μ νΈ λ³µμ 
git clone https://github.com/BanHun28/finetune_study.git

# 4. ν”„λ΅μ νΈ ν΄λ”λ΅ μ΄λ™
cd finetune_study

# 5. νμΌλ“¤μ΄ μ λ‹¤μ΄λ΅λ“ λμ—λ”μ§€ ν™•μΈ
ls -la
```

**π‰ μ„±κ³µν–λ‹¤λ©΄ μ΄λ° νμΌλ“¤μ΄ λ³΄μΌ κ±°μμ”:**
```
π“¦ finetune_study/
β”β”€β”€ π fine_tuning_llms_apple_silicon_optimized.py
β”β”€β”€ π fine_tuning_llms_with_hugging_face_partial_code.py
β”β”€β”€ β™οΈ requirements.txt
β”β”€β”€ π€ run.sh
β”β”€β”€ π€ setup_environment.sh
β””β”€β”€ π“ README.md
```

---

## β΅ 2λ‹¨κ³„: μ›ν΄λ¦­ μ‹¤ν–‰ (κ°€μ¥ μ‰¬μ΄ λ°©λ²•!)

### λ§λ²•μ λ…λ Ήμ–΄ ν• μ¤„
```bash
# μ‹¤ν–‰ κ¶ν• λ¶€μ—¬
chmod +x *.sh

# π― μ΄κ²ƒλ§ μ‹¤ν–‰ν•λ©΄ λ!
./run.sh
```

### π¤– μλ™μΌλ΅ μΌμ–΄λ‚λ” μΌλ“¤
1. **μ‹μ¤ν… κ°μ§€**: "μ–΄? μ• ν” μ‹¤λ¦¬μ½ λ§¥λ¶μ΄λ„¤!" λλ” "NVIDIA GPUκµ¬λ‚!"
2. **ν™κ²½ μ„¤μ •**: Python κ°€μƒν™κ²½ μλ™ μƒμ„±
3. **ν¨ν‚¤μ§€ μ„¤μΉ**: ν•„μ”ν• λΌμ΄λΈλ¬λ¦¬λ“¤ μλ™ μ„¤μΉ
4. **μµμ ν™” μ μ©**: ν•λ“μ›¨μ–΄μ— λ§λ” μµμ  μ„¤μ • μ μ©
5. **ν•™μµ μ‹μ‘**: μλ£ λ°μ΄ν„°μ…‹μΌλ΅ λ¨λΈ νμΈνλ‹ μ‹μ‘!

---

## π“ 3λ‹¨κ³„: ν•™μµ κ³Όμ • λ¨λ‹ν„°λ§

### μ‹¤μ‹κ°„μΌλ΅ μ§„ν–‰μƒν™© ν™•μΈν•κΈ°
```bash
# μƒ ν„°λ―Έλ„ μ°½μ„ μ—΄κ³  μ‹¤ν–‰
tail -f training_log_*.log
```

**μ΄λ° λ΅κ·Έλ“¤μ΄ μ‹¤μ‹κ°„μΌλ΅ λ³΄μΌ κ±°μμ”:**
```
π€ Starting LLM Fine-tuning...
π“± Detected: Apple Silicon MacBook
π§  Using MPS backend for acceleration
π“ Loading medical dataset...
β΅ Training step 1/100 - Loss: 2.341
β΅ Training step 2/100 - Loss: 2.298
π’Ύ Saving checkpoint...
```

### π’» μ‹μ¤ν… μƒνƒ λ¨λ‹ν„°λ§
```bash
# λ§¥μ—μ„ μ‹μ¤ν… λ¨λ‹ν„° μ—΄κΈ°
open -a "Activity Monitor"

# λλ” ν„°λ―Έλ„μ—μ„ ν™•μΈ
top -o cpu
```

---

## π”§ 4λ‹¨κ³„: μ„¤μ • μ»¤μ¤ν„°λ§μ΄μ§•

### π“ κ°„λ‹¨ν• μ„¤μ • λ³€κ²½ν•κΈ°

λ©”λ¨λ¦¬κ°€ λ¶€μ΅±ν•κ±°λ‚ λ” λΉ λ¥΄κ² μ‹¤ν–‰ν•κ³  μ‹¶λ‹¤λ©΄:

```python
# fine_tuning_llms_apple_silicon_optimized.py νμΌμ„ μ—΄μ–΄μ„
# μ΄ κ°’λ“¤μ„ μμ •ν•΄λ³΄μ„Έμ”

# π’Ύ λ©”λ¨λ¦¬ μ μ•½ μ„¤μ •
per_device_train_batch_size = 1  # κΈ°λ³Έκ°’: 4
max_seq_length = 256            # κΈ°λ³Έκ°’: 512

# β΅ λΉ λ¥Έ ν…μ¤νΈμ© μ„¤μ •  
max_steps = 50                  # κΈ°λ³Έκ°’: 100

# π― LoRA μ„¤μ • (κ³ κΈ‰μμ©)
r = 16                         # κΈ°λ³Έκ°’: 32 (μ‘μ„μλ΅ λΉ λ¦„)
lora_alpha = 16               # κΈ°λ³Έκ°’: 16
```

### π† λ©”λ¨λ¦¬ λ¶€μ΅± μ‹ λ€μ²λ²•
```bash
# λ°©λ²• 1: λ” μ‘μ€ λ°°μΉ ν¬κΈ°λ΅ μ‹¤ν–‰
export BATCH_SIZE=1
./run.sh

# λ°©λ²• 2: ν™κ²½λ³€μλ΅ μ„¤μ • μ΅°μ •
export MAX_STEPS=50
export OUTPUT_DIR="./quick_test"
./run.sh
```

---

## π― 5λ‹¨κ³„: κ²°κ³Ό ν™•μΈν•κΈ°

### π“ μƒμ„±λ νμΌλ“¤ ν™•μΈ
```bash
# κ²°κ³Ό ν΄λ” ν™•μΈ
ls -la ./results/

# λ΅κ·Έ νμΌ ν™•μΈ
ls -la training_log_*.log
```

**μ„±κ³µν–λ‹¤λ©΄ μ΄λ° νμΌλ“¤μ΄ μƒμ„±λ©λ‹λ‹¤:**
```
π“ ./results/
β”β”€β”€ π¤– adapter_model.bin      # ν•™μµλ LoRA μ–΄λ‘ν„°
β”β”€β”€ β™οΈ adapter_config.json    # μ„¤μ • νμΌ
β”β”€β”€ π“ training_args.bin      # ν•™μµ νλΌλ―Έν„°
β””β”€β”€ π”¤ tokenizer_config.json  # ν† ν¬λ‚μ΄μ € μ„¤μ •

π“„ training_log_2024_xxx.log  # μƒμ„Έ ν•™μµ λ΅κ·Έ
```

### π§ λ¨λΈ ν…μ¤νΈν•΄λ³΄κΈ°
```python
# κ°„λ‹¨ν• ν…μ¤νΈ μ¤ν¬λ¦½νΈ
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# κΈ°λ³Έ λ¨λΈ λ΅λ“
base_model = AutoModelForCausalLM.from_pretrained("microsoft/DialoGPT-medium")
tokenizer = AutoTokenizer.from_pretrained("microsoft/DialoGPT-medium")

# νμΈνλ‹λ μ–΄λ‘ν„° λ΅λ“
model = PeftModel.from_pretrained(base_model, "./results")

# ν…μ¤νΈ μ…λ ¥
text = "μν•™μ—μ„ κ³ νμ••μ μ¦μƒμ€"
inputs = tokenizer.encode(text, return_tensors="pt")

# κ²°κ³Ό μƒμ„±
outputs = model.generate(inputs, max_length=100, pad_token_id=tokenizer.eos_token_id)
result = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(f"μ…λ ¥: {text}")
print(f"μ¶λ ¥: {result}")
```

---

## π¨ λ¬Έμ ν•΄κ²° κ°€μ΄λ“

### β μμ£Ό λ°μƒν•λ” λ¬Έμ λ“¤

#### 1. **"MPS λ°±μ—”λ“λ¥Ό μ°Ύμ„ μ μ—†μµλ‹λ‹¤"**
```bash
# ν•΄κ²°λ°©λ²•: PyTorch μ—…λ°μ΄νΈ
pip install --upgrade torch torchvision torchaudio
```

#### 2. **"CUDA out of memory" (NVIDIA GPU)**
```bash
# ν•΄κ²°λ°©λ²•: GPU λ©”λ¨λ¦¬ μ •λ¦¬
python -c "import torch; torch.cuda.empty_cache()"

# λλ” λ°°μΉ ν¬κΈ° μ¤„μ΄κΈ°
export BATCH_SIZE=1
```

#### 3. **"Permission denied" μ¤λ¥**
```bash
# ν•΄κ²°λ°©λ²•: μ‹¤ν–‰ κ¶ν• λ¶€μ—¬
chmod +x *.sh
```

#### 4. **ν•™μµμ΄ λ„λ¬΄ λλ ¤μ”**
```python
# fine_tuning_llms_apple_silicon_optimized.pyμ—μ„ μμ •
max_steps = 10          # λ§¤μ° λΉ λ¥Έ ν…μ¤νΈ
per_device_train_batch_size = 1  # λ©”λ¨λ¦¬ μ μ•½
```

### π’΅ μ„±λ¥ μµμ ν™” ν

#### π μ• ν” μ‹¤λ¦¬μ½ λ§¥λ¶ μ‚¬μ©μ
- **MPS λ°±μ—”λ“** μλ™ ν™μ© β…
- **ν†µν•© λ©”λ¨λ¦¬** μ•„ν‚¤ν…μ² ν™μ© β…
- **bfloat16** λ°μ΄ν„° νƒ€μ…μΌλ΅ λ©”λ¨λ¦¬ μ μ•½ β…

#### π® NVIDIA GPU μ‚¬μ©μ  
- **CUDA κ°€μ†** μλ™ ν™μ© β…
- **bitsandbytes μ–‘μν™”** λ©”λ¨λ¦¬ μ μ•½ β…
- **λ©€ν‹° GPU** μ§€μ› β…

#### π–¥οΈ CPUλ§ μλ” κ²½μ°
- μ‹κ°„μ΄ λ” μ¤λ κ±Έλ¦¬μ§€λ§ λ™μ‘ν•¨ β…
- λ°°μΉ ν¬κΈ°λ¥Ό 1λ΅ μ„¤μ • κ¶μ¥
- μΈλ‚΄μ‹¬μ΄ ν•„μ”ν•΄μ”! β°

---

## π“ λ” μ•μ•„λ³΄κΈ°

### π“ ν•µμ‹¬ κ°λ… μ •λ¦¬

**LoRA (Low-Rank Adaptation)**
- μ „μ²΄ λ¨λΈμ„ λ‹¤μ‹ ν•™μµν•μ§€ μ•κ³  μ‘μ€ μ–΄λ‘ν„°λ§ ν•™μµ
- λ©”λ¨λ¦¬ μ‚¬μ©λ‰ λ€ν­ κ°μ† (90% μ μ•½!)
- ν•™μµ μ†λ„ ν–¥μƒ

**PEFT (Parameter Efficient Fine-Tuning)**
- νλΌλ―Έν„° ν¨μ¨μ  νμΈνλ‹ ν”„λ μ„μ›ν¬
- Hugging Faceμ—μ„ μ κ³µν•λ” ν‘μ¤€ λΌμ΄λΈλ¬λ¦¬

**μ–‘μν™” (Quantization)**
- λ¨λΈ ν¬κΈ°λ¥Ό μ¤„μ—¬ λ©”λ¨λ¦¬ μ μ•½
- 4bit/8bit μ •λ°€λ„λ΅ μ••μ¶•

### π€ λ‹¤μ λ‹¨κ³„ μ μ•

1. **λ‹¤λ¥Έ λ°μ΄ν„°μ…‹μΌλ΅ μ‹¤ν—ν•΄λ³΄κΈ°**
   - λ²•λ¥  λ¬Έμ„, μ½”λ“, μΌλ° λ€ν™” λ“±

2. **ν•μ΄νΌνλΌλ―Έν„° νλ‹**
   - ν•™μµλ¥ , LoRA rank, λ°°μΉ ν¬κΈ° μ΅°μ •

3. **λ¨λΈ μ„λΉ™**
   - FastAPIλ΅ μ›Ή μ„λΉ„μ¤ λ§λ“¤κΈ°
   - Docker μ»¨ν…μ΄λ„ν™”

4. **μ„±λ¥ ν‰κ°€**
   - BLEU, ROUGE μ¤μ½”μ–΄ μΈ΅μ •
   - μ‹¤μ  νƒμ¤ν¬λ΅ ν‰κ°€

---

## π’¬ λ§λ¬΄λ¦¬

μ¶•ν•ν•©λ‹λ‹¤! π‰ μ—¬λ¬λ¶„μ€ λ°©κΈ **λ‚λ§μ AI λ¨λΈ**μ„ λ§λ“¤μ–΄ λ³΄μ•μµλ‹λ‹¤!

μ΄ κ³Όμ •μ„ ν†µν•΄ λ°°μ΄ κ²ƒλ“¤:
- LLM νμΈνλ‹μ κΈ°λ³Έ κ°λ…
- μ‹¤μ  μ½”λ“ μ‹¤ν–‰κ³Ό λ¬Έμ  ν•΄κ²°
- ν•λ“μ›¨μ–΄λ³„ μµμ ν™” λ°©λ²•
- μ‹¤λ¬΄μ—μ„ ν™μ©ν•  μ μλ” κΈ°μ 

---

ν–‰λ³µν• AI κ°λ° λμ„Έμ”! π€β¨

---
