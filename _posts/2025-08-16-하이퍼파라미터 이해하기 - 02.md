---
title: 하이퍼파라미터 이해하기 - 02
date: 2025-08-16 10:57:56 +0900
categories: [artificial intelligence, machine learning]
tags: [machine learning, deep learning, hyperparameters, finetuning, llm, llamafactory, pytorch, optimization, training, scheduler, ai]     # TAG names should always be lowercase
---

# 학습률 스케줄러 이해하기.

## 📚 개념 이해

### 학습률 스케줄러란?

학습률 스케줄러(Learning Rate Scheduler)는 훈련 과정에서 학습률을 동적으로 조정하는 알고리즘입니다. 고정된 학습률 대신, 훈련 진행에 따라 학습률을 변경하여 더 효과적인 학습을 달성합니다.

```python
# 기본 개념
초기 학습률: 5e-5
스케줄러: cosine
결과: 5e-5 → ... → 거의 0 (코사인 곡선을 따라 감소)
```

### 왜 학습률 스케줄링이 필요한가?

**1. 초기 단계**: 큰 학습률로 빠른 수렴
**2. 후반 단계**: 작은 학습률로 정밀한 최적화

```python
# 고정 학습률의 문제점
learning_rate = 5e-4  # 항상 동일

문제:
- 초기: 느린 수렴
- 후반: 최적값 주변에서 진동 (overshooting)

# 스케줄링의 해결책
초기: 큰 학습률 → 빠른 수렴
후반: 작은 학습률 → 정밀한 최적화
```

## 🔄 LLaMA Factory에서 지원하는 스케줄러

### 1. linear (선형 감소)

**동작 방식**: 학습률이 선형적으로 감소합니다.

```python
# 수학적 공식
lr(step) = max_lr * (1 - step / max_steps)

# 실제 예시 (max_lr=5e-5, max_steps=1000)
Step 0:    lr = 5e-5  (100%)
Step 250:  lr = 3.75e-5 (75%)
Step 500:  lr = 2.5e-5  (50%)
Step 750:  lr = 1.25e-5 (25%)
Step 1000: lr = 0       (0%)
```

**설정**:
```yaml
lr_scheduler_type: linear
learning_rate: 5e-5
warmup_steps: 100  # warmup 후 선형 감소 시작
```

**장점**:
- 간단하고 예측 가능한 패턴
- 빠른 초기 학습과 안정적인 수렴
- 디버깅이 쉬움

**단점**:
- 후반부에 학습률이 너무 빨리 감소
- 지역 최적값에 빠질 위험

**적합한 상황**:
- 짧은 훈련 (1-3 에포크)
- 안정적인 데이터셋
- 빠른 프로토타이핑

### 2. cosine (코사인 스케줄링)

**동작 방식**: 코사인 함수를 따라 부드럽게 감소합니다.

```python
# 수학적 공식  
lr(step) = min_lr + (max_lr - min_lr) * 0.5 * (1 + cos(π * step / max_steps))

# 실제 예시 (max_lr=5e-5, min_lr=0, max_steps=1000)
Step 0:    lr = 5e-5     (100%)
Step 250:  lr = 3.54e-5  (≈71%)  
Step 500:  lr = 2.5e-5   (50%)
Step 750:  lr = 1.46e-5  (≈29%)
Step 1000: lr = 0        (0%)
```

**설정**:
```yaml
lr_scheduler_type: cosine
learning_rate: 5e-5
warmup_steps: 100
# cosine_restarts: false  # 기본값
```

**장점**:
- 부드러운 감소 곡선
- 후반부까지 적절한 학습률 유지
- 대부분의 상황에서 우수한 성능
- 이론적 근거가 탄탄함

**단점**:
- 복잡한 수학적 공식
- 하이퍼파라미터 튜닝이 상대적으로 어려움

**적합한 상황**:
- 대부분의 파인튜닝 작업 (권장)
- 긴 훈련 시간
- 고품질 결과가 필요한 경우

### 3. cosine_with_restarts (코사인 재시작)

**동작 방식**: 주기적으로 학습률을 초기값으로 리셋하며 코사인 감소를 반복합니다.

```python
# 개념
Period 1: 5e-5 → 0 (1000 steps)
Restart:  다시 5e-5부터 시작
Period 2: 5e-5 → 0 (1000 steps)
...
```

**설정**:
```yaml
lr_scheduler_type: cosine_with_restarts
learning_rate: 5e-5
warmup_steps: 100
num_cycles: 3  # 재시작 횟수
```

**장점**:
- 지역 최적값 탈출 가능
- 긴 훈련에서 성능 향상
- 다양한 최적값 탐색

**단점**:
- 복잡한 설정
- 불안정할 수 있음
- 훈련 시간 증가

**적합한 상황**:
- 매우 긴 훈련 (10+ 에포크)
- 복잡한 데이터셋
- 최고 성능이 필요한 연구

### 4. polynomial (다항식 감소)

**동작 방식**: 다항식 함수를 따라 감소합니다.

```python
# 수학적 공식 (power=1이면 linear와 동일)
lr(step) = max_lr * (1 - step / max_steps) ^ power

# power=2인 경우 (제곱 감소)
Step 0:    lr = 5e-5    (100%)
Step 250:  lr = 2.81e-5 (≈56%)
Step 500:  lr = 1.25e-5 (25%)
Step 750:  lr = 0.31e-5 (≈6%)
Step 1000: lr = 0       (0%)
```

**설정**:
```yaml
lr_scheduler_type: polynomial
learning_rate: 5e-5
power: 2.0  # 감소 정도 조절
```

**장점**:
- 감소 곡선을 세밀하게 조절 가능
- power 값으로 다양한 패턴 생성

**단점**:
- 추가 하이퍼파라미터 (power) 필요
- 일반적으로 cosine보다 성능이 떨어짐

**적합한 상황**:
- 특별한 요구사항이 있는 경우
- 실험적 연구

### 5. constant (상수 유지)

**동작 방식**: Warmup 후 학습률을 일정하게 유지합니다.

```python
# 패턴
Step 1-100:  0 → 5e-5 (warmup)
Step 101+:   5e-5 (constant)
```

**설정**:
```yaml
lr_scheduler_type: constant
learning_rate: 5e-5
warmup_steps: 100
```

**장점**:
- 매우 간단함
- 예측 가능한 동작
- 안정적인 훈련

**단점**:
- 후반부 최적화 부족
- 지역 최적값에 빠질 위험

**적합한 상황**:
- 매우 짧은 훈련
- 안정성이 최우선인 경우
- 디버깅 목적

### 6. constant_with_warmup (워밍업 후 상수)

**동작 방식**: `constant`와 동일하지만 명시적으로 warmup을 표현합니다.

```yaml
lr_scheduler_type: constant_with_warmup
learning_rate: 5e-5
warmup_steps: 100
```

### 7. inverse_sqrt (역제곱근 감소)

**동작 방식**: 역제곱근 함수를 따라 감소합니다.

```python
# 수학적 공식
lr(step) = max_lr / sqrt(max(step, warmup_steps))

# 특징: 초기에는 빠르게, 후반에는 천천히 감소
```

**설정**:
```yaml
lr_scheduler_type: inverse_sqrt
learning_rate: 5e-5
warmup_steps: 100
```

**적합한 상황**:
- Transformer 모델의 원래 논문에서 사용
- 사전 훈련 (pre-training)

## 📊 시각적 비교

### 각 스케줄러의 학습률 변화 패턴

```python
# 1000 steps, warmup 100 steps, initial_lr = 5e-5 기준

Linear:
  |\
  | \
  |  \
  |   \______
  0   100   1000

Cosine:
  |\
  | ╲
  |  ╲
  |   ╲___
  0   100   1000

Constant:
  |\
  | |────────
  |
  |
  0   100   1000

Polynomial (power=2):
  |\
  | ╲
  |  ╲╲
  |    ╲╲___
  0   100   1000
```

## 🎯 실무 적용 가이드

### 작업 유형별 권장 스케줄러

#### 1. 일반적인 텍스트 분류/생성
```yaml
# 가장 안전하고 효과적인 선택
lr_scheduler_type: cosine
learning_rate: 5e-5
warmup_steps: 100
num_train_epochs: 3
```

#### 2. 대화형 AI (챗봇)
```yaml
# 안정성과 일관성 중시
lr_scheduler_type: cosine
learning_rate: 3e-5
warmup_steps: 200
num_train_epochs: 3
```

#### 3. 코드 생성
```yaml
# 정확성 중시
lr_scheduler_type: linear
learning_rate: 1e-5
warmup_steps: 500
num_train_epochs: 5
```

#### 4. 빠른 프로토타이핑
```yaml
# 간단하고 빠른 설정
lr_scheduler_type: linear
learning_rate: 1e-4
warmup_steps: 50
num_train_epochs: 1
```

#### 5. 연구/실험 목적
```yaml
# 최고 성능 추구
lr_scheduler_type: cosine_with_restarts
learning_rate: 5e-5
warmup_steps: 100
num_cycles: 2
num_train_epochs: 10
```

### 데이터셋 크기별 권장사항

#### 소규모 데이터셋 (< 1,000 샘플)
```yaml
lr_scheduler_type: linear
learning_rate: 1e-4
warmup_steps: 20
num_train_epochs: 5

# 이유: 빠른 수렴이 필요, 과적합 위험
```

#### 중간 규모 (1,000-10,000 샘플)
```yaml
lr_scheduler_type: cosine
learning_rate: 5e-5
warmup_steps: 100
num_train_epochs: 3

# 이유: 균형잡힌 접근
```

#### 대규모 (10,000+ 샘플)
```yaml
lr_scheduler_type: cosine
learning_rate: 3e-5
warmup_steps: 500
num_train_epochs: 2

# 이유: 안정적이고 정교한 최적화
```

### 파인튜닝 방법별 권장사항

#### LoRA 파인튜닝
```yaml
# LoRA는 상대적으로 안정적
lr_scheduler_type: cosine
learning_rate: 1e-4
warmup_steps: 100
```

#### QLoRA 파인튜닝
```yaml
# 양자화로 인한 불안정성 고려
lr_scheduler_type: linear
learning_rate: 2e-4
warmup_steps: 200
```

#### Full Parameter 파인튜닝
```yaml
# 신중한 접근 필요
lr_scheduler_type: cosine
learning_rate: 1e-5
warmup_steps: 1000
```

## 🔬 고급 설정 및 조합

### 1. 다단계 학습률 스케줄링

실제 프로덕션에서는 여러 단계로 나누어 훈련하는 경우가 많습니다:

```yaml
# Stage 1: 빠른 적응
stage: sft
lr_scheduler_type: linear
learning_rate: 1e-4
warmup_steps: 100
num_train_epochs: 1
output_dir: saves/stage1

# Stage 2: 정교한 튜닝 
stage: sft
lr_scheduler_type: cosine
learning_rate: 3e-5
warmup_steps: 50
num_train_epochs: 2
model_name_or_path: saves/stage1  # Stage 1 결과 사용
output_dir: saves/stage2
```

### 2. 동적 학습률 조정

대규모 프로젝트에서의 적응적 접근:

```python
# 의사코드: 성능에 따른 동적 조정
if validation_loss가 개선되지 않으면:
    learning_rate *= 0.5
    lr_scheduler_type = "constant"  # 안정화
else:
    lr_scheduler_type = "cosine"  # 계속 최적화
```

### 3. 온도 스케줄링과 결합

```yaml
# 고급 설정 예시
lr_scheduler_type: cosine_with_restarts
learning_rate: 5e-5
warmup_steps: 200
num_cycles: 3

# 추가 최적화 기법들과 결합
gradient_accumulation_steps: 8
gradient_checkpointing: true
fp16: true
```

## 📈 실제 사례 분석

### 사례 1: ITEASY 고객 지원 챗봇

**요구사항**:
- 5,000개 고객 문의 데이터
- 24/7 안정적 서비스
- 일관된 응답 품질

**1차 시도 (실패)**:
```yaml
lr_scheduler_type: linear
learning_rate: 1e-3  # 너무 큰 학습률
warmup_steps: 10     # 너무 짧은 warmup
num_train_epochs: 1

결과: 불안정한 학습, 일관성 없는 응답
```

**최종 성공 설정**:
```yaml
lr_scheduler_type: cosine
learning_rate: 3e-5  # 안정적인 학습률
warmup_steps: 300    # 충분한 warmup
num_train_epochs: 3
per_device_train_batch_size: 4
gradient_accumulation_steps: 8

결과: 
- 안정적인 loss 감소
- 일관된 응답 품질
- 고객 만족도 95% 달성
```

### 사례 2: 기술 문서 자동 요약

**요구사항**:
- 50,000개 기술 문서
- 정확한 요약 생성
- 긴 문서 처리 (2048 토큰)

**실험 결과**:

```yaml
# 실험 A: Linear scheduler
lr_scheduler_type: linear
learning_rate: 5e-5
ROUGE-L: 0.42

# 실험 B: Cosine scheduler  
lr_scheduler_type: cosine
learning_rate: 5e-5
ROUGE-L: 0.47  # 5% 개선

# 실험 C: Cosine with restarts
lr_scheduler_type: cosine_with_restarts
learning_rate: 5e-5
num_cycles: 2
ROUGE-L: 0.49  # 최고 성능
```

**선택된 최종 설정**:
```yaml
lr_scheduler_type: cosine_with_restarts
learning_rate: 5e-5
warmup_steps: 500
num_cycles: 2
num_train_epochs: 4
```

### 사례 3: 빠른 프로토타이핑 (연구용)

**요구사항**:
- 빠른 결과 확인
- 다양한 아이디어 테스트
- 제한된 GPU 시간

**최적화된 설정**:
```yaml
lr_scheduler_type: linear
learning_rate: 2e-4  # 큰 학습률로 빠른 수렴
warmup_steps: 20     # 최소한의 warmup
num_train_epochs: 1
max_samples: 1000    # 작은 데이터셋

결과: 30분 만에 기본 성능 확인 가능
```

## 🚨 흔한 실수와 해결방법

### 실수 1: 스케줄러와 학습률 불일치

**문제**:
```yaml
lr_scheduler_type: cosine
learning_rate: 1e-3  # 스케줄러에 비해 너무 큰 학습률
```

**증상**: 초기 불안정, NaN 발생

**해결**:
```yaml
lr_scheduler_type: cosine
learning_rate: 5e-5  # 스케줄러에 적합한 학습률
warmup_steps: 200    # 충분한 warmup
```

### 실수 2: 짧은 훈련에 복잡한 스케줄러

**문제**:
```yaml
num_train_epochs: 1
max_steps: 100
lr_scheduler_type: cosine_with_restarts  # 복잡한 스케줄러
```

**해결**:
```yaml
num_train_epochs: 1
max_steps: 100
lr_scheduler_type: linear  # 간단한 스케줄러
```

### 실수 3: Warmup과 스케줄러 충돌

**문제**:
```yaml
warmup_steps: 500
max_steps: 600       # warmup이 전체 훈련의 83%
lr_scheduler_type: cosine
```

**해결**:
```yaml
warmup_steps: 60     # 전체의 10%
max_steps: 600
lr_scheduler_type: cosine
```

## 🔧 디버깅 및 모니터링

### 학습률 변화 모니터링

```yaml
# 상세 로깅 설정
logging_steps: 10
report_to: tensorboard
use_swanlab: true

# 관찰할 지표들:
# - learning_rate: 현재 학습률
# - train_loss: 손실 함수 값
# - eval_loss: 검증 손실
```

### TensorBoard에서 확인할 패턴

#### 정상적인 패턴
```python
# 학습률 그래프
- Warmup 구간: 0에서 목표값까지 선형 증가
- Main 구간: 스케줄러에 따른 부드러운 감소

# Loss 그래프  
- 초기: 빠른 감소
- 중간: 안정적 감소
- 후반: 느린 감소 또는 수렴
```

#### 문제가 있는 패턴
```python
# 학습률이 너무 클 때
- Loss가 진동하거나 발산
- NaN 값 발생

# 학습률이 너무 작을 때  
- Loss 감소가 매우 느림
- 수렴하지 않음

# 스케줄러 설정 오류
- 학습률이 예상과 다르게 변화
- 갑작스러운 변화
```

### 실시간 조정 기법

```python
# Early stopping과 결합
early_stopping_patience: 3
early_stopping_threshold: 0.01

# 자동 학습률 감소
reduce_lr_on_plateau: true
reduce_lr_factor: 0.5
reduce_lr_patience: 2
```

## 💡 고급 최적화 팁

### 1. 하이브리드 접근법

```yaml
# Phase 1: 빠른 적응 (Linear)
lr_scheduler_type: linear
learning_rate: 1e-4
num_train_epochs: 1

# Phase 2: 정교한 최적화 (Cosine)  
lr_scheduler_type: cosine
learning_rate: 3e-5
num_train_epochs: 2
```

### 2. 도메인별 최적화

```yaml
# 창작/대화 (창의성 중시)
lr_scheduler_type: constant_with_warmup
learning_rate: 5e-5

# 코드/분석 (정확성 중시)
lr_scheduler_type: cosine
learning_rate: 1e-5

# 분류/추출 (효율성 중시)
lr_scheduler_type: linear  
learning_rate: 1e-4
```

### 3. 리소스별 최적화

```yaml
# 고성능 GPU (A100, H100)
lr_scheduler_type: cosine_with_restarts
learning_rate: 5e-5
per_device_train_batch_size: 16

# 중간 GPU (RTX 4090, V100)
lr_scheduler_type: cosine
learning_rate: 5e-5  
per_device_train_batch_size: 8

# 저사양 GPU (RTX 3080, 4060)
lr_scheduler_type: linear
learning_rate: 1e-4
per_device_train_batch_size: 2
```

## 📋 최종 권장사항

### 상황별 빠른 선택 가이드

#### 🚀 처음 사용자
```yaml
lr_scheduler_type: cosine
learning_rate: 5e-5
warmup_steps: 100
```

#### ⚡ 빠른 실험
```yaml
lr_scheduler_type: linear  
learning_rate: 1e-4
warmup_steps: 50
```

#### 🎯 프로덕션
```yaml
lr_scheduler_type: cosine
learning_rate: 3e-5
warmup_steps: 300
```

#### 🔬 연구 목적
```yaml
lr_scheduler_type: cosine_with_restarts
learning_rate: 5e-5
warmup_steps: 200
num_cycles: 2
```

### 성능 순위 (일반적인 경우)

1. **cosine** - 가장 범용적이고 안정적
2. **linear** - 간단하고 빠른 수렴
3. **cosine_with_restarts** - 최고 성능 (복잡함)
4. **constant** - 안정성 최우선
5. **polynomial** - 특수 목적

학습률 스케줄러는 모델 성능에 직접적인 영향을 미치는 핵심 하이퍼파라미터입니다. 적절한 선택과 설정을 통해 훨씬 더 효과적인 파인튜닝을 달성할 수 있습니다!
