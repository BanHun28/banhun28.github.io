---
title: 하이퍼파라미터 이해하기 - 03
date: 2025-08-17 10:57:59 +0900
categories: [artificial intelligence, machine learning]
tags: [machine learning, deep learning, hyperparameters, finetuning, llm, llamafactory, pytorch, optimization, training, epochs, ai]     # TAG names should always be lowercase
---

# 에포크(Epochs) 및 스텝(Steps) 이해하기

## 📚 개념 이해

### 에포크(Epoch)란?

**에포크**는 전체 훈련 데이터셋을 한 번 완전히 순회하는 것을 의미합니다.

```python
# 기본 개념
데이터셋 크기: 10,000개 샘플
1 에포크 = 모든 10,000개 샘플을 한 번씩 사용하여 훈련

1 에포크 완료 = 모델이 모든 데이터를 한 번씩 "본" 상태
```

### 스텝(Step)이란?

**스텝**은 하나의 배치(batch)로 한 번의 가중치 업데이트를 수행하는 것을 의미합니다.

```python
# 기본 개념
배치 크기: 8개 샘플
1 스텝 = 8개 샘플로 forward → backward → 가중치 업데이트

# 관계식
총 스텝 수 = (데이터셋 크기 / 배치 크기) × 에포크 수
```

### 실제 계산 예시

```python
# 예시 설정
데이터셋 크기: 10,000개
per_device_train_batch_size: 4
gradient_accumulation_steps: 2
GPU 개수: 2
num_train_epochs: 3

# 실제 배치 크기 계산
effective_batch_size = per_device_train_batch_size × gradient_accumulation_steps × GPU개수
effective_batch_size = 4 × 2 × 2 = 16

# 에포크당 스텝 수
steps_per_epoch = 데이터셋_크기 / effective_batch_size
steps_per_epoch = 10,000 / 16 = 625

# 총 훈련 스텝 수
total_training_steps = steps_per_epoch × num_train_epochs
total_training_steps = 625 × 3 = 1,875
```

## ⚙️ LLaMA Factory 설정 옵션

### num_train_epochs (훈련 에포크 수)

**설명**: 전체 데이터셋을 몇 번 반복할지 결정합니다.

```yaml
num_train_epochs: 3.0  # 기본값: 3.0
```

**소수점 사용 가능**:
```yaml
num_train_epochs: 2.5  # 2.5 에포크 (전체 데이터의 2.5배)
num_train_epochs: 1.2  # 1.2 에포크 (전체 데이터의 1.2배)
```

**실제 동작**:
```python
# num_train_epochs: 2.5인 경우
에포크 1: 모든 데이터 사용 (100%)
에포크 2: 모든 데이터 사용 (100%)  
에포크 3: 데이터의 50%만 사용
총 훈련량: 전체 데이터의 2.5배
```

### max_steps (최대 스텝 수)

**설명**: 훈련할 최대 스텝 수를 직접 지정합니다. `num_train_epochs`보다 우선됩니다.

```yaml
max_steps: 1000  # 정확히 1000 스텝만 훈련
# num_train_epochs 설정은 무시됨
```

**우선순위**:
```yaml
# Case 1: max_steps가 설정된 경우
max_steps: 500
num_train_epochs: 10  # 이 설정은 무시됨
# 결과: 500 스텝 후 훈련 종료

# Case 2: max_steps가 -1 또는 미설정
max_steps: -1  # 또는 설정하지 않음
num_train_epochs: 3
# 결과: 3 에포크 완료까지 훈련
```

### 실제 선택 기준

#### 에포크 기반 훈련 (권장)
```yaml
num_train_epochs: 3
max_steps: -1  # 또는 설정하지 않음

장점:
- 직관적이고 이해하기 쉬움
- 데이터셋 크기가 바뀌어도 일관된 훈련량
- 일반적인 관례
```

#### 스텝 기반 훈련
```yaml
max_steps: 1000
# num_train_epochs는 설정하지 않거나 무시됨

장점:
- 정확한 훈련량 제어
- 시간 제약이 있는 실험
- 연구/비교 실험에서 정확한 제어 필요
```

## 📊 데이터셋 크기별 권장 설정

### 소규모 데이터셋 (< 1,000 샘플)

```yaml
# 설정 예시
num_train_epochs: 5-10
max_samples: 1000

# 이유와 계산
데이터가 적어서 많은 반복 필요
과적합 주의가 필요하지만 충분한 학습 시간 확보

# 예시 계산
데이터셋: 500개
배치 크기: 8
steps_per_epoch = 500/8 = 62.5 ≈ 63
5 에포크 = 63 × 5 = 315 스텝
```

**실무 설정**:
```yaml
# 소규모 고객 FAQ 데이터
num_train_epochs: 8
per_device_train_batch_size: 4
learning_rate: 1e-4
early_stopping_patience: 3  # 과적합 방지
```

### 중간 규모 데이터셋 (1,000-10,000 샘플)

```yaml
# 설정 예시  
num_train_epochs: 3-5
max_samples: 10000

# 예시 계산
데이터셋: 5,000개
배치 크기: 16 (4×2×2)
steps_per_epoch = 5000/16 = 312.5 ≈ 313
3 에포크 = 313 × 3 = 939 스텝
```

**실무 설정**:
```yaml
# 일반적인 업무용 데이터
num_train_epochs: 3
per_device_train_batch_size: 8
gradient_accumulation_steps: 2
learning_rate: 5e-5
```

### 대규모 데이터셋 (> 10,000 샘플)

```yaml
# 설정 예시
num_train_epochs: 1-3  
max_samples: 100000

# 예시 계산
데이터셋: 50,000개
배치 크기: 32 (8×2×2)  
steps_per_epoch = 50000/32 = 1562.5 ≈ 1563
2 에포크 = 1563 × 2 = 3126 스텝
```

**실무 설정**:
```yaml
# 대규모 기술 문서 데이터
num_train_epochs: 2
per_device_train_batch_size: 8
gradient_accumulation_steps: 4
learning_rate: 3e-5
```

## 🎯 파인튜닝 방법별 권장사항

### LoRA 파인튜닝

**특징**: 
- 적은 파라미터만 훈련
- 상대적으로 안정적
- 빠른 수렴 가능

```yaml
# LoRA 권장 설정
num_train_epochs: 3-5
learning_rate: 1e-4

# 이유: LoRA는 효율적이라 더 많은 에포크로 세밀한 조정 가능
```

**실제 예시**:
```yaml
finetuning_type: lora
num_train_epochs: 4
per_device_train_batch_size: 8
gradient_accumulation_steps: 2
learning_rate: 1e-4
lora_rank: 8
lora_alpha: 16
```

### QLoRA 파인튜닝

**특징**:
- 양자화로 인한 정밀도 손실
- 메모리 효율적
- 약간의 불안정성

```yaml
# QLoRA 권장 설정
num_train_epochs: 3-4
learning_rate: 2e-4

# 이유: 양자화 손실을 보상하기 위해 적절한 반복 필요
```

**실제 예시**:
```yaml
finetuning_type: qlora
quantization_bit: 4
num_train_epochs: 3
per_device_train_batch_size: 4
gradient_accumulation_steps: 4
learning_rate: 2e-4
```

### Full Parameter 파인튜닝

**특징**:
- 모든 파라미터 훈련
- 높은 메모리 요구량
- 강력하지만 과적합 위험

```yaml
# Full 파인튜닝 권장 설정
num_train_epochs: 1-2
learning_rate: 1e-5

# 이유: 강력한 변화로 적은 에포크로도 충분, 과적합 주의
```

**실제 예시**:
```yaml
finetuning_type: full
num_train_epochs: 2
per_device_train_batch_size: 2
gradient_accumulation_steps: 8
learning_rate: 1e-5
gradient_checkpointing: true
```

## 🔄 동적 조정 전략

### Early Stopping (조기 종료)

**개념**: 성능이 개선되지 않으면 자동으로 훈련을 중단합니다.

```yaml
# Early stopping 설정
num_train_epochs: 10  # 최대 10 에포크
eval_strategy: epoch
eval_steps: 1  # 매 에포크마다 평가
early_stopping_patience: 3  # 3 에포크 동안 개선 없으면 중단
early_stopping_threshold: 0.01  # 최소 개선 임계값
```

**실제 동작**:
```python
에포크 1: eval_loss = 2.5
에포크 2: eval_loss = 2.3  # 개선됨
에포크 3: eval_loss = 2.35 # 약간 악화 (patience 1/3)
에포크 4: eval_loss = 2.4  # 계속 악화 (patience 2/3)
에포크 5: eval_loss = 2.42 # 계속 악화 (patience 3/3)
→ 훈련 중단 (5 에포크에서 종료)
```

### Learning Rate Scheduling과의 조합

```yaml
# 적응적 에포크 설정
num_train_epochs: 5
lr_scheduler_type: cosine
warmup_steps: 100

# 스케줄러와 에포크의 관계
total_steps = (데이터셋_크기 / 배치_크기) × 5
학습률이 5 에포크에 걸쳐 코사인 곡선으로 감소
```

### 검증 기반 에포크 조정

```yaml
# 검증 손실 기반 동적 조정
num_train_epochs: 8  # 최대값
eval_strategy: steps
eval_steps: 100
save_strategy: steps  
save_steps: 100
load_best_model_at_end: true  # 가장 좋은 모델 로드

# 실제 동작: 검증 성능이 가장 좋았던 지점의 모델 사용
```

## 📈 실무 사례 분석

### 사례 1: ITEASY 고객 지원 챗봇

**데이터셋**: 3,000개 고객 문의-응답 쌍

**1차 시도 (과적합)**:
```yaml
num_train_epochs: 10  # 너무 많은 에포크
per_device_train_batch_size: 2
learning_rate: 1e-4

결과:
- 훈련 손실: 0.1 (매우 낮음)
- 검증 손실: 2.5 (매우 높음)
- 실제 응답: 훈련 데이터 암기, 일반화 실패
```

**최종 성공 설정**:
```yaml
num_train_epochs: 4
per_device_train_batch_size: 4
gradient_accumulation_steps: 2
learning_rate: 5e-5
early_stopping_patience: 2

# 계산
effective_batch_size = 4 × 2 × 1 = 8
steps_per_epoch = 3000 / 8 = 375
total_steps = 375 × 4 = 1500 (실제로는 early stopping으로 더 적음)

결과:
- 3 에포크에서 early stopping 발생
- 훈련 손실: 0.8, 검증 손실: 0.9
- 실제 성능: 자연스럽고 적절한 응답 생성
```

### 사례 2: 기술 문서 요약 모델

**데이터셋**: 25,000개 기술 문서

**실험 A (부족한 훈련)**:
```yaml
num_train_epochs: 1
per_device_train_batch_size: 8
learning_rate: 3e-5

결과: ROUGE-L = 0.35 (미흡한 성능)
```

**실험 B (적절한 훈련)**:
```yaml  
num_train_epochs: 3
per_device_train_batch_size: 8
learning_rate: 3e-5

결과: ROUGE-L = 0.47 (만족스러운 성능)
```

**실험 C (과훈련)**:
```yaml
num_train_epochs: 8
per_device_train_batch_size: 8
learning_rate: 3e-5

결과: ROUGE-L = 0.44 (오히려 성능 하락)
```

**최종 선택**: 실험 B (3 에포크)

### 사례 3: 빠른 프로토타이핑

**목표**: 30분 내에 기본 성능 확인

**설정**:
```yaml
max_steps: 200  # 에포크 대신 스텝으로 제한
per_device_train_batch_size: 4
learning_rate: 1e-4
max_samples: 1000  # 작은 데이터셋으로 제한

# 계산
배치 크기: 4
스텝당 처리 샘플: 4
200 스텝 = 800개 샘플 처리
1000개 데이터셋에서 0.8 에포크 상당

결과: 25분만에 기본 동작 확인 완료
```

## 🚨 흔한 실수와 해결방법

### 실수 1: 과도한 에포크 설정

**문제**:
```yaml
num_train_epochs: 20  # 너무 많음
데이터셋 크기: 1,000개 (소규모)
```

**증상**:
- 훈련 손실은 계속 감소하지만 검증 손실은 증가
- 모델이 훈련 데이터를 암기
- 새로운 데이터에서 성능 저하

**해결**:
```yaml
num_train_epochs: 5
early_stopping_patience: 2
eval_strategy: epoch
```

### 실수 2: 불충분한 훈련

**문제**:
```yaml
num_train_epochs: 1
데이터셋 크기: 50,000개 (대규모)
learning_rate: 1e-5  # 작은 학습률
```

**증상**:
- 손실이 충분히 감소하지 않음
- 모델 성능이 기대에 못 미침
- 학습 곡선이 아직 수렴하지 않은 상태

**해결**:
```yaml
num_train_epochs: 3
learning_rate: 5e-5  # 적절한 학습률로 조정
```

### 실수 3: 배치 크기와 에포크 불일치

**문제**:
```yaml
per_device_train_batch_size: 1  # 매우 작은 배치
gradient_accumulation_steps: 1
num_train_epochs: 2
데이터셋 크기: 10,000개
```

**문제점**:
```python
# 계산
steps_per_epoch = 10,000 / 1 = 10,000 스텝
total_steps = 10,000 × 2 = 20,000 스텝
→ 너무 많은 스텝으로 비효율적
```

**해결**:
```yaml
per_device_train_batch_size: 8
gradient_accumulation_steps: 2
num_train_epochs: 2

# 개선된 계산
effective_batch_size = 8 × 2 = 16
steps_per_epoch = 10,000 / 16 = 625
total_steps = 625 × 2 = 1,250 스텝 (적절함)
```

## 🔧 모니터링 및 디버깅

### 훈련 진행 상황 모니터링

```yaml
# 상세 로깅 설정
logging_steps: 50  # 50 스텝마다 로그
eval_steps: 200    # 200 스텝마다 평가
save_steps: 200    # 200 스텝마다 저장
report_to: tensorboard
```

**관찰할 지표들**:
```python
# TensorBoard/로그에서 확인
1. train_loss: 훈련 손실 (감소해야 함)
2. eval_loss: 검증 손실 (train_loss와 비슷하게 감소)
3. learning_rate: 현재 학습률
4. epoch: 현재 에포크
5. global_step: 현재 전역 스텝

# 정상적인 패턴
- train_loss와 eval_loss가 비슷하게 감소
- 에포크가 진행될수록 안정적인 개선
- 학습률이 스케줄러에 따라 적절히 조정
```

### 과적합 감지

```python
# 과적합 신호들
1. train_loss는 감소하지만 eval_loss는 증가
2. train_loss와 eval_loss의 격차가 계속 벌어짐
3. 에포크 후반부에 검증 성능이 악화

# 대응 방법
- early_stopping 사용
- 에포크 수 줄이기
- 정규화 기법 추가
- 더 많은 데이터 확보
```

### 부족한 훈련 감지

```python
# 부족한 훈련 신호들
1. 손실이 아직 수렴하지 않고 계속 감소 중
2. 학습 곡선이 평평해지지 않음
3. 검증 성능이 계속 개선되고 있음

# 대응 방법
- 에포크 수 늘리기
- 학습률 조정
- 더 긴 훈련 시간 확보
```

## 💡 고급 최적화 기법

### 1. 점진적 에포크 증가

```yaml
# Phase 1: 빠른 탐색
num_train_epochs: 1
learning_rate: 1e-4

# Phase 2: 성능 확인 후 세밀 조정
num_train_epochs: 3  # 추가 2 에포크
learning_rate: 5e-5
load_best_model_at_end: true
```

### 2. 다단계 훈련

```yaml
# Stage 1: 기본 적응
num_train_epochs: 2
max_samples: 50000
learning_rate: 1e-4

# Stage 2: 세밀한 튜닝
num_train_epochs: 3
max_samples: 100000  # 전체 데이터
learning_rate: 3e-5
```

### 3. 적응적 배치 크기

```python
# 에포크별 동적 조정 (의사코드)
epoch 1-2: small_batch_size (안정성)
epoch 3-4: medium_batch_size (효율성)  
epoch 5+: large_batch_size (빠른 수렴)
```

### 4. 실시간 성능 기반 조정

```yaml
# 성능 기반 자동 조정
eval_steps: 100
early_stopping_patience: 3
reduce_lr_on_plateau: true
patience: 2
factor: 0.5

# 동작:
# 성능 정체 → 학습률 감소
# 계속 정체 → early stopping
```

## 📋 상황별 권장 설정

### 🚀 처음 사용자 (안전한 설정)

```yaml
num_train_epochs: 3
per_device_train_batch_size: 4
gradient_accumulation_steps: 2
early_stopping_patience: 2
eval_strategy: epoch
```

### ⚡ 빠른 실험 (시간 제약)

```yaml
max_steps: 500  # 에포크 대신 스텝 제한
per_device_train_batch_size: 8
max_samples: 5000  # 데이터도 제한
```

### 🎯 프로덕션 (최고 품질)

```yaml
num_train_epochs: 5
per_device_train_batch_size: 8
gradient_accumulation_steps: 2
early_stopping_patience: 3
eval_steps: 100
save_strategy: steps
save_steps: 100
load_best_model_at_end: true
```

### 🔬 연구 목적 (완전한 제어)

```yaml
max_steps: 2000  # 정확한 스텝 제어
eval_steps: 100
save_steps: 100
logging_steps: 10
report_to: tensorboard
use_swanlab: true
```

## 📊 최종 권장 공식

### 기본 가이드라인

```python
# 에포크 수 결정 공식
if 데이터셋_크기 < 1000:
    num_train_epochs = 5-8
elif 데이터셋_크기 < 10000:
    num_train_epochs = 3-5
else:  # > 10000
    num_train_epochs = 2-3

# 스텝 기반 제어가 필요한 경우
if 시간_제약 or 정확한_제어_필요:
    max_steps = 원하는_스텝_수
    num_train_epochs = None  # 무시됨
```

### 검증 방법

```python
# 적절한 에포크 수인지 확인
1. 훈련/검증 손실 그래프 관찰
2. Early stopping이 작동하는지 확인
3. 최종 성능이 만족스러운지 평가
4. 과적합/부족한 훈련 신호 감지
```

에포크와 스텝 설정은 모델 훈련의 핵심입니다. 적절한 설정을 통해 효율적이고 효과적인 파인튜닝을 달성할 수 있습니다!

