---
title: í•˜ì´í¼íŒŒë¼ë¯¸í„° ì´í•´í•˜ê¸° - 03
date: 2025-08-17 10:57:59 +0900
categories: [artificial intelligence, machine learning]
tags: [machine learning, deep learning, hyperparameters, finetuning, llm, llamafactory, pytorch, optimization, training, epochs, ai]     # TAG names should always be lowercase
---

# ì—í¬í¬(Epochs) ë° ìŠ¤í…(Steps) ì´í•´í•˜ê¸°

## ğŸ“š ê°œë… ì´í•´

### ì—í¬í¬(Epoch)ë€?

**ì—í¬í¬**ëŠ” ì „ì²´ í›ˆë ¨ ë°ì´í„°ì…‹ì„ í•œ ë²ˆ ì™„ì „íˆ ìˆœíšŒí•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

```python
# ê¸°ë³¸ ê°œë…
ë°ì´í„°ì…‹ í¬ê¸°: 10,000ê°œ ìƒ˜í”Œ
1 ì—í¬í¬ = ëª¨ë“  10,000ê°œ ìƒ˜í”Œì„ í•œ ë²ˆì”© ì‚¬ìš©í•˜ì—¬ í›ˆë ¨

1 ì—í¬í¬ ì™„ë£Œ = ëª¨ë¸ì´ ëª¨ë“  ë°ì´í„°ë¥¼ í•œ ë²ˆì”© "ë³¸" ìƒíƒœ
```

### ìŠ¤í…(Step)ì´ë€?

**ìŠ¤í…**ì€ í•˜ë‚˜ì˜ ë°°ì¹˜(batch)ë¡œ í•œ ë²ˆì˜ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ë¥¼ ìˆ˜í–‰í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

```python
# ê¸°ë³¸ ê°œë…
ë°°ì¹˜ í¬ê¸°: 8ê°œ ìƒ˜í”Œ
1 ìŠ¤í… = 8ê°œ ìƒ˜í”Œë¡œ forward â†’ backward â†’ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸

# ê´€ê³„ì‹
ì´ ìŠ¤í… ìˆ˜ = (ë°ì´í„°ì…‹ í¬ê¸° / ë°°ì¹˜ í¬ê¸°) Ã— ì—í¬í¬ ìˆ˜
```

### ì‹¤ì œ ê³„ì‚° ì˜ˆì‹œ

```python
# ì˜ˆì‹œ ì„¤ì •
ë°ì´í„°ì…‹ í¬ê¸°: 10,000ê°œ
per_device_train_batch_size: 4
gradient_accumulation_steps: 2
GPU ê°œìˆ˜: 2
num_train_epochs: 3

# ì‹¤ì œ ë°°ì¹˜ í¬ê¸° ê³„ì‚°
effective_batch_size = per_device_train_batch_size Ã— gradient_accumulation_steps Ã— GPUê°œìˆ˜
effective_batch_size = 4 Ã— 2 Ã— 2 = 16

# ì—í¬í¬ë‹¹ ìŠ¤í… ìˆ˜
steps_per_epoch = ë°ì´í„°ì…‹_í¬ê¸° / effective_batch_size
steps_per_epoch = 10,000 / 16 = 625

# ì´ í›ˆë ¨ ìŠ¤í… ìˆ˜
total_training_steps = steps_per_epoch Ã— num_train_epochs
total_training_steps = 625 Ã— 3 = 1,875
```

## âš™ï¸ LLaMA Factory ì„¤ì • ì˜µì…˜

### num_train_epochs (í›ˆë ¨ ì—í¬í¬ ìˆ˜)

**ì„¤ëª…**: ì „ì²´ ë°ì´í„°ì…‹ì„ ëª‡ ë²ˆ ë°˜ë³µí• ì§€ ê²°ì •í•©ë‹ˆë‹¤.

```yaml
num_train_epochs: 3.0  # ê¸°ë³¸ê°’: 3.0
```

**ì†Œìˆ˜ì  ì‚¬ìš© ê°€ëŠ¥**:
```yaml
num_train_epochs: 2.5  # 2.5 ì—í¬í¬ (ì „ì²´ ë°ì´í„°ì˜ 2.5ë°°)
num_train_epochs: 1.2  # 1.2 ì—í¬í¬ (ì „ì²´ ë°ì´í„°ì˜ 1.2ë°°)
```

**ì‹¤ì œ ë™ì‘**:
```python
# num_train_epochs: 2.5ì¸ ê²½ìš°
ì—í¬í¬ 1: ëª¨ë“  ë°ì´í„° ì‚¬ìš© (100%)
ì—í¬í¬ 2: ëª¨ë“  ë°ì´í„° ì‚¬ìš© (100%)  
ì—í¬í¬ 3: ë°ì´í„°ì˜ 50%ë§Œ ì‚¬ìš©
ì´ í›ˆë ¨ëŸ‰: ì „ì²´ ë°ì´í„°ì˜ 2.5ë°°
```

### max_steps (ìµœëŒ€ ìŠ¤í… ìˆ˜)

**ì„¤ëª…**: í›ˆë ¨í•  ìµœëŒ€ ìŠ¤í… ìˆ˜ë¥¼ ì§ì ‘ ì§€ì •í•©ë‹ˆë‹¤. `num_train_epochs`ë³´ë‹¤ ìš°ì„ ë©ë‹ˆë‹¤.

```yaml
max_steps: 1000  # ì •í™•íˆ 1000 ìŠ¤í…ë§Œ í›ˆë ¨
# num_train_epochs ì„¤ì •ì€ ë¬´ì‹œë¨
```

**ìš°ì„ ìˆœìœ„**:
```yaml
# Case 1: max_stepsê°€ ì„¤ì •ëœ ê²½ìš°
max_steps: 500
num_train_epochs: 10  # ì´ ì„¤ì •ì€ ë¬´ì‹œë¨
# ê²°ê³¼: 500 ìŠ¤í… í›„ í›ˆë ¨ ì¢…ë£Œ

# Case 2: max_stepsê°€ -1 ë˜ëŠ” ë¯¸ì„¤ì •
max_steps: -1  # ë˜ëŠ” ì„¤ì •í•˜ì§€ ì•ŠìŒ
num_train_epochs: 3
# ê²°ê³¼: 3 ì—í¬í¬ ì™„ë£Œê¹Œì§€ í›ˆë ¨
```

### ì‹¤ì œ ì„ íƒ ê¸°ì¤€

#### ì—í¬í¬ ê¸°ë°˜ í›ˆë ¨ (ê¶Œì¥)
```yaml
num_train_epochs: 3
max_steps: -1  # ë˜ëŠ” ì„¤ì •í•˜ì§€ ì•ŠìŒ

ì¥ì :
- ì§ê´€ì ì´ê³  ì´í•´í•˜ê¸° ì‰¬ì›€
- ë°ì´í„°ì…‹ í¬ê¸°ê°€ ë°”ë€Œì–´ë„ ì¼ê´€ëœ í›ˆë ¨ëŸ‰
- ì¼ë°˜ì ì¸ ê´€ë¡€
```

#### ìŠ¤í… ê¸°ë°˜ í›ˆë ¨
```yaml
max_steps: 1000
# num_train_epochsëŠ” ì„¤ì •í•˜ì§€ ì•Šê±°ë‚˜ ë¬´ì‹œë¨

ì¥ì :
- ì •í™•í•œ í›ˆë ¨ëŸ‰ ì œì–´
- ì‹œê°„ ì œì•½ì´ ìˆëŠ” ì‹¤í—˜
- ì—°êµ¬/ë¹„êµ ì‹¤í—˜ì—ì„œ ì •í™•í•œ ì œì–´ í•„ìš”
```

## ğŸ“Š ë°ì´í„°ì…‹ í¬ê¸°ë³„ ê¶Œì¥ ì„¤ì •

### ì†Œê·œëª¨ ë°ì´í„°ì…‹ (< 1,000 ìƒ˜í”Œ)

```yaml
# ì„¤ì • ì˜ˆì‹œ
num_train_epochs: 5-10
max_samples: 1000

# ì´ìœ ì™€ ê³„ì‚°
ë°ì´í„°ê°€ ì ì–´ì„œ ë§ì€ ë°˜ë³µ í•„ìš”
ê³¼ì í•© ì£¼ì˜ê°€ í•„ìš”í•˜ì§€ë§Œ ì¶©ë¶„í•œ í•™ìŠµ ì‹œê°„ í™•ë³´

# ì˜ˆì‹œ ê³„ì‚°
ë°ì´í„°ì…‹: 500ê°œ
ë°°ì¹˜ í¬ê¸°: 8
steps_per_epoch = 500/8 = 62.5 â‰ˆ 63
5 ì—í¬í¬ = 63 Ã— 5 = 315 ìŠ¤í…
```

**ì‹¤ë¬´ ì„¤ì •**:
```yaml
# ì†Œê·œëª¨ ê³ ê° FAQ ë°ì´í„°
num_train_epochs: 8
per_device_train_batch_size: 4
learning_rate: 1e-4
early_stopping_patience: 3  # ê³¼ì í•© ë°©ì§€
```

### ì¤‘ê°„ ê·œëª¨ ë°ì´í„°ì…‹ (1,000-10,000 ìƒ˜í”Œ)

```yaml
# ì„¤ì • ì˜ˆì‹œ  
num_train_epochs: 3-5
max_samples: 10000

# ì˜ˆì‹œ ê³„ì‚°
ë°ì´í„°ì…‹: 5,000ê°œ
ë°°ì¹˜ í¬ê¸°: 16 (4Ã—2Ã—2)
steps_per_epoch = 5000/16 = 312.5 â‰ˆ 313
3 ì—í¬í¬ = 313 Ã— 3 = 939 ìŠ¤í…
```

**ì‹¤ë¬´ ì„¤ì •**:
```yaml
# ì¼ë°˜ì ì¸ ì—…ë¬´ìš© ë°ì´í„°
num_train_epochs: 3
per_device_train_batch_size: 8
gradient_accumulation_steps: 2
learning_rate: 5e-5
```

### ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ (> 10,000 ìƒ˜í”Œ)

```yaml
# ì„¤ì • ì˜ˆì‹œ
num_train_epochs: 1-3  
max_samples: 100000

# ì˜ˆì‹œ ê³„ì‚°
ë°ì´í„°ì…‹: 50,000ê°œ
ë°°ì¹˜ í¬ê¸°: 32 (8Ã—2Ã—2)  
steps_per_epoch = 50000/32 = 1562.5 â‰ˆ 1563
2 ì—í¬í¬ = 1563 Ã— 2 = 3126 ìŠ¤í…
```

**ì‹¤ë¬´ ì„¤ì •**:
```yaml
# ëŒ€ê·œëª¨ ê¸°ìˆ  ë¬¸ì„œ ë°ì´í„°
num_train_epochs: 2
per_device_train_batch_size: 8
gradient_accumulation_steps: 4
learning_rate: 3e-5
```

## ğŸ¯ íŒŒì¸íŠœë‹ ë°©ë²•ë³„ ê¶Œì¥ì‚¬í•­

### LoRA íŒŒì¸íŠœë‹

**íŠ¹ì§•**: 
- ì ì€ íŒŒë¼ë¯¸í„°ë§Œ í›ˆë ¨
- ìƒëŒ€ì ìœ¼ë¡œ ì•ˆì •ì 
- ë¹ ë¥¸ ìˆ˜ë ´ ê°€ëŠ¥

```yaml
# LoRA ê¶Œì¥ ì„¤ì •
num_train_epochs: 3-5
learning_rate: 1e-4

# ì´ìœ : LoRAëŠ” íš¨ìœ¨ì ì´ë¼ ë” ë§ì€ ì—í¬í¬ë¡œ ì„¸ë°€í•œ ì¡°ì • ê°€ëŠ¥
```

**ì‹¤ì œ ì˜ˆì‹œ**:
```yaml
finetuning_type: lora
num_train_epochs: 4
per_device_train_batch_size: 8
gradient_accumulation_steps: 2
learning_rate: 1e-4
lora_rank: 8
lora_alpha: 16
```

### QLoRA íŒŒì¸íŠœë‹

**íŠ¹ì§•**:
- ì–‘ìí™”ë¡œ ì¸í•œ ì •ë°€ë„ ì†ì‹¤
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
- ì•½ê°„ì˜ ë¶ˆì•ˆì •ì„±

```yaml
# QLoRA ê¶Œì¥ ì„¤ì •
num_train_epochs: 3-4
learning_rate: 2e-4

# ì´ìœ : ì–‘ìí™” ì†ì‹¤ì„ ë³´ìƒí•˜ê¸° ìœ„í•´ ì ì ˆí•œ ë°˜ë³µ í•„ìš”
```

**ì‹¤ì œ ì˜ˆì‹œ**:
```yaml
finetuning_type: qlora
quantization_bit: 4
num_train_epochs: 3
per_device_train_batch_size: 4
gradient_accumulation_steps: 4
learning_rate: 2e-4
```

### Full Parameter íŒŒì¸íŠœë‹

**íŠ¹ì§•**:
- ëª¨ë“  íŒŒë¼ë¯¸í„° í›ˆë ¨
- ë†’ì€ ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰
- ê°•ë ¥í•˜ì§€ë§Œ ê³¼ì í•© ìœ„í—˜

```yaml
# Full íŒŒì¸íŠœë‹ ê¶Œì¥ ì„¤ì •
num_train_epochs: 1-2
learning_rate: 1e-5

# ì´ìœ : ê°•ë ¥í•œ ë³€í™”ë¡œ ì ì€ ì—í¬í¬ë¡œë„ ì¶©ë¶„, ê³¼ì í•© ì£¼ì˜
```

**ì‹¤ì œ ì˜ˆì‹œ**:
```yaml
finetuning_type: full
num_train_epochs: 2
per_device_train_batch_size: 2
gradient_accumulation_steps: 8
learning_rate: 1e-5
gradient_checkpointing: true
```

## ğŸ”„ ë™ì  ì¡°ì • ì „ëµ

### Early Stopping (ì¡°ê¸° ì¢…ë£Œ)

**ê°œë…**: ì„±ëŠ¥ì´ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ ìë™ìœ¼ë¡œ í›ˆë ¨ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.

```yaml
# Early stopping ì„¤ì •
num_train_epochs: 10  # ìµœëŒ€ 10 ì—í¬í¬
eval_strategy: epoch
eval_steps: 1  # ë§¤ ì—í¬í¬ë§ˆë‹¤ í‰ê°€
early_stopping_patience: 3  # 3 ì—í¬í¬ ë™ì•ˆ ê°œì„  ì—†ìœ¼ë©´ ì¤‘ë‹¨
early_stopping_threshold: 0.01  # ìµœì†Œ ê°œì„  ì„ê³„ê°’
```

**ì‹¤ì œ ë™ì‘**:
```python
ì—í¬í¬ 1: eval_loss = 2.5
ì—í¬í¬ 2: eval_loss = 2.3  # ê°œì„ ë¨
ì—í¬í¬ 3: eval_loss = 2.35 # ì•½ê°„ ì•…í™” (patience 1/3)
ì—í¬í¬ 4: eval_loss = 2.4  # ê³„ì† ì•…í™” (patience 2/3)
ì—í¬í¬ 5: eval_loss = 2.42 # ê³„ì† ì•…í™” (patience 3/3)
â†’ í›ˆë ¨ ì¤‘ë‹¨ (5 ì—í¬í¬ì—ì„œ ì¢…ë£Œ)
```

### Learning Rate Schedulingê³¼ì˜ ì¡°í•©

```yaml
# ì ì‘ì  ì—í¬í¬ ì„¤ì •
num_train_epochs: 5
lr_scheduler_type: cosine
warmup_steps: 100

# ìŠ¤ì¼€ì¤„ëŸ¬ì™€ ì—í¬í¬ì˜ ê´€ê³„
total_steps = (ë°ì´í„°ì…‹_í¬ê¸° / ë°°ì¹˜_í¬ê¸°) Ã— 5
í•™ìŠµë¥ ì´ 5 ì—í¬í¬ì— ê±¸ì³ ì½”ì‚¬ì¸ ê³¡ì„ ìœ¼ë¡œ ê°ì†Œ
```

### ê²€ì¦ ê¸°ë°˜ ì—í¬í¬ ì¡°ì •

```yaml
# ê²€ì¦ ì†ì‹¤ ê¸°ë°˜ ë™ì  ì¡°ì •
num_train_epochs: 8  # ìµœëŒ€ê°’
eval_strategy: steps
eval_steps: 100
save_strategy: steps  
save_steps: 100
load_best_model_at_end: true  # ê°€ì¥ ì¢‹ì€ ëª¨ë¸ ë¡œë“œ

# ì‹¤ì œ ë™ì‘: ê²€ì¦ ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ì•˜ë˜ ì§€ì ì˜ ëª¨ë¸ ì‚¬ìš©
```

## ğŸ“ˆ ì‹¤ë¬´ ì‚¬ë¡€ ë¶„ì„

### ì‚¬ë¡€ 1: ITEASY ê³ ê° ì§€ì› ì±—ë´‡

**ë°ì´í„°ì…‹**: 3,000ê°œ ê³ ê° ë¬¸ì˜-ì‘ë‹µ ìŒ

**1ì°¨ ì‹œë„ (ê³¼ì í•©)**:
```yaml
num_train_epochs: 10  # ë„ˆë¬´ ë§ì€ ì—í¬í¬
per_device_train_batch_size: 2
learning_rate: 1e-4

ê²°ê³¼:
- í›ˆë ¨ ì†ì‹¤: 0.1 (ë§¤ìš° ë‚®ìŒ)
- ê²€ì¦ ì†ì‹¤: 2.5 (ë§¤ìš° ë†’ìŒ)
- ì‹¤ì œ ì‘ë‹µ: í›ˆë ¨ ë°ì´í„° ì•”ê¸°, ì¼ë°˜í™” ì‹¤íŒ¨
```

**ìµœì¢… ì„±ê³µ ì„¤ì •**:
```yaml
num_train_epochs: 4
per_device_train_batch_size: 4
gradient_accumulation_steps: 2
learning_rate: 5e-5
early_stopping_patience: 2

# ê³„ì‚°
effective_batch_size = 4 Ã— 2 Ã— 1 = 8
steps_per_epoch = 3000 / 8 = 375
total_steps = 375 Ã— 4 = 1500 (ì‹¤ì œë¡œëŠ” early stoppingìœ¼ë¡œ ë” ì ìŒ)

ê²°ê³¼:
- 3 ì—í¬í¬ì—ì„œ early stopping ë°œìƒ
- í›ˆë ¨ ì†ì‹¤: 0.8, ê²€ì¦ ì†ì‹¤: 0.9
- ì‹¤ì œ ì„±ëŠ¥: ìì—°ìŠ¤ëŸ½ê³  ì ì ˆí•œ ì‘ë‹µ ìƒì„±
```

### ì‚¬ë¡€ 2: ê¸°ìˆ  ë¬¸ì„œ ìš”ì•½ ëª¨ë¸

**ë°ì´í„°ì…‹**: 25,000ê°œ ê¸°ìˆ  ë¬¸ì„œ

**ì‹¤í—˜ A (ë¶€ì¡±í•œ í›ˆë ¨)**:
```yaml
num_train_epochs: 1
per_device_train_batch_size: 8
learning_rate: 3e-5

ê²°ê³¼: ROUGE-L = 0.35 (ë¯¸í¡í•œ ì„±ëŠ¥)
```

**ì‹¤í—˜ B (ì ì ˆí•œ í›ˆë ¨)**:
```yaml  
num_train_epochs: 3
per_device_train_batch_size: 8
learning_rate: 3e-5

ê²°ê³¼: ROUGE-L = 0.47 (ë§Œì¡±ìŠ¤ëŸ¬ìš´ ì„±ëŠ¥)
```

**ì‹¤í—˜ C (ê³¼í›ˆë ¨)**:
```yaml
num_train_epochs: 8
per_device_train_batch_size: 8
learning_rate: 3e-5

ê²°ê³¼: ROUGE-L = 0.44 (ì˜¤íˆë ¤ ì„±ëŠ¥ í•˜ë½)
```

**ìµœì¢… ì„ íƒ**: ì‹¤í—˜ B (3 ì—í¬í¬)

### ì‚¬ë¡€ 3: ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘

**ëª©í‘œ**: 30ë¶„ ë‚´ì— ê¸°ë³¸ ì„±ëŠ¥ í™•ì¸

**ì„¤ì •**:
```yaml
max_steps: 200  # ì—í¬í¬ ëŒ€ì‹  ìŠ¤í…ìœ¼ë¡œ ì œí•œ
per_device_train_batch_size: 4
learning_rate: 1e-4
max_samples: 1000  # ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ ì œí•œ

# ê³„ì‚°
ë°°ì¹˜ í¬ê¸°: 4
ìŠ¤í…ë‹¹ ì²˜ë¦¬ ìƒ˜í”Œ: 4
200 ìŠ¤í… = 800ê°œ ìƒ˜í”Œ ì²˜ë¦¬
1000ê°œ ë°ì´í„°ì…‹ì—ì„œ 0.8 ì—í¬í¬ ìƒë‹¹

ê²°ê³¼: 25ë¶„ë§Œì— ê¸°ë³¸ ë™ì‘ í™•ì¸ ì™„ë£Œ
```

## ğŸš¨ í”í•œ ì‹¤ìˆ˜ì™€ í•´ê²°ë°©ë²•

### ì‹¤ìˆ˜ 1: ê³¼ë„í•œ ì—í¬í¬ ì„¤ì •

**ë¬¸ì œ**:
```yaml
num_train_epochs: 20  # ë„ˆë¬´ ë§ìŒ
ë°ì´í„°ì…‹ í¬ê¸°: 1,000ê°œ (ì†Œê·œëª¨)
```

**ì¦ìƒ**:
- í›ˆë ¨ ì†ì‹¤ì€ ê³„ì† ê°ì†Œí•˜ì§€ë§Œ ê²€ì¦ ì†ì‹¤ì€ ì¦ê°€
- ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ë¥¼ ì•”ê¸°
- ìƒˆë¡œìš´ ë°ì´í„°ì—ì„œ ì„±ëŠ¥ ì €í•˜

**í•´ê²°**:
```yaml
num_train_epochs: 5
early_stopping_patience: 2
eval_strategy: epoch
```

### ì‹¤ìˆ˜ 2: ë¶ˆì¶©ë¶„í•œ í›ˆë ¨

**ë¬¸ì œ**:
```yaml
num_train_epochs: 1
ë°ì´í„°ì…‹ í¬ê¸°: 50,000ê°œ (ëŒ€ê·œëª¨)
learning_rate: 1e-5  # ì‘ì€ í•™ìŠµë¥ 
```

**ì¦ìƒ**:
- ì†ì‹¤ì´ ì¶©ë¶„íˆ ê°ì†Œí•˜ì§€ ì•ŠìŒ
- ëª¨ë¸ ì„±ëŠ¥ì´ ê¸°ëŒ€ì— ëª» ë¯¸ì¹¨
- í•™ìŠµ ê³¡ì„ ì´ ì•„ì§ ìˆ˜ë ´í•˜ì§€ ì•Šì€ ìƒíƒœ

**í•´ê²°**:
```yaml
num_train_epochs: 3
learning_rate: 5e-5  # ì ì ˆí•œ í•™ìŠµë¥ ë¡œ ì¡°ì •
```

### ì‹¤ìˆ˜ 3: ë°°ì¹˜ í¬ê¸°ì™€ ì—í¬í¬ ë¶ˆì¼ì¹˜

**ë¬¸ì œ**:
```yaml
per_device_train_batch_size: 1  # ë§¤ìš° ì‘ì€ ë°°ì¹˜
gradient_accumulation_steps: 1
num_train_epochs: 2
ë°ì´í„°ì…‹ í¬ê¸°: 10,000ê°œ
```

**ë¬¸ì œì **:
```python
# ê³„ì‚°
steps_per_epoch = 10,000 / 1 = 10,000 ìŠ¤í…
total_steps = 10,000 Ã— 2 = 20,000 ìŠ¤í…
â†’ ë„ˆë¬´ ë§ì€ ìŠ¤í…ìœ¼ë¡œ ë¹„íš¨ìœ¨ì 
```

**í•´ê²°**:
```yaml
per_device_train_batch_size: 8
gradient_accumulation_steps: 2
num_train_epochs: 2

# ê°œì„ ëœ ê³„ì‚°
effective_batch_size = 8 Ã— 2 = 16
steps_per_epoch = 10,000 / 16 = 625
total_steps = 625 Ã— 2 = 1,250 ìŠ¤í… (ì ì ˆí•¨)
```

## ğŸ”§ ëª¨ë‹ˆí„°ë§ ë° ë””ë²„ê¹…

### í›ˆë ¨ ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§

```yaml
# ìƒì„¸ ë¡œê¹… ì„¤ì •
logging_steps: 50  # 50 ìŠ¤í…ë§ˆë‹¤ ë¡œê·¸
eval_steps: 200    # 200 ìŠ¤í…ë§ˆë‹¤ í‰ê°€
save_steps: 200    # 200 ìŠ¤í…ë§ˆë‹¤ ì €ì¥
report_to: tensorboard
```

**ê´€ì°°í•  ì§€í‘œë“¤**:
```python
# TensorBoard/ë¡œê·¸ì—ì„œ í™•ì¸
1. train_loss: í›ˆë ¨ ì†ì‹¤ (ê°ì†Œí•´ì•¼ í•¨)
2. eval_loss: ê²€ì¦ ì†ì‹¤ (train_lossì™€ ë¹„ìŠ·í•˜ê²Œ ê°ì†Œ)
3. learning_rate: í˜„ì¬ í•™ìŠµë¥ 
4. epoch: í˜„ì¬ ì—í¬í¬
5. global_step: í˜„ì¬ ì „ì—­ ìŠ¤í…

# ì •ìƒì ì¸ íŒ¨í„´
- train_lossì™€ eval_lossê°€ ë¹„ìŠ·í•˜ê²Œ ê°ì†Œ
- ì—í¬í¬ê°€ ì§„í–‰ë ìˆ˜ë¡ ì•ˆì •ì ì¸ ê°œì„ 
- í•™ìŠµë¥ ì´ ìŠ¤ì¼€ì¤„ëŸ¬ì— ë”°ë¼ ì ì ˆíˆ ì¡°ì •
```

### ê³¼ì í•© ê°ì§€

```python
# ê³¼ì í•© ì‹ í˜¸ë“¤
1. train_lossëŠ” ê°ì†Œí•˜ì§€ë§Œ eval_lossëŠ” ì¦ê°€
2. train_lossì™€ eval_lossì˜ ê²©ì°¨ê°€ ê³„ì† ë²Œì–´ì§
3. ì—í¬í¬ í›„ë°˜ë¶€ì— ê²€ì¦ ì„±ëŠ¥ì´ ì•…í™”

# ëŒ€ì‘ ë°©ë²•
- early_stopping ì‚¬ìš©
- ì—í¬í¬ ìˆ˜ ì¤„ì´ê¸°
- ì •ê·œí™” ê¸°ë²• ì¶”ê°€
- ë” ë§ì€ ë°ì´í„° í™•ë³´
```

### ë¶€ì¡±í•œ í›ˆë ¨ ê°ì§€

```python
# ë¶€ì¡±í•œ í›ˆë ¨ ì‹ í˜¸ë“¤
1. ì†ì‹¤ì´ ì•„ì§ ìˆ˜ë ´í•˜ì§€ ì•Šê³  ê³„ì† ê°ì†Œ ì¤‘
2. í•™ìŠµ ê³¡ì„ ì´ í‰í‰í•´ì§€ì§€ ì•ŠìŒ
3. ê²€ì¦ ì„±ëŠ¥ì´ ê³„ì† ê°œì„ ë˜ê³  ìˆìŒ

# ëŒ€ì‘ ë°©ë²•
- ì—í¬í¬ ìˆ˜ ëŠ˜ë¦¬ê¸°
- í•™ìŠµë¥  ì¡°ì •
- ë” ê¸´ í›ˆë ¨ ì‹œê°„ í™•ë³´
```

## ğŸ’¡ ê³ ê¸‰ ìµœì í™” ê¸°ë²•

### 1. ì ì§„ì  ì—í¬í¬ ì¦ê°€

```yaml
# Phase 1: ë¹ ë¥¸ íƒìƒ‰
num_train_epochs: 1
learning_rate: 1e-4

# Phase 2: ì„±ëŠ¥ í™•ì¸ í›„ ì„¸ë°€ ì¡°ì •
num_train_epochs: 3  # ì¶”ê°€ 2 ì—í¬í¬
learning_rate: 5e-5
load_best_model_at_end: true
```

### 2. ë‹¤ë‹¨ê³„ í›ˆë ¨

```yaml
# Stage 1: ê¸°ë³¸ ì ì‘
num_train_epochs: 2
max_samples: 50000
learning_rate: 1e-4

# Stage 2: ì„¸ë°€í•œ íŠœë‹
num_train_epochs: 3
max_samples: 100000  # ì „ì²´ ë°ì´í„°
learning_rate: 3e-5
```

### 3. ì ì‘ì  ë°°ì¹˜ í¬ê¸°

```python
# ì—í¬í¬ë³„ ë™ì  ì¡°ì • (ì˜ì‚¬ì½”ë“œ)
epoch 1-2: small_batch_size (ì•ˆì •ì„±)
epoch 3-4: medium_batch_size (íš¨ìœ¨ì„±)  
epoch 5+: large_batch_size (ë¹ ë¥¸ ìˆ˜ë ´)
```

### 4. ì‹¤ì‹œê°„ ì„±ëŠ¥ ê¸°ë°˜ ì¡°ì •

```yaml
# ì„±ëŠ¥ ê¸°ë°˜ ìë™ ì¡°ì •
eval_steps: 100
early_stopping_patience: 3
reduce_lr_on_plateau: true
patience: 2
factor: 0.5

# ë™ì‘:
# ì„±ëŠ¥ ì •ì²´ â†’ í•™ìŠµë¥  ê°ì†Œ
# ê³„ì† ì •ì²´ â†’ early stopping
```

## ğŸ“‹ ìƒí™©ë³„ ê¶Œì¥ ì„¤ì •

### ğŸš€ ì²˜ìŒ ì‚¬ìš©ì (ì•ˆì „í•œ ì„¤ì •)

```yaml
num_train_epochs: 3
per_device_train_batch_size: 4
gradient_accumulation_steps: 2
early_stopping_patience: 2
eval_strategy: epoch
```

### âš¡ ë¹ ë¥¸ ì‹¤í—˜ (ì‹œê°„ ì œì•½)

```yaml
max_steps: 500  # ì—í¬í¬ ëŒ€ì‹  ìŠ¤í… ì œí•œ
per_device_train_batch_size: 8
max_samples: 5000  # ë°ì´í„°ë„ ì œí•œ
```

### ğŸ¯ í”„ë¡œë•ì…˜ (ìµœê³  í’ˆì§ˆ)

```yaml
num_train_epochs: 5
per_device_train_batch_size: 8
gradient_accumulation_steps: 2
early_stopping_patience: 3
eval_steps: 100
save_strategy: steps
save_steps: 100
load_best_model_at_end: true
```

### ğŸ”¬ ì—°êµ¬ ëª©ì  (ì™„ì „í•œ ì œì–´)

```yaml
max_steps: 2000  # ì •í™•í•œ ìŠ¤í… ì œì–´
eval_steps: 100
save_steps: 100
logging_steps: 10
report_to: tensorboard
use_swanlab: true
```

## ğŸ“Š ìµœì¢… ê¶Œì¥ ê³µì‹

### ê¸°ë³¸ ê°€ì´ë“œë¼ì¸

```python
# ì—í¬í¬ ìˆ˜ ê²°ì • ê³µì‹
if ë°ì´í„°ì…‹_í¬ê¸° < 1000:
    num_train_epochs = 5-8
elif ë°ì´í„°ì…‹_í¬ê¸° < 10000:
    num_train_epochs = 3-5
else:  # > 10000
    num_train_epochs = 2-3

# ìŠ¤í… ê¸°ë°˜ ì œì–´ê°€ í•„ìš”í•œ ê²½ìš°
if ì‹œê°„_ì œì•½ or ì •í™•í•œ_ì œì–´_í•„ìš”:
    max_steps = ì›í•˜ëŠ”_ìŠ¤í…_ìˆ˜
    num_train_epochs = None  # ë¬´ì‹œë¨
```

### ê²€ì¦ ë°©ë²•

```python
# ì ì ˆí•œ ì—í¬í¬ ìˆ˜ì¸ì§€ í™•ì¸
1. í›ˆë ¨/ê²€ì¦ ì†ì‹¤ ê·¸ë˜í”„ ê´€ì°°
2. Early stoppingì´ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸
3. ìµœì¢… ì„±ëŠ¥ì´ ë§Œì¡±ìŠ¤ëŸ¬ìš´ì§€ í‰ê°€
4. ê³¼ì í•©/ë¶€ì¡±í•œ í›ˆë ¨ ì‹ í˜¸ ê°ì§€
```

ì—í¬í¬ì™€ ìŠ¤í… ì„¤ì •ì€ ëª¨ë¸ í›ˆë ¨ì˜ í•µì‹¬ì…ë‹ˆë‹¤. ì ì ˆí•œ ì„¤ì •ì„ í†µí•´ íš¨ìœ¨ì ì´ê³  íš¨ê³¼ì ì¸ íŒŒì¸íŠœë‹ì„ ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!

