---
title: GPT-2 êµ¬í˜„ìœ¼ë¡œ ë°°ìš°ëŠ” Transformer ì™„ì „ ì •ë³µ ì‹œë¦¬ì¦ˆ - 03
date: 2025-08-12 08:10:20 +0900
categories: [machine learning, GPT]
tags: [machine learning, GPT, Transformer]      # TAG names should always be lowercase
---

# GPT ì™„ì „ ì •ë³µ 3í¸: Attentionì˜ ë§ˆë²• - ì»´í“¨í„°ê°€ "ë¬¸ë§¥"ì„ ì´í•´í•˜ëŠ” ë°©ë²•

> **ì´ì „ í¸ ìš”ì•½**: 2í¸ì—ì„œëŠ” í…ìŠ¤íŠ¸ê°€ ì–´ë–»ê²Œ í† í°ìœ¼ë¡œ ë³€í™˜ë˜ê³ , ì„ë² ë”©ì„ í†µí•´ ì˜ë¯¸ ìˆëŠ” ë²¡í„°ê°€ ë˜ëŠ”ì§€ ë°°ì› ìŠµë‹ˆë‹¤. ì´ì œ ì´ ë²¡í„°ë“¤ì´ ì–´ë–»ê²Œ ì„œë¡œ "ì†Œí†µ"í•˜ì—¬ ë¬¸ë§¥ì„ ì´í•´í•˜ëŠ”ì§€ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.

---

## ë“¤ì–´ê°€ë©°: ì¸ê°„ì€ ì–´ë–»ê²Œ ë¬¸ë§¥ì„ ì´í•´í• ê¹Œ?

ë‹¤ìŒ ë¬¸ì¥ì„ ì½ì–´ë³´ì„¸ìš”:

```
"ê·¸ ë‚¨ìëŠ” ê³µì›ì—ì„œ ê°œë¥¼ ì‚°ì±…ì‹œí‚¤ê³  ìˆì—ˆë‹¤. ê°œê°€ ê°‘ìê¸° ë›°ì–´ê°”ë‹¤."
```

ë‘ ë²ˆì§¸ ë¬¸ì¥ì˜ "ê°œ"ë¥¼ ì½ëŠ” ìˆœê°„, ìš°ë¦¬ëŠ” ìë™ìœ¼ë¡œ ì²« ë²ˆì§¸ ë¬¸ì¥ì˜ "ê°œ"ì™€ ì—°ê²°í•©ë‹ˆë‹¤. ì´ê²ƒì´ ë°”ë¡œ **ë¬¸ë§¥ ì´í•´**ì…ë‹ˆë‹¤.

**ì»´í“¨í„°ëŠ” ì–´ë–»ê²Œ ì´ëŸ° ì—°ê²°ì„ ë§Œë“¤ ìˆ˜ ìˆì„ê¹Œìš”?**

ì´ê²ƒì´ ë°”ë¡œ **Attention ë©”ì»¤ë‹ˆì¦˜**ì´ í•´ê²°í•˜ëŠ” í•µì‹¬ ë¬¸ì œì…ë‹ˆë‹¤.

---

## 1. Attentionì˜ ì§ê´€ì  ì´í•´: ë„ì„œê´€ì—ì„œ ì •ë³´ ì°¾ê¸°

### Attention = ë˜‘ë˜‘í•œ ê²€ìƒ‰ ì‹œìŠ¤í…œ

ë„ì„œê´€ì—ì„œ ì±…ì„ ì°¾ëŠ” ìƒí™©ì„ ìƒìƒí•´ë³´ì„¸ìš”:

```
ìƒí™©: "ì…°ìµìŠ¤í”¼ì–´ì˜ í–„ë¦¿ì— ëŒ€í•œ ì •ë³´ê°€ í•„ìš”í•´"

1. Query (ì§ˆë¬¸): "í–„ë¦¿ì— ëŒ€í•´ ì•Œê³  ì‹¶ë‹¤"
2. Key (ìƒ‰ì¸): ê° ì±…ì˜ ì œëª©ê³¼ í‚¤ì›Œë“œë“¤
3. Value (ë‚´ìš©): ê° ì±…ì˜ ì‹¤ì œ ë‚´ìš©

ê³¼ì •:
1. ì‚¬ì„œê°€ ì§ˆë¬¸ì„ ë“£ëŠ”ë‹¤ (Query)
2. ë„ì„œ ëª©ë¡ì—ì„œ ê´€ë ¨ ìˆëŠ” ì±…ë“¤ì„ ì°¾ëŠ”ë‹¤ (Keyì™€ Query ë§¤ì¹­)
3. ê´€ë ¨ë„ì— ë”°ë¼ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•œë‹¤ (Attention Score)
4. ê°€ì¥ ê´€ë ¨ ìˆëŠ” ì±…ë“¤ì˜ ë‚´ìš©ì„ ì¢…í•©í•´ì„œ ë‹µí•œë‹¤ (Weighted Sum of Values)
```

### ì‹¤ì œ ë¬¸ì¥ì—ì„œì˜ Attention

```
ë¬¸ì¥: "ë‚˜ëŠ” í•™êµì— ê°„ë‹¤"
í˜„ì¬ ì˜ˆì¸¡í•  ë‹¨ì–´: "ê°„ë‹¤"

Query: "ê°„ë‹¤"ê°€ ë¬»ëŠ” ì§ˆë¬¸ - "ë‚˜ëŠ” ëˆ„êµ¬ì´ê³ , ì–´ë””ë¡œ ê°€ëŠ”ê°€?"
Key: ê° ë‹¨ì–´ê°€ ë‹µí•  ìˆ˜ ìˆëŠ” ê²ƒ
- "ë‚˜ëŠ”": "ì£¼ì²´ì— ëŒ€í•œ ì •ë³´ë¥¼ ì¤„ ìˆ˜ ìˆì–´"
- "í•™êµì—": "ëª©ì ì§€ì— ëŒ€í•œ ì •ë³´ë¥¼ ì¤„ ìˆ˜ ìˆì–´"
Value: ì‹¤ì œë¡œ ì „ë‹¬í•  ì •ë³´  
- "ë‚˜ëŠ”": [ì£¼ì²´, 1ì¸ì¹­, ëŠ¥ë™ì , ...]
- "í•™êµì—": [ëª©ì ì§€, êµìœ¡ê¸°ê´€, ì¥ì†Œ, ...]

ê²°ê³¼: "ê°„ë‹¤"ëŠ” "ë‚˜ëŠ”"ì—ì„œ ì£¼ì²´ ì •ë³´ë¥¼, "í•™êµì—"ì—ì„œ ëª©ì ì§€ ì •ë³´ë¥¼ ê°€ì ¸ì™€ 
      "1ì¸ì¹­ì´ êµìœ¡ê¸°ê´€ìœ¼ë¡œ ì´ë™í•œë‹¤"ëŠ” ì˜ë¯¸ë¥¼ êµ¬ì„±
```

---

## 2. CausalSelfAttention í´ë˜ìŠ¤ ì™„ì „ ë¶„í•´

### ìš°ë¦¬ êµ¬í˜„ ì½”ë“œ ì‚´í´ë³´ê¸°

[ë ˆí¬ì§€í† ë¦¬](https://github.com/BanHun28/gpt2_study)ì˜ `main.py`ì—ì„œ:

```python
class CausalSelfAttention(nn.Module):
    """
    ì¸ê³¼ê´€ê³„ ìê¸° ì£¼ì˜ê¸°ì œ(Causal Self-Attention) í´ë˜ìŠ¤
    ì´ì „ ë‹¨ì–´ë“¤ë§Œ ì°¸ê³ í•  ìˆ˜ ìˆê³ , ë¯¸ë˜ ë‹¨ì–´ëŠ” ë³¼ ìˆ˜ ì—†ë„ë¡ ë§ˆìŠ¤í‚¹ëœ Attention
    """
    def __init__(self, config):
        super().__init__()
        
        # ã€í•µì‹¬ã€‘Q, K, Vë¥¼ í•œ ë²ˆì— ìƒì„±í•˜ëŠ” ë ˆì´ì–´
        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)
        
        # ã€í•µì‹¬ã€‘Multi-Head ê²°ê³¼ë¥¼ í•©ì¹˜ëŠ” ë ˆì´ì–´  
        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)
        
        # ã€í•µì‹¬ã€‘ë¯¸ë˜ë¥¼ ë³¼ ìˆ˜ ì—†ê²Œ í•˜ëŠ” ë§ˆìŠ¤í¬
        self.register_buffer("bias", torch.tril(torch.ones(config.block_size, config.block_size))
                                    .view(1, 1, config.block_size, config.block_size))
```

### 1ë‹¨ê³„: Q, K, V ìƒì„±ì˜ ë¹„ë°€

#### ì™œ í•˜ë‚˜ì˜ ë ˆì´ì–´ë¡œ 3ê°œë¥¼ ë§Œë“¤ê¹Œ?

```python
# ë¹„íš¨ìœ¨ì ì¸ ë°©ë²• (3ê°œ ë³„ë„ ë ˆì´ì–´)
self.query_layer = nn.Linear(config.n_embd, config.n_embd)
self.key_layer = nn.Linear(config.n_embd, config.n_embd)  
self.value_layer = nn.Linear(config.n_embd, config.n_embd)

# íš¨ìœ¨ì ì¸ ë°©ë²• (1ê°œ ë ˆì´ì–´ë¡œ 3ë°° ì¶œë ¥)
self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)
```

**ì¥ì :**
1. **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: 3ë²ˆì˜ í–‰ë ¬ ê³±ì…ˆ â†’ 1ë²ˆì˜ í–‰ë ¬ ê³±ì…ˆ
2. **ë³‘ë ¬ ì²˜ë¦¬**: GPUì—ì„œ ë” ë¹ ë¥¸ ì—°ì‚°
3. **ìºì‹œ íš¨ìœ¨ì„±**: ë©”ëª¨ë¦¬ ì ‘ê·¼ íŒ¨í„´ ìµœì í™”

#### Q, K, V ë¶„í•  ê³¼ì •

```python
def forward(self, x):
    B, T, C = x.size()  # ë°°ì¹˜, ì‹œí€€ìŠ¤ ê¸¸ì´, ì„ë² ë”© ì°¨ì›
    
    # 1. í•˜ë‚˜ì˜ ë ˆì´ì–´ë¡œ 3ë°° í¬ê¸° ì¶œë ¥ ìƒì„±
    qkv = self.c_attn(x)  # (B, T, C) â†’ (B, T, 3*C)
    
    # 2. 3ë“±ë¶„ìœ¼ë¡œ ë¶„í• 
    q, k, v = qkv.split(self.n_embd, dim=2)  # ê°ê° (B, T, C)
    
    # 3. Multi-Headë¥¼ ìœ„í•œ ì°¨ì› ì¬ë°°ì—´
    # (B, T, C) â†’ (B, T, n_head, C//n_head) â†’ (B, n_head, T, C//n_head)
    q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
    k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  
    v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
```

### 2ë‹¨ê³„: Scaled Dot-Product Attention ìˆ˜ì‹ ì´í•´

#### í•µì‹¬ ìˆ˜ì‹ ë¶„í•´

$$Attention(Q, K, V) = softmax(QK^T / âˆšd_k)V$$

ì´ ìˆ˜ì‹ì„ ë‹¨ê³„ë³„ë¡œ ë¶„í•´í•´ë³´ê² ìŠµë‹ˆë‹¤:

```python
# 1ë‹¨ê³„: Attention Score ê³„ì‚°
att = (q @ k.transpose(-2, -1))  # QK^T: ì§ˆë¬¸ê³¼ í‚¤ì˜ ìœ ì‚¬ë„

# 2ë‹¨ê³„: ìŠ¤ì¼€ì¼ë§  
att = att * (1.0 / math.sqrt(k.size(-1)))  # âˆšd_kë¡œ ë‚˜ëˆ„ê¸°

# 3ë‹¨ê³„: ì¸ê³¼ê´€ê³„ ë§ˆìŠ¤í‚¹ (ë¯¸ë˜ ì°¨ë‹¨)
att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))

# 4ë‹¨ê³„: í™•ë¥ ë¡œ ë³€í™˜
att = F.softmax(att, dim=-1)  # ê° í–‰ì˜ í•©ì´ 1ì´ ë˜ë„ë¡

# 5ë‹¨ê³„: Valueì™€ ê°€ì¤‘í•©
y = att @ v  # ê°€ì¤‘ í‰ê·  ê³„ì‚°
```

#### ê° ë‹¨ê³„ì˜ ì§ê´€ì  ì˜ë¯¸

**1ë‹¨ê³„: QK^T (ì§ˆë¬¸-í‚¤ ë§¤ì¹­)**
```
Q: "ê°„ë‹¤"ì˜ ì§ˆë¬¸ ë²¡í„° [0.1, 0.5, -0.2, ...]
K: "ë‚˜ëŠ”"ì˜ í‚¤ ë²¡í„°   [0.2, 0.3, -0.1, ...]

ë‚´ì  = 0.1*0.2 + 0.5*0.3 + (-0.2)*(-0.1) + ... = 0.37

ì˜ë¯¸: "ê°„ë‹¤"ê°€ "ë‚˜ëŠ”"ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ ìˆëŠ”ê°€? â†’ 0.37
```

**2ë‹¨ê³„: âˆšd_k ìŠ¤ì¼€ì¼ë§**
```python
# ì™œ âˆšd_kë¡œ ë‚˜ëˆŒê¹Œ?
d_k = 64  # ê° í—¤ë“œì˜ ì°¨ì›

# ìŠ¤ì¼€ì¼ë§ ì „: ë‚´ì  ê°’ì´ ë„ˆë¬´ í´ ìˆ˜ ìˆìŒ
raw_score = 37.2  # í° ê°’

# ìŠ¤ì¼€ì¼ë§ í›„: ì ì ˆí•œ í¬ê¸°ë¡œ ì¡°ì •
scaled_score = 37.2 / math.sqrt(64) = 37.2 / 8 = 4.65

# íš¨ê³¼: softmaxì—ì„œ ê·¹ë‹¨ì  í™•ë¥  ë°©ì§€ (ëª¨ë“  í™•ë¥ ì´ 0 ë˜ëŠ” 1ë¡œ ìˆ˜ë ´í•˜ëŠ” ê²ƒ ë°©ì§€)
```

**3ë‹¨ê³„: ì¸ê³¼ê´€ê³„ ë§ˆìŠ¤í‚¹**
```python
# ë¯¸ë˜ë¥¼ ë³¼ ìˆ˜ ì—†ê²Œ í•˜ëŠ” ë§ˆìŠ¤í¬
mask = [[1, 0, 0, 0],    # ì²« ë²ˆì§¸ ë‹¨ì–´ëŠ” ìê¸°ë§Œ ë´„
        [1, 1, 0, 0],    # ë‘ ë²ˆì§¸ ë‹¨ì–´ëŠ” ì²« ë²ˆì§¸ì™€ ìê¸°ë§Œ ë´„  
        [1, 1, 1, 0],    # ì„¸ ë²ˆì§¸ ë‹¨ì–´ëŠ” ì´ì „ ëª¨ë“  ê²ƒê³¼ ìê¸°ë§Œ ë´„
        [1, 1, 1, 1]]    # ë„¤ ë²ˆì§¸ ë‹¨ì–´ëŠ” ëª¨ë“  ì´ì „ ë‹¨ì–´ ë´„

# 0ì¸ ìœ„ì¹˜ë¥¼ -âˆë¡œ ì„¤ì • â†’ softmaxì—ì„œ í™•ë¥  0ì´ ë¨
```

**4-5ë‹¨ê³„: í™•ë¥  ë³€í™˜ ë° ê°€ì¤‘í•©**
```python
# ì˜ˆì‹œ: "ê°„ë‹¤" ìœ„ì¹˜ì—ì„œì˜ Attention
attention_weights = [0.1, 0.7, 0.2]  # "ë‚˜ëŠ”", "í•™êµì—", "ê°„ë‹¤" ê°ê°ì— ëŒ€í•œ ê°€ì¤‘ì¹˜

# Valueë“¤ì˜ ê°€ì¤‘í•©
result = 0.1 * value_ë‚˜ëŠ” + 0.7 * value_í•™êµì— + 0.2 * value_ê°„ë‹¤
# â†’ "ê°„ë‹¤"ëŠ” ì£¼ë¡œ "í•™êµì—"ì˜ ì •ë³´ë¥¼ ì°¸ê³ í•¨!
```

---

## 3. Multi-Head Attention: ì—¬ëŸ¬ ì‹œê°ìœ¼ë¡œ ë³´ê¸°

### ì™œ í•˜ë‚˜ì˜ Attentionìœ¼ë¡œ ë¶€ì¡±í• ê¹Œ?

í•˜ë‚˜ì˜ Attentionì€ í•˜ë‚˜ì˜ ê´€ì ë§Œ ê°€ì§‘ë‹ˆë‹¤:

```
ë‹¨ì¼ Attentionì´ ë†“ì¹  ìˆ˜ ìˆëŠ” ê²ƒë“¤:

ë¬¸ì¥: "The bank can guarantee deposits will eventually cover future tuition costs"

Head 1: ë¬¸ë²•ì  ê´€ê³„ë§Œ íŒŒì•…
- "bank" â†” "guarantee" (ì£¼ì–´-ë™ì‚¬)
- "deposits" â†” "cover" (ì£¼ì–´-ë™ì‚¬)

Head 2: ì˜ë¯¸ì  ì—°ê´€ë§Œ íŒŒì•…  
- "bank" â†” "deposits" (ê¸ˆìœµ ê´€ë ¨)
- "tuition" â†” "costs" (êµìœ¡ë¹„ ê´€ë ¨)

Head 3: ì‹œê°„ì  ê´€ê³„ë§Œ íŒŒì•…
- "eventually" â†” "future" (ì‹œê°„ ìˆœì„œ)
```

### Multi-Headì˜ êµ¬í˜„ ì›ë¦¬

```python
# main.pyì—ì„œ
self.n_head = config.n_head  # ë³´í†µ 12ê°œ

# 768ì°¨ì›ì„ 12ê°œ í—¤ë“œë¡œ ë‚˜ëˆ„ë©´
head_dim = 768 // 12 = 64  # ê° í—¤ë“œëŠ” 64ì°¨ì› ë‹´ë‹¹

# ì°¨ì› ì¬ë°°ì—´
q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
# (B, T, 768) â†’ (B, T, 12, 64) â†’ (B, 12, T, 64)
```

#### ê° í—¤ë“œê°€ ë³´ëŠ” ë‹¤ë¥¸ ì‹œê°

```python
# 12ê°œ í—¤ë“œì˜ ì„œë¡œ ë‹¤ë¥¸ ì—­í•  (í•™ìŠµìœ¼ë¡œ ìë™ ë¶„í™”)
í—¤ë“œ 1: ì£¼ì–´-ë™ì‚¬ ê´€ê³„ ì „ë¬¸
í—¤ë“œ 2: ìˆ˜ì‹ì–´-í”¼ìˆ˜ì‹ì–´ ê´€ê³„ ì „ë¬¸  
í—¤ë“œ 3: ì‹œê°„ ìˆœì„œ ê´€ê³„ ì „ë¬¸
í—¤ë“œ 4: ì¸ê³¼ê´€ê³„ ì „ë¬¸
í—¤ë“œ 5: ë™ì˜ì–´/ë°˜ì˜ì–´ ê´€ê³„ ì „ë¬¸
...
í—¤ë“œ 12: ë³µí•©ì  ì˜ë¯¸ ê´€ê³„ ì „ë¬¸
```

### í—¤ë“œë“¤ì˜ ê²°ê³¼ í†µí•©

```python
# ê° í—¤ë“œì˜ ì¶œë ¥ì„ ë‹¤ì‹œ í•©ì¹˜ê¸°
y = y.transpose(1, 2).contiguous().view(B, T, C)
# (B, 12, T, 64) â†’ (B, T, 12, 64) â†’ (B, T, 768)

# ìµœì¢… í”„ë¡œì ì…˜ìœ¼ë¡œ ì •ë³´ í†µí•©
y = self.c_proj(y)  # ì—¬ëŸ¬ ì‹œê°ì„ í•˜ë‚˜ë¡œ ì¢…í•©
```

---

## 4. Causal Masking: ì‹œê°„ì˜ ë°©í–¥ì„± ì§€í‚¤ê¸°

### GPTëŠ” ì™œ ë¯¸ë˜ë¥¼ ë³¼ ìˆ˜ ì—†ì„ê¹Œ?

**GPTì˜ ëª©í‘œ**: ì´ì „ ë‹¨ì–´ë“¤ë§Œ ë³´ê³  ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡

```
ì˜ëª»ëœ ì˜ˆ (ë¯¸ë˜ë¥¼ ë³¸ë‹¤ë©´):
"ë‚˜ëŠ” ??? ê°„ë‹¤" â†’ "???" ì˜ˆì¸¡ ì‹œ "ê°„ë‹¤"ë¥¼ ë¯¸ë¦¬ ë³¸ë‹¤ë©´?
â†’ ë„ˆë¬´ ì‰¬ì›Œì§! í•™ìŠµì´ ì œëŒ€ë¡œ ì•ˆ ë¨

ì˜¬ë°”ë¥¸ ì˜ˆ (ë¯¸ë˜ë¥¼ ì°¨ë‹¨):
"ë‚˜ëŠ” ???" â†’ "???" ì˜ˆì¸¡ ì‹œ "ê°„ë‹¤"ë¥¼ ë³¼ ìˆ˜ ì—†ìŒ
â†’ "ë‚˜ëŠ”" ì •ë³´ë§Œìœ¼ë¡œ ë‹¤ìŒ ë‹¨ì–´ ì¶”ë¡ í•´ì•¼ í•¨
```

### ë§ˆìŠ¤í¬ êµ¬í˜„ì˜ ì„¸ë¶€ì‚¬í•­

```python
# ë§ˆìŠ¤í¬ ìƒì„±
self.register_buffer("bias", torch.tril(torch.ones(config.block_size, config.block_size))
                            .view(1, 1, config.block_size, config.block_size))

# torch.tril(): í•˜ì‚¼ê° í–‰ë ¬ ìƒì„±
mask = torch.tril(torch.ones(4, 4))
print(mask)
# tensor([[1., 0., 0., 0.],
#         [1., 1., 0., 0.],  
#         [1., 1., 1., 0.],
#         [1., 1., 1., 1.]])
```

#### ë§ˆìŠ¤í‚¹ ì ìš© ê³¼ì •

```python
# Attention Scoreì— ë§ˆìŠ¤í¬ ì ìš©
att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))

# ì˜ˆì‹œ: 4ê°œ ë‹¨ì–´ ë¬¸ì¥ì—ì„œ
ì›ë³¸ Attention Score:
[[2.1, 1.5, 0.8, 1.2],   # ì²« ë²ˆì§¸ ë‹¨ì–´
 [1.3, 2.5, 1.1, 0.9],   # ë‘ ë²ˆì§¸ ë‹¨ì–´  
 [0.7, 1.8, 2.2, 1.4],   # ì„¸ ë²ˆì§¸ ë‹¨ì–´
 [1.0, 1.6, 1.9, 2.3]]   # ë„¤ ë²ˆì§¸ ë‹¨ì–´

ë§ˆìŠ¤í‚¹ í›„:
[[2.1, -âˆ,  -âˆ,  -âˆ ],   # ì²« ë²ˆì§¸ëŠ” ìê¸°ë§Œ ë´„
 [1.3, 2.5, -âˆ,  -âˆ ],   # ë‘ ë²ˆì§¸ëŠ” ì´ì „ê¹Œì§€ë§Œ ë´„
 [0.7, 1.8, 2.2, -âˆ ],   # ì„¸ ë²ˆì§¸ëŠ” ì´ì „ê¹Œì§€ë§Œ ë´„  
 [1.0, 1.6, 1.9, 2.3]]   # ë„¤ ë²ˆì§¸ëŠ” ëª¨ë“  ì´ì „ ë´„

Softmax í›„:
[[1.0, 0.0, 0.0, 0.0],   # -âˆëŠ” í™•ë¥  0ì´ ë¨
 [0.2, 0.8, 0.0, 0.0],
 [0.1, 0.3, 0.6, 0.0],
 [0.1, 0.2, 0.3, 0.4]]
```

---

## 5. ìµœì‹  ìµœì í™”: Flash Attention

### PyTorch 2.0+ì˜ í˜ì‹ 

ìš°ë¦¬ ì½”ë“œì—ì„œ ë³¼ ìˆ˜ ìˆëŠ” ìµœì‹  ìµœì í™”:

```python
# PyTorch 2.0+ì—ì„œ ì œê³µí•˜ëŠ” ìµœì í™”ëœ scaled_dot_product_attention ì‚¬ìš©
if hasattr(F, 'scaled_dot_product_attention'):
    # ìµœì‹  ë²„ì „ì˜ ê³ ì„±ëŠ¥ Attention êµ¬í˜„ ì‚¬ìš©
    y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, 
                                      dropout_p=self.dropout if self.training else 0, 
                                      is_causal=True)
else:
    # ì´ì „ PyTorch ë²„ì „ì„ ìœ„í•œ ìˆ˜ë™ êµ¬í˜„
    att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
    att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))
    att = F.softmax(att, dim=-1)
    att = self.attn_dropout(att)
    y = att @ v
```

### Flash Attentionì˜ í˜ì‹ 

**ê¸°ì¡´ ë°©ì‹ì˜ ë¬¸ì œì :**
```python
# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: O(nÂ²)
attention_matrix = Q @ K.T  # (seq_len, seq_len) í¬ê¸°ì˜ í–‰ë ¬ ì €ì¥
# 1024 í† í° â†’ 1Mê°œ ì›ì†Œ â†’ 4MB (float32 ê¸°ì¤€)
# 4096 í† í° â†’ 16Mê°œ ì›ì†Œ â†’ 64MB
```

**Flash Attentionì˜ í•´ê²°ì±…:**
- **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: ì¤‘ê°„ í–‰ë ¬ì„ ì €ì¥í•˜ì§€ ì•Šê³  ì¦‰ì‹œ ê³„ì‚°
- **ì†ë„ í–¥ìƒ**: GPU ë©”ëª¨ë¦¬ ê³„ì¸µì„ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©
- **ìˆ˜í•™ì  ë™ì¼ì„±**: ê²°ê³¼ëŠ” ì™„ì „íˆ ë™ì¼

---

## 6. ì‹¤ì „ ë¶„ì„: Attention ê°€ì¤‘ì¹˜ ì‹œê°í™”

### Attention Pattern ì´í•´í•˜ê¸°

```python
# Attention ê°€ì¤‘ì¹˜ ì¶”ì¶œ ë° ë¶„ì„
def analyze_attention_patterns():
    model.eval()
    
    # ì˜ˆì‹œ ë¬¸ì¥
    text = "ROMEO: But soft, what light through yonder window breaks?"
    tokens = enc.encode(text)
    
    # ì¤‘ê°„ ê²°ê³¼ ì¶”ì¶œì„ ìœ„í•œ hook ì„¤ì •
    attention_weights = []
    
    def hook_fn(module, input, output):
        # Attention ê°€ì¤‘ì¹˜ ì €ì¥
        attention_weights.append(output[1])  # [1]ì´ attention weights
    
    # ëª¨ë¸ ì‹¤í–‰
    with torch.no_grad():
        output = model(torch.tensor([tokens]))
    
    # ì²« ë²ˆì§¸ ë ˆì´ì–´, ì²« ë²ˆì§¸ í—¤ë“œì˜ íŒ¨í„´ ë¶„ì„
    att_weights = attention_weights[0][0, 0]  # [seq_len, seq_len]
    
    # ì‹œê°í™”ìš© í† í° í…ìŠ¤íŠ¸
    token_texts = [enc.decode([token]) for token in tokens]
    
    # ê° í† í°ì´ ì–´ë–¤ í† í°ë“¤ì— ì£¼ëª©í•˜ëŠ”ì§€ ì¶œë ¥
    for i, token in enumerate(token_texts):
        top_indices = att_weights[i].topk(3).indices
        top_tokens = [token_texts[j] for j in top_indices]
        print(f"'{token}' ì£¼ëª© â†’ {top_tokens}")
```

**ì˜ˆìƒ ì¶œë ¥:**
```
'ROMEO' ì£¼ëª© â†’ ['ROMEO', 'But', 'soft']
'But' ì£¼ëª© â†’ ['ROMEO', 'But', 'soft']  
'soft' ì£¼ëª© â†’ ['But', 'soft', 'what']
'what' ì£¼ëª© â†’ ['soft', 'what', 'light']
'light' ì£¼ëª© â†’ ['what', 'light', 'through']
...
```

### í¥ë¯¸ë¡œìš´ Attention íŒ¨í„´ë“¤

**1. ì§€ì—­ì  íŒ¨í„´**: ì¸ì ‘í•œ ë‹¨ì–´ë“¤ì— ì£¼ëª©
**2. êµ¬ë¬¸ì  íŒ¨í„´**: ì£¼ì–´-ë™ì‚¬, ìˆ˜ì‹ì–´-í”¼ìˆ˜ì‹ì–´ ê´€ê³„
**3. ì˜ë¯¸ì  íŒ¨í„´**: ì˜ë¯¸ì ìœ¼ë¡œ ì—°ê´€ëœ ë‹¨ì–´ë“¤
**4. ìœ„ì¹˜ì  íŒ¨í„´**: íŠ¹ì • ìœ„ì¹˜(ì²« ë²ˆì§¸, ë§ˆì§€ë§‰ ë“±)ì— ì£¼ëª©

---

## 7. Attentionì˜ í•œê³„ì™€ í•´ê²°ì±…

### í˜„ì¬ Attentionì˜ í•œê³„

#### 1. ê³„ì‚° ë³µì¡ë„: O(nÂ²)
```python
# ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ 2ë°° â†’ ê³„ì‚°ëŸ‰ 4ë°°
seq_len_512 = 512
attention_ops_512 = 512 * 512 = 262,144

seq_len_1024 = 1024  
attention_ops_1024 = 1024 * 1024 = 1,048,576  # 4ë°°!
```

#### 2. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í­ì¦
```python
# GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (float32 ê¸°ì¤€)
def memory_usage(seq_len, batch_size=1):
    attention_matrix = seq_len * seq_len * 4  # bytes
    total_memory = attention_matrix * batch_size
    return f"{total_memory / (1024**2):.1f} MB"

print(f"1K í† í°: {memory_usage(1024)}")    # 4.0 MB
print(f"4K í† í°: {memory_usage(4096)}")    # 64.0 MB  
print(f"16K í† í°: {memory_usage(16384)}")  # 1024.0 MB = 1GB!
```

### ì°¨ì„¸ëŒ€ í•´ê²°ì±…ë“¤

#### 1. Sparse Attention
- ëª¨ë“  í† í°ì„ ë³´ì§€ ì•Šê³  ì¼ë¶€ë§Œ ì„ íƒì ìœ¼ë¡œ ì°¸ì¡°
- Longformer, BigBird ë“±ì—ì„œ ì‚¬ìš©

#### 2. Linear Attention  
- Attention ê³„ì‚°ì„ ì„ í˜• ë³µì¡ë„ë¡œ ê·¼ì‚¬
- Performer, FNet ë“±ì—ì„œ ì‹œë„

#### 3. Retrieval-Augmented Generation
- í•„ìš”í•œ ì •ë³´ë§Œ ì™¸ë¶€ì—ì„œ ê°€ì ¸ì™€ ì‚¬ìš©
- RAG, FiD ë“±ì˜ ë°©ë²•ë¡ 

---

## 8. ì‹¤ìŠµ: Attention ì§ì ‘ êµ¬í˜„í•˜ê¸°

### ì‹¤ìŠµ 1: ë¯¸ë‹ˆ Attention êµ¬í˜„

```python
import torch
import torch.nn.functional as F
import math

def simple_attention(query, key, value, mask=None):
    """
    ê°„ë‹¨í•œ Attention êµ¬í˜„
    """
    # Q, Kì˜ ë‚´ì ìœ¼ë¡œ Attention Score ê³„ì‚°
    scores = torch.matmul(query, key.transpose(-2, -1))
    
    # ìŠ¤ì¼€ì¼ë§
    d_k = key.size(-1)
    scores = scores / math.sqrt(d_k)
    
    # ë§ˆìŠ¤í‚¹ (ì˜µì…˜)
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)
    
    # Softmaxë¡œ í™•ë¥  ë³€í™˜
    attention_weights = F.softmax(scores, dim=-1)
    
    # Valueì™€ ê°€ì¤‘í•©
    output = torch.matmul(attention_weights, value)
    
    return output, attention_weights

# í…ŒìŠ¤íŠ¸
seq_len, d_model = 4, 8
q = torch.randn(1, seq_len, d_model)
k = torch.randn(1, seq_len, d_model)  
v = torch.randn(1, seq_len, d_model)

output, weights = simple_attention(q, k, v)
print(f"ì…ë ¥ í¬ê¸°: {q.shape}")
print(f"ì¶œë ¥ í¬ê¸°: {output.shape}")
print(f"Attention ê°€ì¤‘ì¹˜: {weights.shape}")
```

### ì‹¤ìŠµ 2: Multi-Head êµ¬í˜„

```python
class SimpleMultiHeadAttention(nn.Module):
    def __init__(self, d_model, n_heads):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        self.w_q = nn.Linear(d_model, d_model)
        self.w_k = nn.Linear(d_model, d_model) 
        self.w_v = nn.Linear(d_model, d_model)
        self.w_o = nn.Linear(d_model, d_model)
        
    def forward(self, x):
        batch_size, seq_len, d_model = x.size()
        
        # Q, K, V ìƒì„±
        Q = self.w_q(x)
        K = self.w_k(x)
        V = self.w_v(x)
        
        # Multi-Headë¡œ ë¶„í• 
        Q = Q.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        K = K.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        V = V.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        
        # Attention ê³„ì‚°
        output, _ = simple_attention(Q, K, V)
        
        # í—¤ë“œ í•©ì¹˜ê¸°
        output = output.transpose(1, 2).contiguous().view(
            batch_size, seq_len, d_model)
        
        # ìµœì¢… í”„ë¡œì ì…˜
        return self.w_o(output)

# í…ŒìŠ¤íŠ¸
mha = SimpleMultiHeadAttention(d_model=128, n_heads=8)
x = torch.randn(2, 10, 128)
output = mha(x)
print(f"Multi-Head Attention ì¶œë ¥: {output.shape}")
```

### ì‹¤ìŠµ 3: Causal Mask ì‹¤í—˜

```python
def create_causal_mask(seq_len):
    """í•˜ì‚¼ê° ë§ˆìŠ¤í¬ ìƒì„±"""
    mask = torch.tril(torch.ones(seq_len, seq_len))
    return mask

def visualize_attention_pattern(seq_len=8):
    """Attention íŒ¨í„´ ì‹œê°í™”"""
    # ëœë¤ Attention Score ìƒì„±
    scores = torch.randn(seq_len, seq_len)
    
    # Causal Mask ì ìš©
    mask = create_causal_mask(seq_len)
    masked_scores = scores.masked_fill(mask == 0, float('-inf'))
    
    # Softmax ì ìš©
    attention_weights = F.softmax(masked_scores, dim=-1)
    
    print("Causal Attention Pattern:")
    print(attention_weights.numpy().round(3))
    
    # ê° ìœ„ì¹˜ê°€ ì´ì „ ìœ„ì¹˜ë“¤ë§Œ ì°¸ì¡°í•¨ì„ í™•ì¸
    for i in range(seq_len):
        future_sum = attention_weights[i, i+1:].sum()
        print(f"ìœ„ì¹˜ {i}: ë¯¸ë˜ í† í° ê°€ì¤‘ì¹˜ í•© = {future_sum:.6f}")

visualize_attention_pattern()
```

---

## ë§ˆë¬´ë¦¬: Attentionì˜ ë§ˆë²•ì„ ì´í•´í–ˆìŠµë‹ˆë‹¤

### ì˜¤ëŠ˜ ë°°ìš´ í•µì‹¬ ë‚´ìš©

1. **Attentionì˜ ì§ê´€**: ë„ì„œê´€ì—ì„œ ì •ë³´ ì°¾ê¸°ì™€ ê°™ì€ ë©”ì»¤ë‹ˆì¦˜
2. **Q, K, V ì‹œìŠ¤í…œ**: Query(ì§ˆë¬¸), Key(ìƒ‰ì¸), Value(ë‚´ìš©)ì˜ ì—­í• 
3. **Scaled Dot-Product**: ìˆ˜ì‹ì˜ ê° ë‹¨ê³„ë³„ ì˜ë¯¸
4. **Multi-Head**: ì—¬ëŸ¬ ì‹œê°ìœ¼ë¡œ ë™ì‹œì— ë³´ëŠ” ì§€í˜œ
5. **Causal Masking**: ë¯¸ë˜ë¥¼ ì°¨ë‹¨í•˜ì—¬ ì˜¬ë°”ë¥¸ í•™ìŠµ ìœ ë„

### Attentionì˜ í˜ì‹ ì„±

```
ì´ì „ (RNN): A â†’ B â†’ C â†’ D (ìˆœì°¨ì , ëŠë¦¼)
ì´í›„ (Attention): A â†” B â†” C â†” D (ë³‘ë ¬ì , ë¹ ë¦„, ì§ì ‘ ì—°ê²°)
```

**Attentionì´ ê°€ëŠ¥í•˜ê²Œ í•œ ê²ƒë“¤:**
- **ë³‘ë ¬ ì²˜ë¦¬**: ëª¨ë“  ìœ„ì¹˜ ë™ì‹œ ê³„ì‚°
- **ì¥ê±°ë¦¬ ì˜ì¡´ì„±**: ê±°ë¦¬ì— ìƒê´€ì—†ì´ ì§ì ‘ ì—°ê²°
- **í•´ì„ ê°€ëŠ¥ì„±**: ì–´ë–¤ ë‹¨ì–´ì— ì£¼ëª©í–ˆëŠ”ì§€ í™•ì¸ ê°€ëŠ¥
- **ìŠ¤ì¼€ì¼ë§**: ë” ê¸´ ë¬¸ì¥, ë” í° ëª¨ë¸ ì²˜ë¦¬ ê°€ëŠ¥

### ë‹¤ìŒ í¸ ì˜ˆê³ : Transformer Blockì˜ ì™„ì„±

ë‹¤ìŒ í¸ì—ì„œëŠ” Attentionê³¼ í•¨ê»˜ **Transformer Block**ì„ êµ¬ì„±í•˜ëŠ” ë‚˜ë¨¸ì§€ ìš”ì†Œë“¤ì„ ë°°ì›ë‹ˆë‹¤:

- **Layer Normalization**: ì•ˆì •ì ì¸ í•™ìŠµì„ ìœ„í•œ ì •ê·œí™”
- **Residual Connection**: ê¹Šì€ ë„¤íŠ¸ì›Œí¬ì˜ ê·¸ë˜ë””ì–¸íŠ¸ ì†Œì‹¤ í•´ê²°
- **Feed-Forward Network**: Attentionì´ ë†“ì¹œ íŒ¨í„´ ë³´ì™„
- **Block ì „ì²´ êµ¬ì¡°**: ëª¨ë“  ìš”ì†Œë“¤ì˜ ìœ ê¸°ì  ê²°í•©

**ë¯¸ë¦¬ ìƒê°í•´ë³¼ ì§ˆë¬¸:**
Attentionìœ¼ë¡œ ë‹¨ì–´ ê°„ ê´€ê³„ëŠ” íŒŒì•…í–ˆëŠ”ë°, ê° ë‹¨ì–´ì˜ "ì˜ë¯¸ ë³€í™˜"ì€ ëˆ„ê°€ ë‹´ë‹¹í• ê¹Œìš”? ë°”ë¡œ **MLP(Feed-Forward Network)**ê°€ ê·¸ ì—­í• ì„ í•©ë‹ˆë‹¤!

### ì‹¤ìŠµ ê³¼ì œ

ë‹¤ìŒ í¸ê¹Œì§€ í•´ë³¼ ê³¼ì œ:

1. **Attention ì‹œê°í™”**: ì‹¤ì œ ë¬¸ì¥ì—ì„œ ì–´ë–¤ ë‹¨ì–´ë“¤ì´ ì„œë¡œ ì£¼ëª©í•˜ëŠ”ì§€ í™•ì¸
2. **ë§ˆìŠ¤í¬ ì‹¤í—˜**: Causal Maskë¥¼ ì œê±°í•˜ë©´ ì–´ë–»ê²Œ ë ì§€ í…ŒìŠ¤íŠ¸
3. **í—¤ë“œ ìˆ˜ ì‹¤í—˜**: n_headë¥¼ ë°”ê¿”ê°€ë©° ì„±ëŠ¥ ë³€í™” ê´€ì°°

Attentionì˜ ë§ˆë²•ì„ ì´í•´í–ˆìœ¼ë‹ˆ, ì´ì œ ì™„ì „í•œ Transformer Blockìœ¼ë¡œ ë„˜ì–´ê°‘ì‹œë‹¤! ğŸ”®
