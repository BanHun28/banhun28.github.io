---
title: í•˜ì´í¼íŒŒë¼ë¯¸í„° ì´í•´í•˜ê¸° - 04
date: 2025-08-18 11:02:03 +0900
categories: [artificial intelligence, machine learning]
tags: [machine learning, deep learning, hyperparameters, finetuning, llm, llamafactory, pytorch, optimization, training, precision, ai]     # TAG names should always be lowercase
---

# ì •ë°€ë„(Precision) ì´í•´í•˜ê¸°

## ğŸ“š ê°œë… ì´í•´

### ì •ë°€ë„(Precision)ë€?

**ì •ë°€ë„**ëŠ” ëª¨ë¸ í›ˆë ¨ ì‹œ ê°€ì¤‘ì¹˜ì™€ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ í‘œí˜„í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ë¶€ë™ì†Œìˆ˜ì  ìˆ˜ì˜ ë¹„íŠ¸ ìˆ˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ê³¼ ì—°ì‚° ì†ë„, ê·¸ë¦¬ê³  ìˆ˜ì¹˜ì  ì•ˆì •ì„±ì— ì§ì ‘ì ì¸ ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.

```python
# ê¸°ë³¸ ê°œë…
FP32 (32-bit): ì™„ì „í•œ ì •ë°€ë„, ìµœê³  í’ˆì§ˆ, ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©
FP16 (16-bit): ì ˆë°˜ ì •ë°€ë„, ë©”ëª¨ë¦¬ ì ˆì•½, ì†ë„ í–¥ìƒ
BF16 (16-bit): Brain Float 16, FP16ì˜ ê°œì„ ëœ ë²„ì „
```

### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ

```python
# ëª¨ë¸ í¬ê¸°ë³„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (7B íŒŒë¼ë¯¸í„° ëª¨ë¸ ê¸°ì¤€)
FP32: 7B Ã— 4 bytes = 28GB
FP16: 7B Ã— 2 bytes = 14GB  (50% ì ˆì•½)
BF16: 7B Ã— 2 bytes = 14GB  (50% ì ˆì•½)

# ì‹¤ì œ í›ˆë ¨ ì‹œ (ê·¸ë˜ë””ì–¸íŠ¸, ì˜µí‹°ë§ˆì´ì € ìƒíƒœ í¬í•¨)
FP32: ~84GB (3ë°° ì¦ê°€)
FP16: ~42GB (Mixed Precision ì‚¬ìš© ì‹œ)
BF16: ~42GB (Mixed Precision ì‚¬ìš© ì‹œ)
```

## ğŸ”¢ LLaMA Factory ì •ë°€ë„ ì˜µì…˜

### 1. fp16 (16-bit Floating Point)

**ì„¤ëª…**: IEEE 754 í‘œì¤€ì˜ 16ë¹„íŠ¸ ë¶€ë™ì†Œìˆ˜ì  í˜•ì‹

```yaml
fp16: true  # ê¸°ë³¸ê°’: false
bf16: false  # fp16ê³¼ bf16ì€ ë™ì‹œ ì‚¬ìš© ë¶ˆê°€
```

**ë‚´ë¶€ êµ¬ì¡°**:
```python
# FP16 ë¹„íŠ¸ êµ¬ì„± (ì´ 16ë¹„íŠ¸)
Sign bit:     1 bit  (ë¶€í˜¸)
Exponent:     5 bits (ì§€ìˆ˜, -14 ~ +15)
Mantissa:     10 bits (ê°€ìˆ˜)

# í‘œí˜„ ë²”ìœ„
ìµœì†Œê°’: ~6.1 Ã— 10^-5
ìµœëŒ€ê°’: ~65,504
ì •ë°€ë„: ì•½ 3-4 ìœ íš¨ ìˆ«ì
```

**ì¥ì **:
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 50% ì ˆì•½
- GPU ì—°ì‚° ì†ë„ í–¥ìƒ (íŠ¹íˆ Tensor Core í™œìš©)
- ëŒ€ë¶€ë¶„ì˜ í˜„ëŒ€ GPUì—ì„œ ì§€ì›

**ë‹¨ì **:
- ìˆ˜ì¹˜ì  ë¶ˆì•ˆì •ì„± (gradient underflow)
- ì œí•œëœ í‘œí˜„ ë²”ìœ„
- ì‘ì€ ê·¸ë˜ë””ì–¸íŠ¸ ì†ì‹¤ ìœ„í—˜

**ì í•©í•œ ìƒí™©**:
- V100, A100, RTX 30/40 ì‹œë¦¬ì¦ˆ GPU
- ë©”ëª¨ë¦¬ ì œì•½ì´ ìˆëŠ” í™˜ê²½
- ëŒ€ë¶€ë¶„ì˜ ì¼ë°˜ì ì¸ íŒŒì¸íŠœë‹ ì‘ì—…

### 2. bf16 (Brain Float 16)

**ì„¤ëª…**: Googleì´ ê°œë°œí•œ 16ë¹„íŠ¸ í˜•ì‹, ë¨¸ì‹ ëŸ¬ë‹ì— ìµœì í™”

```yaml
bf16: true   # ê¸°ë³¸ê°’: false  
fp16: false  # bf16ê³¼ fp16ì€ ë™ì‹œ ì‚¬ìš© ë¶ˆê°€
```

**ë‚´ë¶€ êµ¬ì¡°**:
```python
# BF16 ë¹„íŠ¸ êµ¬ì„± (ì´ 16ë¹„íŠ¸)
Sign bit:     1 bit  (ë¶€í˜¸)
Exponent:     8 bits (ì§€ìˆ˜, FP32ì™€ ë™ì¼ ë²”ìœ„)
Mantissa:     7 bits (ê°€ìˆ˜)

# í‘œí˜„ ë²”ìœ„
ìµœì†Œê°’: ~1.18 Ã— 10^-38 (FP32ì™€ ë™ì¼)
ìµœëŒ€ê°’: ~3.39 Ã— 10^38  (FP32ì™€ ë™ì¼)
ì •ë°€ë„: ì•½ 2-3 ìœ íš¨ ìˆ«ì
```

**ì¥ì **:
- FP32ì™€ ë™ì¼í•œ í‘œí˜„ ë²”ìœ„ (overflow ì—†ìŒ)
- ìˆ˜ì¹˜ì  ì•ˆì •ì„±ì´ FP16ë³´ë‹¤ ìš°ìˆ˜
- ê·¸ë˜ë””ì–¸íŠ¸ ì–¸ë”í”Œë¡œìš° ìœ„í—˜ ë‚®ìŒ
- ê°„ë‹¨í•œ FP32 â†’ BF16 ë³€í™˜

**ë‹¨ì **:
- FP16ë³´ë‹¤ ë‚®ì€ ì •ë°€ë„ (7ë¹„íŠ¸ vs 10ë¹„íŠ¸ ê°€ìˆ˜)
- ìƒëŒ€ì ìœ¼ë¡œ ìƒˆë¡œìš´ í•˜ë“œì›¨ì–´ì—ì„œë§Œ ì™„ì „ ì§€ì›
- TPU, ìµœì‹  GPUì—ì„œë§Œ ìµœì í™”

**ì í•©í•œ ìƒí™©**:
- Ampere ì•„í‚¤í…ì²˜ ì´ìƒ (A100, RTX 30/40 ì‹œë¦¬ì¦ˆ)
- ìˆ˜ì¹˜ì  ì•ˆì •ì„±ì´ ì¤‘ìš”í•œ ê²½ìš°
- í° ëª¨ë¸ í›ˆë ¨ (gradient overflow ë°©ì§€)

### 3. FP32 (ê¸°ë³¸ ì •ë°€ë„)

**ì„¤ëª…**: 32ë¹„íŠ¸ ë‹¨ì •ë°€ë„ ë¶€ë™ì†Œìˆ˜ì  (ê¸°ë³¸ê°’)

```yaml
fp16: false  # ê¸°ë³¸ê°’
bf16: false  # ê¸°ë³¸ê°’
# FP32ê°€ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©ë¨
```

**ë‚´ë¶€ êµ¬ì¡°**:
```python
# FP32 ë¹„íŠ¸ êµ¬ì„± (ì´ 32ë¹„íŠ¸)
Sign bit:     1 bit  (ë¶€í˜¸)
Exponent:     8 bits (ì§€ìˆ˜)
Mantissa:     23 bits (ê°€ìˆ˜)

# í‘œí˜„ ë²”ìœ„
ìµœì†Œê°’: ~1.18 Ã— 10^-38
ìµœëŒ€ê°’: ~3.39 Ã— 10^38
ì •ë°€ë„: ì•½ 7 ìœ íš¨ ìˆ«ì
```

**ì¥ì **:
- ìµœê³ ì˜ ìˆ˜ì¹˜ì  ì•ˆì •ì„±
- ëª¨ë“  í•˜ë“œì›¨ì–´ì—ì„œ ì§€ì›
- í‘œì¤€ì ì´ê³  ì•ˆì „í•œ ì„ íƒ

**ë‹¨ì **:
- ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
- ëŠë¦° ì—°ì‚° ì†ë„
- ì œí•œëœ ë°°ì¹˜ í¬ê¸°

**ì í•©í•œ ìƒí™©**:
- ì¶©ë¶„í•œ ë©”ëª¨ë¦¬ê°€ ìˆëŠ” í™˜ê²½
- ìˆ˜ì¹˜ì  ì •í™•ì„±ì´ ìµœìš°ì„ ì¸ ì—°êµ¬
- ë””ë²„ê¹… ë° ê¸°ì¤€ ì„¤ì •

## âš™ï¸ Mixed Precision Training

### ê°œë…

**Mixed Precision**ì€ FP16/BF16ê³¼ FP32ë¥¼ í˜¼í•©í•˜ì—¬ ì‚¬ìš©í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.

```python
# Mixed Precision ë™ì‘ ë°©ì‹
Forward Pass: FP16/BF16ìœ¼ë¡œ ê³„ì‚° (ë¹ ë¥¸ ì†ë„)
Loss ê³„ì‚°: FP32ë¡œ ìœ ì§€ (ì •í™•ì„±)
Backward Pass: FP16/BF16ìœ¼ë¡œ ê³„ì‚°
Gradient: FP32ë¡œ ë³€í™˜í•˜ì—¬ ì—…ë°ì´íŠ¸ (ì•ˆì •ì„±)
Weight Update: FP32ì—ì„œ ìˆ˜í–‰ í›„ FP16/BF16ìœ¼ë¡œ ë³€í™˜
```

### LLaMA Factoryì—ì„œì˜ Mixed Precision

```yaml
# FP16 Mixed Precision
fp16: true
dataloader_pin_memory: true  # ë©”ëª¨ë¦¬ ìµœì í™”
gradient_checkpointing: true  # ì¶”ê°€ ë©”ëª¨ë¦¬ ì ˆì•½

# BF16 Mixed Precision  
bf16: true
tf32: true  # NVIDIA Ampere GPUì—ì„œ ì¶”ê°€ ìµœì í™”
```

### Loss Scaling (FP16 ì „ìš©)

FP16 ì‚¬ìš© ì‹œ gradient underflow ë°©ì§€ë¥¼ ìœ„í•œ ê¸°ë²•:

```python
# Loss Scaling ë™ì‘
1. Lossì— í° ìˆ˜(scale factor)ë¥¼ ê³±í•¨
2. Gradientì—ë„ ê°™ì€ scaleì´ ì ìš©ë¨
3. Weight ì—…ë°ì´íŠ¸ ì‹œ scaleë¡œ ë‚˜ëˆ„ì–´ ì›ë˜ ê°’ ë³µì›

# ìë™ ì¡°ì •
if gradientê°€ overflow ë°œìƒ:
    scale_factor /= 2  # ìŠ¤ì¼€ì¼ ê°ì†Œ
else:
    scale_factor *= 2  # ìŠ¤ì¼€ì¼ ì¦ê°€ (ì ì§„ì )
```

## ğŸ–¥ï¸ í•˜ë“œì›¨ì–´ë³„ ìµœì  ì„¤ì •

### NVIDIA GPUë³„ ê¶Œì¥ì‚¬í•­

#### RTX 3080/3090 (Ampere)
```yaml
# 1ì°¨ ê¶Œì¥: BF16
bf16: true
tf32: true
flash_attn: auto

# 2ì°¨ ê¶Œì¥: FP16 (í˜¸í™˜ì„±)
fp16: true
dataloader_pin_memory: true
```

#### RTX 4060/4070/4080/4090 (Ada Lovelace)
```yaml
# ìµœì  ì„¤ì •
bf16: true
tf32: true
flash_attn: auto
gradient_checkpointing: true
```

#### A100/H100 (ë°ì´í„°ì„¼í„° ê¸‰)
```yaml
# ê³ ì„±ëŠ¥ ì„¤ì •
bf16: true
tf32: true
flash_attn: auto
per_device_train_batch_size: 16  # í° ë°°ì¹˜ í¬ê¸° ê°€ëŠ¥
```

#### GTX 1080/RTX 20 ì‹œë¦¬ì¦ˆ (êµ¬í˜•)
```yaml
# í˜¸í™˜ì„± ìš°ì„ 
fp16: false  # FP32 ì‚¬ìš©
bf16: false
per_device_train_batch_size: 2  # ì‘ì€ ë°°ì¹˜
gradient_accumulation_steps: 8
```

### AMD GPU (ROCm)
```yaml
# ì•ˆì „í•œ ì„¤ì •
fp16: true   # BF16 ì§€ì› ì œí•œì 
tf32: false  # NVIDIA ì „ìš©
flash_attn: false  # í˜¸í™˜ì„± ì´ìŠˆ ê°€ëŠ¥
```

### Intel GPU (Arc)
```yaml
# ë³´ìˆ˜ì  ì„¤ì •
fp16: false  # ì•ˆì •ì„± ìš°ì„ 
bf16: false
per_device_train_batch_size: 4
```

## ğŸ“Š ì„±ëŠ¥ ë¹„êµ ë° ë²¤ì¹˜ë§ˆí¬

### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì‹¤ì¸¡ (Llama-3-8B ê¸°ì¤€)

```python
# LoRA íŒŒì¸íŠœë‹ ê¸°ì¤€
ì„¤ì •: batch_size=4, sequence_length=2048

FP32:
- ëª¨ë¸ ë¡œë”©: 28GB
- í›ˆë ¨ ì¤‘ í”¼í¬: 32GB
- ì‚¬ìš© ê°€ëŠ¥ ìµœì†Œ VRAM: 40GB

FP16:
- ëª¨ë¸ ë¡œë”©: 14GB  
- í›ˆë ¨ ì¤‘ í”¼í¬: 18GB
- ì‚¬ìš© ê°€ëŠ¥ ìµœì†Œ VRAM: 24GB

BF16:
- ëª¨ë¸ ë¡œë”©: 14GB
- í›ˆë ¨ ì¤‘ í”¼í¬: 18GB  
- ì‚¬ìš© ê°€ëŠ¥ ìµœì†Œ VRAM: 24GB
```

### í›ˆë ¨ ì†ë„ ë¹„êµ

```python
# RTX 4090 ê¸°ì¤€, Llama-3-8B LoRA
FP32: 100% (ê¸°ì¤€)
FP16: 140-160% (40-60% í–¥ìƒ)
BF16: 130-150% (30-50% í–¥ìƒ)

# A100 ê¸°ì¤€
FP32: 100% (ê¸°ì¤€)  
FP16: 180-200% (80-100% í–¥ìƒ)
BF16: 170-190% (70-90% í–¥ìƒ)
```

### ìˆ˜ì¹˜ì  ì•ˆì •ì„± í…ŒìŠ¤íŠ¸

```python
# Gradient Overflow ë°œìƒë¥  (ì‹¤í—˜ì  ìˆ˜ì¹˜)
FP32: 0% (overflow ì—†ìŒ)
BF16: 0.1% (ë§¤ìš° ë‚®ìŒ)
FP16: 2-5% (ìƒëŒ€ì ìœ¼ë¡œ ë†’ìŒ, Loss Scalingìœ¼ë¡œ í•´ê²°)

# ìµœì¢… ëª¨ë¸ í’ˆì§ˆ (BLEU Score ê¸°ì¤€)
FP32: 100% (ê¸°ì¤€)
BF16: 99.8% (ê±°ì˜ ë™ì¼)
FP16: 99.5% (ì•½ê°„ì˜ í’ˆì§ˆ ì €í•˜)
```

## ğŸ¯ ì‹¤ë¬´ ì‚¬ë¡€ ë¶„ì„

### ì‚¬ë¡€ 1: ITEASY ê³ ê° ì§€ì› ì±—ë´‡ (RTX 4090)

**ì´ˆê¸° ì„¤ì • (ë©”ëª¨ë¦¬ ë¶€ì¡±)**:
```yaml
model_name_or_path: Qwen/Qwen2.5-7B-Instruct
fp16: false  # FP32 ì‚¬ìš©
bf16: false
per_device_train_batch_size: 8

ê²°ê³¼: CUDA out of memory ì˜¤ë¥˜
í•„ìš” ë©”ëª¨ë¦¬: ~32GB > 24GB VRAM
```

**ìµœì í™”ëœ ì„¤ì •**:
```yaml
model_name_or_path: Qwen/Qwen2.5-7B-Instruct
bf16: true      # ë©”ëª¨ë¦¬ 50% ì ˆì•½
tf32: true      # ì¶”ê°€ ìµœì í™”
per_device_train_batch_size: 8
gradient_checkpointing: true
flash_attn: auto

ê²°ê³¼:
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: ~18GB (ì—¬ìœ  ìˆìŒ)
- í›ˆë ¨ ì†ë„: 50% í–¥ìƒ
- ëª¨ë¸ í’ˆì§ˆ: FP32 ëŒ€ë¹„ 99.8% ìœ ì§€
```

### ì‚¬ë¡€ 2: ê¸°ìˆ  ë¬¸ì„œ ìš”ì•½ (RTX 3080 10GB)

**ì œì•½ ì¡°ê±´**: 10GB VRAM, ê¸´ ë¬¸ì„œ ì²˜ë¦¬

**ìµœì í™” ê³¼ì •**:
```yaml
# 1ì°¨ ì‹œë„ (ì‹¤íŒ¨)
fp16: false
cutoff_len: 4096
per_device_train_batch_size: 4
ê²°ê³¼: Out of memory

# 2ì°¨ ì‹œë„ (ë¶€ë¶„ ì„±ê³µ)  
fp16: true
cutoff_len: 2048
per_device_train_batch_size: 2
ê²°ê³¼: í›ˆë ¨ ê°€ëŠ¥í•˜ì§€ë§Œ ëŠë¦¼

# ìµœì¢… ìµœì í™”
fp16: true
cutoff_len: 2048
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
gradient_checkpointing: true
dataloader_pin_memory: true

ê²°ê³¼:
- ë©”ëª¨ë¦¬: 9.2GB (ì•ˆì •ì )
- ì†ë„: ì ì ˆí•¨
- í’ˆì§ˆ: ëª©í‘œ ë‹¬ì„±
```

### ì‚¬ë¡€ 3: ëŒ€ê·œëª¨ í›ˆë ¨ (A100 80GB Ã— 4)

**ìš”êµ¬ì‚¬í•­**: ìµœê³  ì„±ëŠ¥, 70B ëª¨ë¸ í›ˆë ¨

**ìµœì  ì„¤ì •**:
```yaml
model_name_or_path: meta-llama/Llama-2-70b-hf
bf16: true           # ìˆ˜ì¹˜ ì•ˆì •ì„±
tf32: true           # í•˜ë“œì›¨ì–´ ìµœì í™”
flash_attn: auto     # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
fsdp: full_shard     # ëª¨ë¸ ë¶„ì‚°
per_device_train_batch_size: 4
gradient_accumulation_steps: 2

ê²°ê³¼:
- 4 GPUì— ëª¨ë¸ ë¶„ì‚°
- GPUë‹¹ ë©”ëª¨ë¦¬: ~65GB ì‚¬ìš©
- í›ˆë ¨ ì†ë„: ìµœì í™”ë¨
- ìˆ˜ì¹˜ ì•ˆì •ì„±: ìš°ìˆ˜
```

## ğŸš¨ ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

### ë¬¸ì œ 1: CUDA Out of Memory

**ì¦ìƒ**:
```bash
RuntimeError: CUDA out of memory. Tried to allocate 2.00 GiB
```

**í•´ê²° ë‹¨ê³„**:
```yaml
# 1ë‹¨ê³„: ì •ë°€ë„ ë³€ê²½
fp16: true  # ë˜ëŠ” bf16: true

# 2ë‹¨ê³„: ë°°ì¹˜ í¬ê¸° ì¡°ì •
per_device_train_batch_size: 1
gradient_accumulation_steps: 8

# 3ë‹¨ê³„: ì¶”ê°€ ìµœì í™”
gradient_checkpointing: true
dataloader_pin_memory: true

# 4ë‹¨ê³„: ì‹œí€€ìŠ¤ ê¸¸ì´ ì¡°ì •
cutoff_len: 1024  # 2048ì—ì„œ ì¤„ì„
```

### ë¬¸ì œ 2: NaN Loss (FP16 ì‚¬ìš© ì‹œ)

**ì¦ìƒ**:
```python
Step 100: loss = nan
Step 101: loss = nan
```

**ì›ì¸**: Gradient underflow ë˜ëŠ” overflow

**í•´ê²° ë°©ë²•**:
```yaml
# ë°©ë²• 1: BF16 ì‚¬ìš©
fp16: false
bf16: true

# ë°©ë²• 2: í•™ìŠµë¥  ì¡°ì •
learning_rate: 3e-5  # 5e-5ì—ì„œ ì¤„ì„

# ë°©ë²• 3: Warmup ì¦ê°€
warmup_steps: 200  # 100ì—ì„œ ì¦ê°€

# ë°©ë²• 4: Loss Scaling ì¡°ì • (ìë™ìœ¼ë¡œ ì²˜ë¦¬ë¨)
fp16_full_eval: true
```

### ë¬¸ì œ 3: ì„±ëŠ¥ ì €í•˜

**ì¦ìƒ**: FP16/BF16 ì‚¬ìš© ì‹œ ì˜ˆìƒë³´ë‹¤ ë‚®ì€ ì„±ëŠ¥

**ì§„ë‹¨ ë° í•´ê²°**:
```yaml
# 1. ìˆ˜ì¹˜ ì •ë°€ë„ ë¬¸ì œ í™•ì¸
eval_steps: 50  # ìì£¼ í‰ê°€í•˜ì—¬ ëª¨ë‹ˆí„°ë§

# 2. BF16 ì‹œë„ (FP16 ëŒ€ì‹ )
fp16: false
bf16: true

# 3. Mixed Precision ì„¸ë°€ ì¡°ì •
fp16_full_eval: true  # í‰ê°€ëŠ” FP32ë¡œ

# 4. ë§ˆì§€ë§‰ ìˆ˜ë‹¨: FP32 ë³µê·€
fp16: false
bf16: false
```

## ğŸ’¡ ê³ ê¸‰ ìµœì í™” ê¸°ë²•

### 1. ë™ì  ì •ë°€ë„ ì „í™˜

```yaml
# Phase 1: ì•ˆì •ì  ì‹œì‘ (FP32)
num_train_epochs: 1
fp16: false
bf16: false

# Phase 2: íš¨ìœ¨ì  í›ˆë ¨ (BF16)
num_train_epochs: 2  # ì¶”ê°€ 2 ì—í¬í¬
bf16: true
load_best_model_at_end: true
```

### 2. ê³„ì¸µë³„ ì •ë°€ë„ (ê³ ê¸‰)

```python
# ì˜ì‚¬ì½”ë“œ: ì¤‘ìš”í•œ ë ˆì´ì–´ëŠ” FP32, ë‚˜ë¨¸ì§€ëŠ” FP16
attention_layers: FP32  # ì •í™•ì„± ì¤‘ìš”
feed_forward: FP16      # ì†ë„ ìš°ì„ 
output_layer: FP32      # ìµœì¢… ì¶œë ¥ ì •í™•ì„±
```

### 3. ë©”ëª¨ë¦¬ ìµœì í™” ì¡°í•©

```yaml
# ê·¹í•œ ë©”ëª¨ë¦¬ ìµœì í™”
bf16: true
gradient_checkpointing: true
dataloader_pin_memory: true
per_device_train_batch_size: 1
gradient_accumulation_steps: 16
optim: "adafactor"  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì˜µí‹°ë§ˆì´ì €
```

### 4. í’ˆì§ˆ vs íš¨ìœ¨ì„± ê· í˜•

```yaml
# ê· í˜•ì¡íŒ ì„¤ì •
bf16: true              # ì¢‹ì€ ìˆ˜ì¹˜ ì•ˆì •ì„±
tf32: true              # í•˜ë“œì›¨ì–´ ìµœì í™”
flash_attn: auto        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
fp16_full_eval: true    # í‰ê°€ëŠ” ì •í™•í•˜ê²Œ
```

## ğŸ“Š í•˜ë“œì›¨ì–´ë³„ ìµœì  ì„¤ì • ë§¤íŠ¸ë¦­ìŠ¤

| GPU ëª¨ë¸ | VRAM | ê¶Œì¥ ì •ë°€ë„ | ë°°ì¹˜ í¬ê¸° | ì¶”ê°€ ìµœì í™”           |
| -------- | ---- | ----------- | --------- | --------------------- |
| RTX 4090 | 24GB | BF16        | 8-16      | TF32, Flash Attn      |
| RTX 4080 | 16GB | BF16        | 4-8       | TF32, Grad Checkpoint |
| RTX 4070 | 12GB | FP16        | 2-4       | Grad Checkpoint       |
| RTX 3080 | 10GB | FP16        | 1-2       | ëª¨ë“  ìµœì í™”           |
| RTX 3060 | 8GB  | FP16        | 1         | QLoRA + ìµœì í™”        |
| A100     | 80GB | BF16        | 16-32     | ëª¨ë“  ê¸°ëŠ¥ í™œìš©        |
| V100     | 32GB | FP16        | 8-16      | Mixed Precision       |

## ğŸ“‹ ìµœì¢… ê¶Œì¥ì‚¬í•­

### ì¼ë°˜ì ì¸ ê°€ì´ë“œë¼ì¸

1. **í˜„ëŒ€ GPU (RTX 30/40, A100)**: BF16 ìš°ì„ 
2. **êµ¬í˜• GPU (RTX 20, GTX)**: FP16 ë˜ëŠ” FP32
3. **ë©”ëª¨ë¦¬ ì œì•½**: FP16/BF16 + ì¶”ê°€ ìµœì í™”
4. **í’ˆì§ˆ ìš°ì„ **: FP32 (ì¶©ë¶„í•œ ë©”ëª¨ë¦¬ ì‹œ)

### ë¹ ë¥¸ ì„ íƒ ê°€ì´ë“œ

```yaml
# ğŸš€ ì²˜ìŒ ì‚¬ìš©ì (ì•ˆì „)
bf16: true
tf32: true
gradient_checkpointing: true

# âš¡ ë©”ëª¨ë¦¬ ì œì•½ (ìµœì í™”)
fp16: true
gradient_checkpointing: true
per_device_train_batch_size: 1
gradient_accumulation_steps: 8

# ğŸ¯ ê³ ì„±ëŠ¥ (A100/H100)
bf16: true
tf32: true
flash_attn: auto
per_device_train_batch_size: 16

# ğŸ”¬ ì—°êµ¬ ëª©ì  (ì •í™•ì„±)
fp16: false  # FP32 ì‚¬ìš©
bf16: false
eval_strategy: steps
eval_steps: 100
```

ì •ë°€ë„ ì„¤ì •ì€ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±, í›ˆë ¨ ì†ë„, ëª¨ë¸ í’ˆì§ˆì˜ ê· í˜•ì„ ë§ì¶”ëŠ” í•µì‹¬ ìš”ì†Œì…ë‹ˆë‹¤. í•˜ë“œì›¨ì–´ í™˜ê²½ê³¼ í”„ë¡œì íŠ¸ ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” ì ì ˆí•œ ì„ íƒì„ í†µí•´ ìµœì ì˜ í›ˆë ¨ í™˜ê²½ì„ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!

