---
title: 하이퍼파라미터 이해하기 - 04
date: 2025-08-18 11:02:03 +0900
categories: [artificial intelligence, machine learning]
tags: [machine learning, deep learning, hyperparameters, finetuning, llm, llamafactory, pytorch, optimization, training, precision, ai]     # TAG names should always be lowercase
---

# 정밀도(Precision) 이해하기

## 📚 개념 이해

### 정밀도(Precision)란?

**정밀도**는 모델 훈련 시 가중치와 그래디언트를 표현하는 데 사용되는 부동소수점 수의 비트 수를 의미합니다. 메모리 사용량과 연산 속도, 그리고 수치적 안정성에 직접적인 영향을 미칩니다.

```python
# 기본 개념
FP32 (32-bit): 완전한 정밀도, 최고 품질, 높은 메모리 사용
FP16 (16-bit): 절반 정밀도, 메모리 절약, 속도 향상
BF16 (16-bit): Brain Float 16, FP16의 개선된 버전
```

### 메모리 사용량 비교

```python
# 모델 크기별 메모리 사용량 (7B 파라미터 모델 기준)
FP32: 7B × 4 bytes = 28GB
FP16: 7B × 2 bytes = 14GB  (50% 절약)
BF16: 7B × 2 bytes = 14GB  (50% 절약)

# 실제 훈련 시 (그래디언트, 옵티마이저 상태 포함)
FP32: ~84GB (3배 증가)
FP16: ~42GB (Mixed Precision 사용 시)
BF16: ~42GB (Mixed Precision 사용 시)
```

## 🔢 LLaMA Factory 정밀도 옵션

### 1. fp16 (16-bit Floating Point)

**설명**: IEEE 754 표준의 16비트 부동소수점 형식

```yaml
fp16: true  # 기본값: false
bf16: false  # fp16과 bf16은 동시 사용 불가
```

**내부 구조**:
```python
# FP16 비트 구성 (총 16비트)
Sign bit:     1 bit  (부호)
Exponent:     5 bits (지수, -14 ~ +15)
Mantissa:     10 bits (가수)

# 표현 범위
최소값: ~6.1 × 10^-5
최대값: ~65,504
정밀도: 약 3-4 유효 숫자
```

**장점**:
- 메모리 사용량 50% 절약
- GPU 연산 속도 향상 (특히 Tensor Core 활용)
- 대부분의 현대 GPU에서 지원

**단점**:
- 수치적 불안정성 (gradient underflow)
- 제한된 표현 범위
- 작은 그래디언트 손실 위험

**적합한 상황**:
- V100, A100, RTX 30/40 시리즈 GPU
- 메모리 제약이 있는 환경
- 대부분의 일반적인 파인튜닝 작업

### 2. bf16 (Brain Float 16)

**설명**: Google이 개발한 16비트 형식, 머신러닝에 최적화

```yaml
bf16: true   # 기본값: false  
fp16: false  # bf16과 fp16은 동시 사용 불가
```

**내부 구조**:
```python
# BF16 비트 구성 (총 16비트)
Sign bit:     1 bit  (부호)
Exponent:     8 bits (지수, FP32와 동일 범위)
Mantissa:     7 bits (가수)

# 표현 범위
최소값: ~1.18 × 10^-38 (FP32와 동일)
최대값: ~3.39 × 10^38  (FP32와 동일)
정밀도: 약 2-3 유효 숫자
```

**장점**:
- FP32와 동일한 표현 범위 (overflow 없음)
- 수치적 안정성이 FP16보다 우수
- 그래디언트 언더플로우 위험 낮음
- 간단한 FP32 → BF16 변환

**단점**:
- FP16보다 낮은 정밀도 (7비트 vs 10비트 가수)
- 상대적으로 새로운 하드웨어에서만 완전 지원
- TPU, 최신 GPU에서만 최적화

**적합한 상황**:
- Ampere 아키텍처 이상 (A100, RTX 30/40 시리즈)
- 수치적 안정성이 중요한 경우
- 큰 모델 훈련 (gradient overflow 방지)

### 3. FP32 (기본 정밀도)

**설명**: 32비트 단정밀도 부동소수점 (기본값)

```yaml
fp16: false  # 기본값
bf16: false  # 기본값
# FP32가 기본으로 사용됨
```

**내부 구조**:
```python
# FP32 비트 구성 (총 32비트)
Sign bit:     1 bit  (부호)
Exponent:     8 bits (지수)
Mantissa:     23 bits (가수)

# 표현 범위
최소값: ~1.18 × 10^-38
최대값: ~3.39 × 10^38
정밀도: 약 7 유효 숫자
```

**장점**:
- 최고의 수치적 안정성
- 모든 하드웨어에서 지원
- 표준적이고 안전한 선택

**단점**:
- 높은 메모리 사용량
- 느린 연산 속도
- 제한된 배치 크기

**적합한 상황**:
- 충분한 메모리가 있는 환경
- 수치적 정확성이 최우선인 연구
- 디버깅 및 기준 설정

## ⚙️ Mixed Precision Training

### 개념

**Mixed Precision**은 FP16/BF16과 FP32를 혼합하여 사용하는 기법입니다.

```python
# Mixed Precision 동작 방식
Forward Pass: FP16/BF16으로 계산 (빠른 속도)
Loss 계산: FP32로 유지 (정확성)
Backward Pass: FP16/BF16으로 계산
Gradient: FP32로 변환하여 업데이트 (안정성)
Weight Update: FP32에서 수행 후 FP16/BF16으로 변환
```

### LLaMA Factory에서의 Mixed Precision

```yaml
# FP16 Mixed Precision
fp16: true
dataloader_pin_memory: true  # 메모리 최적화
gradient_checkpointing: true  # 추가 메모리 절약

# BF16 Mixed Precision  
bf16: true
tf32: true  # NVIDIA Ampere GPU에서 추가 최적화
```

### Loss Scaling (FP16 전용)

FP16 사용 시 gradient underflow 방지를 위한 기법:

```python
# Loss Scaling 동작
1. Loss에 큰 수(scale factor)를 곱함
2. Gradient에도 같은 scale이 적용됨
3. Weight 업데이트 시 scale로 나누어 원래 값 복원

# 자동 조정
if gradient가 overflow 발생:
    scale_factor /= 2  # 스케일 감소
else:
    scale_factor *= 2  # 스케일 증가 (점진적)
```

## 🖥️ 하드웨어별 최적 설정

### NVIDIA GPU별 권장사항

#### RTX 3080/3090 (Ampere)
```yaml
# 1차 권장: BF16
bf16: true
tf32: true
flash_attn: auto

# 2차 권장: FP16 (호환성)
fp16: true
dataloader_pin_memory: true
```

#### RTX 4060/4070/4080/4090 (Ada Lovelace)
```yaml
# 최적 설정
bf16: true
tf32: true
flash_attn: auto
gradient_checkpointing: true
```

#### A100/H100 (데이터센터 급)
```yaml
# 고성능 설정
bf16: true
tf32: true
flash_attn: auto
per_device_train_batch_size: 16  # 큰 배치 크기 가능
```

#### GTX 1080/RTX 20 시리즈 (구형)
```yaml
# 호환성 우선
fp16: false  # FP32 사용
bf16: false
per_device_train_batch_size: 2  # 작은 배치
gradient_accumulation_steps: 8
```

### AMD GPU (ROCm)
```yaml
# 안전한 설정
fp16: true   # BF16 지원 제한적
tf32: false  # NVIDIA 전용
flash_attn: false  # 호환성 이슈 가능
```

### Intel GPU (Arc)
```yaml
# 보수적 설정
fp16: false  # 안정성 우선
bf16: false
per_device_train_batch_size: 4
```

## 📊 성능 비교 및 벤치마크

### 메모리 사용량 실측 (Llama-3-8B 기준)

```python
# LoRA 파인튜닝 기준
설정: batch_size=4, sequence_length=2048

FP32:
- 모델 로딩: 28GB
- 훈련 중 피크: 32GB
- 사용 가능 최소 VRAM: 40GB

FP16:
- 모델 로딩: 14GB  
- 훈련 중 피크: 18GB
- 사용 가능 최소 VRAM: 24GB

BF16:
- 모델 로딩: 14GB
- 훈련 중 피크: 18GB  
- 사용 가능 최소 VRAM: 24GB
```

### 훈련 속도 비교

```python
# RTX 4090 기준, Llama-3-8B LoRA
FP32: 100% (기준)
FP16: 140-160% (40-60% 향상)
BF16: 130-150% (30-50% 향상)

# A100 기준
FP32: 100% (기준)  
FP16: 180-200% (80-100% 향상)
BF16: 170-190% (70-90% 향상)
```

### 수치적 안정성 테스트

```python
# Gradient Overflow 발생률 (실험적 수치)
FP32: 0% (overflow 없음)
BF16: 0.1% (매우 낮음)
FP16: 2-5% (상대적으로 높음, Loss Scaling으로 해결)

# 최종 모델 품질 (BLEU Score 기준)
FP32: 100% (기준)
BF16: 99.8% (거의 동일)
FP16: 99.5% (약간의 품질 저하)
```

## 🎯 실무 사례 분석

### 사례 1: ITEASY 고객 지원 챗봇 (RTX 4090)

**초기 설정 (메모리 부족)**:
```yaml
model_name_or_path: Qwen/Qwen2.5-7B-Instruct
fp16: false  # FP32 사용
bf16: false
per_device_train_batch_size: 8

결과: CUDA out of memory 오류
필요 메모리: ~32GB > 24GB VRAM
```

**최적화된 설정**:
```yaml
model_name_or_path: Qwen/Qwen2.5-7B-Instruct
bf16: true      # 메모리 50% 절약
tf32: true      # 추가 최적화
per_device_train_batch_size: 8
gradient_checkpointing: true
flash_attn: auto

결과:
- 메모리 사용량: ~18GB (여유 있음)
- 훈련 속도: 50% 향상
- 모델 품질: FP32 대비 99.8% 유지
```

### 사례 2: 기술 문서 요약 (RTX 3080 10GB)

**제약 조건**: 10GB VRAM, 긴 문서 처리

**최적화 과정**:
```yaml
# 1차 시도 (실패)
fp16: false
cutoff_len: 4096
per_device_train_batch_size: 4
결과: Out of memory

# 2차 시도 (부분 성공)  
fp16: true
cutoff_len: 2048
per_device_train_batch_size: 2
결과: 훈련 가능하지만 느림

# 최종 최적화
fp16: true
cutoff_len: 2048
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
gradient_checkpointing: true
dataloader_pin_memory: true

결과:
- 메모리: 9.2GB (안정적)
- 속도: 적절함
- 품질: 목표 달성
```

### 사례 3: 대규모 훈련 (A100 80GB × 4)

**요구사항**: 최고 성능, 70B 모델 훈련

**최적 설정**:
```yaml
model_name_or_path: meta-llama/Llama-2-70b-hf
bf16: true           # 수치 안정성
tf32: true           # 하드웨어 최적화
flash_attn: auto     # 메모리 효율성
fsdp: full_shard     # 모델 분산
per_device_train_batch_size: 4
gradient_accumulation_steps: 2

결과:
- 4 GPU에 모델 분산
- GPU당 메모리: ~65GB 사용
- 훈련 속도: 최적화됨
- 수치 안정성: 우수
```

## 🚨 문제 해결 가이드

### 문제 1: CUDA Out of Memory

**증상**:
```bash
RuntimeError: CUDA out of memory. Tried to allocate 2.00 GiB
```

**해결 단계**:
```yaml
# 1단계: 정밀도 변경
fp16: true  # 또는 bf16: true

# 2단계: 배치 크기 조정
per_device_train_batch_size: 1
gradient_accumulation_steps: 8

# 3단계: 추가 최적화
gradient_checkpointing: true
dataloader_pin_memory: true

# 4단계: 시퀀스 길이 조정
cutoff_len: 1024  # 2048에서 줄임
```

### 문제 2: NaN Loss (FP16 사용 시)

**증상**:
```python
Step 100: loss = nan
Step 101: loss = nan
```

**원인**: Gradient underflow 또는 overflow

**해결 방법**:
```yaml
# 방법 1: BF16 사용
fp16: false
bf16: true

# 방법 2: 학습률 조정
learning_rate: 3e-5  # 5e-5에서 줄임

# 방법 3: Warmup 증가
warmup_steps: 200  # 100에서 증가

# 방법 4: Loss Scaling 조정 (자동으로 처리됨)
fp16_full_eval: true
```

### 문제 3: 성능 저하

**증상**: FP16/BF16 사용 시 예상보다 낮은 성능

**진단 및 해결**:
```yaml
# 1. 수치 정밀도 문제 확인
eval_steps: 50  # 자주 평가하여 모니터링

# 2. BF16 시도 (FP16 대신)
fp16: false
bf16: true

# 3. Mixed Precision 세밀 조정
fp16_full_eval: true  # 평가는 FP32로

# 4. 마지막 수단: FP32 복귀
fp16: false
bf16: false
```

## 💡 고급 최적화 기법

### 1. 동적 정밀도 전환

```yaml
# Phase 1: 안정적 시작 (FP32)
num_train_epochs: 1
fp16: false
bf16: false

# Phase 2: 효율적 훈련 (BF16)
num_train_epochs: 2  # 추가 2 에포크
bf16: true
load_best_model_at_end: true
```

### 2. 계층별 정밀도 (고급)

```python
# 의사코드: 중요한 레이어는 FP32, 나머지는 FP16
attention_layers: FP32  # 정확성 중요
feed_forward: FP16      # 속도 우선
output_layer: FP32      # 최종 출력 정확성
```

### 3. 메모리 최적화 조합

```yaml
# 극한 메모리 최적화
bf16: true
gradient_checkpointing: true
dataloader_pin_memory: true
per_device_train_batch_size: 1
gradient_accumulation_steps: 16
optim: "adafactor"  # 메모리 효율적 옵티마이저
```

### 4. 품질 vs 효율성 균형

```yaml
# 균형잡힌 설정
bf16: true              # 좋은 수치 안정성
tf32: true              # 하드웨어 최적화
flash_attn: auto        # 메모리 효율성
fp16_full_eval: true    # 평가는 정확하게
```

## 📊 하드웨어별 최적 설정 매트릭스

| GPU 모델 | VRAM | 권장 정밀도 | 배치 크기 | 추가 최적화           |
| -------- | ---- | ----------- | --------- | --------------------- |
| RTX 4090 | 24GB | BF16        | 8-16      | TF32, Flash Attn      |
| RTX 4080 | 16GB | BF16        | 4-8       | TF32, Grad Checkpoint |
| RTX 4070 | 12GB | FP16        | 2-4       | Grad Checkpoint       |
| RTX 3080 | 10GB | FP16        | 1-2       | 모든 최적화           |
| RTX 3060 | 8GB  | FP16        | 1         | QLoRA + 최적화        |
| A100     | 80GB | BF16        | 16-32     | 모든 기능 활용        |
| V100     | 32GB | FP16        | 8-16      | Mixed Precision       |

## 📋 최종 권장사항

### 일반적인 가이드라인

1. **현대 GPU (RTX 30/40, A100)**: BF16 우선
2. **구형 GPU (RTX 20, GTX)**: FP16 또는 FP32
3. **메모리 제약**: FP16/BF16 + 추가 최적화
4. **품질 우선**: FP32 (충분한 메모리 시)

### 빠른 선택 가이드

```yaml
# 🚀 처음 사용자 (안전)
bf16: true
tf32: true
gradient_checkpointing: true

# ⚡ 메모리 제약 (최적화)
fp16: true
gradient_checkpointing: true
per_device_train_batch_size: 1
gradient_accumulation_steps: 8

# 🎯 고성능 (A100/H100)
bf16: true
tf32: true
flash_attn: auto
per_device_train_batch_size: 16

# 🔬 연구 목적 (정확성)
fp16: false  # FP32 사용
bf16: false
eval_strategy: steps
eval_steps: 100
```

정밀도 설정은 메모리 효율성, 훈련 속도, 모델 품질의 균형을 맞추는 핵심 요소입니다. 하드웨어 환경과 프로젝트 요구사항에 맞는 적절한 선택을 통해 최적의 훈련 환경을 구성할 수 있습니다!

