---
title: GPT-2 êµ¬í˜„ìœ¼ë¡œ ë°°ìš°ëŠ” Transformer ì™„ì „ ì •ë³µ ì‹œë¦¬ì¦ˆ - 07
date: 2025-08-12 08:10:27 +0900
categories: [machine learning, GPT]
tags: [machine learning, GPT, Transformer]       # TAG names should always be lowercase
---

# GPT ì™„ì „ ì •ë³µ 7í¸: í…ìŠ¤íŠ¸ ìƒì„±ì˜ ì˜ˆìˆ  - ì°½ì‘í•˜ëŠ” AI

> **ì´ì „ í¸ ìš”ì•½**: 6í¸ì—ì„œëŠ” ëª¨ë¸ì´ ì–´ë–»ê²Œ í•™ìŠµì„ í†µí•´ ì§€ëŠ¥ì„ íšë“í•˜ëŠ”ì§€ ë°°ì› ìŠµë‹ˆë‹¤. ì´ì œ í•™ìŠµëœ ëª¨ë¸ì´ ì–´ë–»ê²Œ ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ë¥¼ ì°½ì‘í•˜ëŠ”ì§€, ê·¸ ì˜ˆìˆ ì ì´ë©´ì„œë„ ê³¼í•™ì ì¸ ê³¼ì •ì„ ì™„ì „íˆ ì´í•´í•´ë³´ê² ìŠµë‹ˆë‹¤.

---

## ë“¤ì–´ê°€ë©°: ì°½ì‘ì˜ ìˆœê°„

ì‹œì¸ì´ ë¹ˆ ì¢…ì´ ì•ì—ì„œ ì²« ë‹¨ì–´ë¥¼ ê³ ë¯¼í•˜ëŠ” ìˆœê°„ì„ ìƒìƒí•´ë³´ì„¸ìš”:

```
ì‹œì¸ì˜ ê³ ë¯¼: "ì‚¬ë‘ì— ëŒ€í•œ ì‹œë¥¼ ì“°ê³  ì‹¶ì–´..."
ì²« ë²ˆì§¸ ë‹¨ì–´: "ì‚¬ë‘"? "ê·¸ëŒ€"? "ë°¤í•˜ëŠ˜"? "ê½ƒì"?
ì„ íƒ: "ê·¸ëŒ€" (ê°ì •ì  ìš¸ë¦¼ì„ ê³ ë ¤)
ë‘ ë²ˆì§¸ ë‹¨ì–´: "ê·¸ëŒ€ì˜"? "ê·¸ëŒ€ëŠ”"? "ê·¸ëŒ€ì—¬"?
ì„ íƒ: "ê·¸ëŒ€ì˜" (ìì—°ìŠ¤ëŸ¬ìš´ íë¦„)
...ê³„ì† ì´ì–´ì§
```

**GPTë„ ë˜‘ê°™ì€ ê³¼ì •ì„ ê±°ì¹©ë‹ˆë‹¤.**

```
GPTì˜ ì°½ì‘: "ROMEO:"ì—ì„œ ì‹œì‘
ì²« ë²ˆì§¸ ë‹¨ì–´ í›„ë³´: "But"(23%), "O"(18%), "What"(12%), "Alas"(8%)...
ì„ íƒ ì „ëµ: í™•ë¥ ì  ìƒ˜í”Œë§ìœ¼ë¡œ "But" ì„ íƒ
ë‘ ë²ˆì§¸ ë‹¨ì–´ í›„ë³´: "soft"(31%), "hark"(15%), "what"(12%)...
ì„ íƒ: "soft" 
...ì…°ìµìŠ¤í”¼ì–´ ìŠ¤íƒ€ì¼ ëŒ€ì‚¬ ì™„ì„±
```

ì´ê²ƒì´ ë°”ë¡œ **ìê¸°íšŒê·€ì  ìƒì„±(Autoregressive Generation)**ì˜ ë§ˆë²•ì…ë‹ˆë‹¤.

---

## 1. Autoregressive Generation: í•œ ë‹¨ì–´ì”© ì„¸ìƒì„ ë§Œë“¤ì–´ê°€ê¸°

### ìš°ë¦¬ êµ¬í˜„ì˜ í•µì‹¬: generate() í•¨ìˆ˜

[ë ˆí¬ì§€í† ë¦¬](https://github.com/BanHun28/gpt2_study)ì˜ `main.py`ì—ì„œ:

```python
@torch.no_grad()  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°ì„ ë¹„í™œì„±í™” (ì¶”ë¡ ì‹œ ë©”ëª¨ë¦¬ ì ˆì•½)
def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):
    """
    í…ìŠ¤íŠ¸ ìƒì„± ë©”ì„œë“œ
    ì£¼ì–´ì§„ ì‹œì‘ í† í°ë“¤ì—ì„œ ì‹œì‘í•˜ì—¬ ìƒˆë¡œìš´ í† í°ë“¤ì„ ìƒì„±
    """
    for _ in range(max_new_tokens):  # ì§€ì •ëœ ê°œìˆ˜ë§Œí¼ í† í° ìƒì„±
        # ì»¨í…ìŠ¤íŠ¸ê°€ ëª¨ë¸ì˜ ìµœëŒ€ ì²˜ë¦¬ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ë©´ ë’¤ìª½ë§Œ ì‚¬ìš©
        idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]
        
        # ë‹¤ìŒ í† í° ì˜ˆì¸¡
        logits, _ = self(idx_cond)  # ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜í–‰
        logits = logits[:, -1, :] / temperature  # ë§ˆì§€ë§‰ ìœ„ì¹˜ë§Œ ì‚¬ìš©í•˜ê³  ì˜¨ë„ ì ìš©
        
        # top-k í•„í„°ë§: ìƒìœ„ kê°œ í† í°ë§Œ ê³ ë ¤í•˜ì—¬ ë‹¤ì–‘ì„± ì¡°ì ˆ
        if top_k is not None:
            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
            logits[logits < v[:, [-1]]] = -float('Inf')
        
        # ë¡œì§“ì„ í™•ë¥ ë¡œ ë³€í™˜í•˜ê³  ìƒ˜í”Œë§
        probs = F.softmax(logits, dim=-1)  # í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜
        idx_next = torch.multinomial(probs, num_samples=1)  # í™•ë¥ ì— ë”°ë¼ ë‹¤ìŒ í† í° ìƒ˜í”Œë§
        
        # ìƒì„±ëœ í† í°ì„ ê¸°ì¡´ ì‹œí€€ìŠ¤ì— ì¶”ê°€
        idx = torch.cat((idx, idx_next), dim=1)
        
    return idx
```

### ë‹¨ê³„ë³„ ìƒì„± ê³¼ì • ì™„ì „ ì¶”ì 

```python
# ì˜ˆì‹œ: "ROMEO:" â†’ "ROMEO: But soft, what light"

# ì´ˆê¸° ìƒíƒœ
current_sequence = [15496, 11]  # "ROMEO:"

# Step 1: ì²« ë²ˆì§¸ ë‹¨ì–´ ìƒì„±
input_to_model = [15496, 11]
model_output = model(input_to_model)
logits = model_output[:, -1, :]  # ë§ˆì§€ë§‰ ìœ„ì¹˜ì˜ logits

# 50257ê°œ ë‹¨ì–´ì— ëŒ€í•œ ì ìˆ˜ (ì˜ˆì‹œ)
logits_sample = {
    "But": 2.34,     # ë†’ì€ ì ìˆ˜
    "O": 1.87,       # ì¤‘ê°„ ì ìˆ˜  
    "What": 1.45,    # ì¤‘ê°„ ì ìˆ˜
    "The": 0.23,     # ë‚®ì€ ì ìˆ˜
    "xyz": -3.45,    # ë§¤ìš° ë‚®ì€ ì ìˆ˜
    ...
}

# í™•ë¥ ë¡œ ë³€í™˜
probabilities = softmax(logits_sample)
# {"But": 0.234, "O": 0.187, "What": 0.145, ...}

# ìƒ˜í”Œë§ìœ¼ë¡œ "But" ì„ íƒ (284ë²ˆ í† í°)
next_token = 284

# ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸
current_sequence = [15496, 11, 284]  # "ROMEO: But"

# Step 2: ë‘ ë²ˆì§¸ ë‹¨ì–´ ìƒì„±
input_to_model = [15496, 11, 284]
# ëª¨ë¸ì´ ì´ì œ "ROMEO: But" ì „ì²´ë¥¼ ë³´ê³  ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡
...
```

### Autoregressiveì˜ í•µì‹¬ íŠ¹ì§•ë“¤

#### 1. ìˆœì°¨ì  ì˜ì¡´ì„±

```python
# ê° ë‹¨ì–´ëŠ” ì´ì „ ëª¨ë“  ë‹¨ì–´ì— ì˜ì¡´
word_1 = P(w1 | start_token)
word_2 = P(w2 | start_token, w1)  
word_3 = P(w3 | start_token, w1, w2)
word_4 = P(w4 | start_token, w1, w2, w3)
...

# ì „ì²´ ì‹œí€€ìŠ¤ì˜ í™•ë¥ 
P(ì „ì²´_ì‹œí€€ìŠ¤) = P(w1) Ã— P(w2|w1) Ã— P(w3|w1,w2) Ã— ...
```

#### 2. ë¶ˆê°€ì—­ì„±

```python
# í•œ ë²ˆ ì„ íƒí•œ ë‹¨ì–´ëŠ” ë˜ëŒë¦´ ìˆ˜ ì—†ìŒ
if current_word == "wrong_choice":
    # ì´ë¯¸ ëŠ¦ìŒ! ë‹¤ìŒ ë‹¨ì–´ë“¤ë„ ì˜í–¥ë°›ìŒ
    # ì²˜ìŒë¶€í„° ë‹¤ì‹œ ìƒì„±í•´ì•¼ í•¨

# ì´ê²ƒì´ ìƒì„±ì˜ í¥ë¯¸ë¡œìš´ ì : ë§¤ë²ˆ ë‹¤ë¥¸ ê²°ê³¼
```

#### 3. ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ

```python
# ëª¨ë¸ì´ ê¸°ì–µí•  ìˆ˜ ìˆëŠ” ê¸¸ì´ ì œí•œ
max_context = 1024  # GPT-2ì˜ ê²½ìš°

if len(current_sequence) > max_context:
    # ì˜¤ë˜ëœ í† í°ë“¤ì„ ì œê±° (sliding window)
    context = current_sequence[-max_context:]
else:
    context = current_sequence

# ì¥ë¬¸ ìƒì„± ì‹œ ì´ˆë°˜ ë‚´ìš©ì„ "ìŠì–´ë²„ë¦¼"
```

---

## 2. Temperature: ì°½ì˜ì„±ê³¼ ì¼ê´€ì„±ì˜ ê· í˜•

### Temperatureì˜ ìˆ˜í•™ì  ì •ì˜

```python
# ì›ë³¸ logits
logits = [2.0, 1.0, 0.5, 0.3, 0.1]

# Temperature ì ìš©
def apply_temperature(logits, temperature):
    return logits / temperature

# ë‹¤ì–‘í•œ Temperature íš¨ê³¼
temp_0_5 = apply_temperature(logits, 0.5)  # [4.0, 2.0, 1.0, 0.6, 0.2]
temp_1_0 = apply_temperature(logits, 1.0)  # [2.0, 1.0, 0.5, 0.3, 0.1] (ì›ë³¸)
temp_2_0 = apply_temperature(logits, 2.0)  # [1.0, 0.5, 0.25, 0.15, 0.05]
```

### Temperatureë³„ í™•ë¥  ë¶„í¬ ë³€í™”

```python
import torch.nn.functional as F

logits = torch.tensor([2.0, 1.0, 0.5, 0.3, 0.1])

# Temperature = 0.1 (ë§¤ìš° ë³´ìˆ˜ì )
probs_01 = F.softmax(logits / 0.1, dim=-1)
print("T=0.1:", probs_01)  # [0.99, 0.01, 0.00, 0.00, 0.00] - ê±°ì˜ í™•ì •ì 

# Temperature = 1.0 (ì›ë³¸)  
probs_10 = F.softmax(logits / 1.0, dim=-1)
print("T=1.0:", probs_10)  # [0.53, 0.19, 0.11, 0.09, 0.08] - ê· í˜•ì 

# Temperature = 2.0 (ì°½ì˜ì )
probs_20 = F.softmax(logits / 2.0, dim=-1)  
print("T=2.0:", probs_20)  # [0.40, 0.22, 0.16, 0.14, 0.12] - ë” ê³ ë¥¸ ë¶„í¬
```

### ì‹¤ì œ í…ìŠ¤íŠ¸ ìƒì„±ì—ì„œì˜ Temperature íš¨ê³¼

```python
# "ROMEO:" ì‹œì‘ìœ¼ë¡œ ë‹¤ì–‘í•œ Temperature ì‹¤í—˜

Temperature = 0.2 (ë³´ìˆ˜ì ):
"ROMEO: But soft, what light through yonder window breaks?
It is the east, and Juliet is the sun."
â†’ ì˜ˆì¸¡ ê°€ëŠ¥, ì •í™•í•œ ì…°ìµìŠ¤í”¼ì–´ ì¸ìš©

Temperature = 0.8 (ê· í˜•):  
"ROMEO: But soft, what gentle spirit walks these halls?
Methinks I hear the whispers of sweet love."
â†’ ì…°ìµìŠ¤í”¼ì–´ ìŠ¤íƒ€ì¼ì´ì§€ë§Œ ìƒˆë¡œìš´ ë‚´ìš©

Temperature = 1.5 (ì°½ì˜ì ):
"ROMEO: But soft, the moonbeams dance like silver tears,
And every shadow speaks of forgotten dreams."
â†’ ë” ì‹œì ì´ê³  ë…ì°½ì ì´ì§€ë§Œ ë‹¤ì†Œ ì¶”ìƒì 

Temperature = 3.0 (ê³¼ë„í•˜ê²Œ ì°½ì˜ì ):
"ROMEO: But zebra quantum pickle seventeen blue..."
â†’ ì˜ë¯¸ ì—†ëŠ” ë‚´ìš©, ì¼ê´€ì„± ìƒì‹¤
```

### Temperature ì„ íƒ ê°€ì´ë“œë¼ì¸

```python
def choose_temperature(purpose):
    """ìš©ë„ë³„ ì ì ˆí•œ Temperature ì¶”ì²œ"""
    
    guidelines = {
        "ì •í™•í•œ_ì¸ìš©": 0.1,        # ê¸°ì¡´ í…ìŠ¤íŠ¸ ì¬í˜„
        "ê¸°ìˆ _ë¬¸ì„œ": 0.3,          # ì •í™•ì„±ì´ ì¤‘ìš”
        "ì¼ë°˜_ëŒ€í™”": 0.7,          # ìì—°ìŠ¤ëŸ½ê³  ì ë‹¹íˆ ì°½ì˜ì 
        "ì°½ì‘_ê¸€ì“°ê¸°": 1.0,        # ì°½ì˜ì„±ê³¼ ì¼ê´€ì„± ê· í˜•
        "ì‹œ_ì°½ì‘": 1.3,            # ë” ì‹œì ì´ê³  ë…ì°½ì 
        "ì‹¤í—˜ì _ì°½ì‘": 2.0,        # ì˜ˆìƒì¹˜ ëª»í•œ ì¡°í•©
    }
    
    return guidelines.get(purpose, 0.8)

print("ì†Œì„¤ ì°½ì‘ìš©:", choose_temperature("ì°½ì‘_ê¸€ì“°ê¸°"))  # 1.0
print("ê¸°ìˆ  ë¬¸ì„œìš©:", choose_temperature("ê¸°ìˆ _ë¬¸ì„œ"))    # 0.3
```

---

## 3. Top-k Sampling: í˜„ì‹¤ì ì¸ ì„ íƒì§€ ì œí•œ

### Top-kì˜ ë™ì‘ ì›ë¦¬

```python
def top_k_sampling_example():
    """Top-k ìƒ˜í”Œë§ ë™ì‘ ë°©ì‹ ì‹œì—°"""
    
    # ì›ë³¸ í™•ë¥  ë¶„í¬ (50257ê°œ ë‹¨ì–´ ì¤‘ ìƒìœ„ 10ê°œë§Œ í‘œì‹œ)
    word_probs = {
        "But": 0.23,     # 1ìˆœìœ„
        "O": 0.18,       # 2ìˆœìœ„  
        "What": 0.12,    # 3ìˆœìœ„
        "Soft": 0.08,    # 4ìˆœìœ„
        "Hark": 0.06,    # 5ìˆœìœ„
        "Come": 0.05,    # 6ìˆœìœ„
        "Now": 0.04,     # 7ìˆœìœ„
        "Yet": 0.03,     # 8ìˆœìœ„
        "Fair": 0.02,    # 9ìˆœìœ„
        "Sweet": 0.02,   # 10ìˆœìœ„
        # ... ë‚˜ë¨¸ì§€ 50247ê°œ ë‹¨ì–´ë“¤ (ë§¤ìš° ë‚®ì€ í™•ë¥ )
    }
    
    # Top-k=5 ì ìš©
    top_k_5 = {k: v for k, v in list(word_probs.items())[:5]}
    
    # í™•ë¥  ì¬ì •ê·œí™” (ì„ íƒëœ ë‹¨ì–´ë“¤ì˜ í™•ë¥  í•©ì´ 1ì´ ë˜ë„ë¡)
    total_prob = sum(top_k_5.values())  # 0.67
    normalized_probs = {k: v/total_prob for k, v in top_k_5.items()}
    
    print("Top-k=5 ì¬ì •ê·œí™” í›„:")
    for word, prob in normalized_probs.items():
        print(f"{word}: {prob:.3f}")
    
    # ê²°ê³¼: ìƒìœ„ 5ê°œ ë‹¨ì–´ë§Œìœ¼ë¡œ ì„ íƒ ë²”ìœ„ ì œí•œ
    # "xyz", "nonsense" ê°™ì€ ì´ìƒí•œ ë‹¨ì–´ë“¤ ë°°ì œ

top_k_sampling_example()
```

### Top-k ê°’ì— ë”°ë¥¸ íš¨ê³¼ ë¹„êµ

```python
def compare_top_k_effects():
    """ë‹¤ì–‘í•œ Top-k ê°’ì˜ íš¨ê³¼ ë¹„êµ"""
    
    scenarios = {
        "top_k_1": "ê°€ì¥ í™•ë¥  ë†’ì€ 1ê°œë§Œ â†’ í•­ìƒ ê°™ì€ ê²°ê³¼ (Greedy)",
        "top_k_5": "ìƒìœ„ 5ê°œë§Œ â†’ ì•ˆì „í•˜ê³  í’ˆì§ˆ ë†’ìŒ",  
        "top_k_20": "ìƒìœ„ 20ê°œ â†’ ì ë‹¹í•œ ë‹¤ì–‘ì„±",
        "top_k_50": "ìƒìœ„ 50ê°œ â†’ ì°½ì˜ì ì´ì§€ë§Œ ê°€ë” ì´ìƒí•¨",
        "top_k_None": "ëª¨ë“  ë‹¨ì–´ ê³ ë ¤ â†’ ë§¤ìš° ì°½ì˜ì , ë•Œë¡œëŠ” íš¡ì„¤ìˆ˜ì„¤"
    }
    
    # ì‹¤ì œ ìƒì„± ì˜ˆì‹œ (ê°œë…ì )
    examples = {
        "top_k_1": "ROMEO: But soft, what light through yonder window breaks?",
        "top_k_5": "ROMEO: But soft, what gentle voice calls from above?", 
        "top_k_20": "ROMEO: But soft, what strange melody fills the night air?",
        "top_k_50": "ROMEO: But soft, what peculiar shadows dance in moonlight?",
        "top_k_None": "ROMEO: But soft, what purple elephants sing opera tonight?"
    }
    
    print("=== Top-k ê°’ë³„ ìƒì„± ê²°ê³¼ ë¹„êµ ===")
    for k, description in scenarios.items():
        print(f"\n{k}: {description}")
        print(f"ì˜ˆì‹œ: {examples[k]}")

compare_top_k_effects()
```

### Top-kì˜ ì¥ë‹¨ì 

```python
# ì¥ì :
âœ… ì´ìƒí•œ ë‹¨ì–´ ë°°ì œ: "xyz", "qwerty" ê°™ì€ ë¬´ì˜ë¯¸í•œ í† í° ì œê±°
âœ… í’ˆì§ˆ ë³´ì¥: ìµœì†Œí•œì˜ ë¬¸ë²•ì /ì˜ë¯¸ì  íƒ€ë‹¹ì„± í™•ë³´
âœ… ê³„ì‚° íš¨ìœ¨ì„±: ëª¨ë“  ì–´íœ˜ ëŒ€ì‹  ì¼ë¶€ë§Œ ê³ ë ¤
âœ… ì¡°ì ˆ ê°€ëŠ¥: k ê°’ìœ¼ë¡œ ë‹¤ì–‘ì„± ì •ë„ ì œì–´

# ë‹¨ì :  
âŒ ê³ ì •ëœ ì œí•œ: ìƒí™©ì— ê´€ê³„ì—†ì´ í•­ìƒ kê°œë§Œ ê³ ë ¤
âŒ í™•ë¥  ë¶„í¬ ë¬´ì‹œ: 2ìˆœìœ„ì™€ kìˆœìœ„ì˜ í™•ë¥  ì°¨ì´ë¥¼ ê³ ë ¤í•˜ì§€ ì•ŠìŒ
âŒ ì°½ì˜ì„± ì œí•œ: ë•Œë¡œëŠ” ë‚®ì€ í™•ë¥ ì´ì§€ë§Œ í›Œë¥­í•œ ì„ íƒì„ ë°°ì œ
```

---

## 4. Top-p (Nucleus) Sampling: ë™ì ì¸ ì„ íƒì§€ ì¡°ì ˆ

### Top-pì˜ í˜ì‹ ì  ì•„ì´ë””ì–´

```python
def nucleus_sampling_example():
    """Nucleus (Top-p) ìƒ˜í”Œë§ ì‹œì—°"""
    
    # í™•ë¥  ë¶„í¬ (ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬)
    sorted_probs = [
        ("But", 0.30),
        ("O", 0.25), 
        ("What", 0.15),
        ("Soft", 0.10),
        ("Hark", 0.08),
        ("Come", 0.05),
        ("Now", 0.03),
        ("Yet", 0.02),
        ("Fair", 0.01),
        ("Sweet", 0.01)
    ]
    
    # Top-p = 0.8 ì ìš©
    cumulative_prob = 0.0
    selected_words = []
    
    for word, prob in sorted_probs:
        cumulative_prob += prob
        selected_words.append((word, prob))
        
        print(f"{word}: {prob:.2f} (ëˆ„ì : {cumulative_prob:.2f})")
        
        if cumulative_prob >= 0.8:  # 80%ì— ë„ë‹¬í•˜ë©´ ì¤‘ë‹¨
            print(f"â†’ Top-p=0.8 ë„ë‹¬, {len(selected_words)}ê°œ ë‹¨ì–´ ì„ íƒ")
            break
    
    # ê²°ê³¼: ìƒí™©ì— ë”°ë¼ ì„ íƒë˜ëŠ” ë‹¨ì–´ ìˆ˜ê°€ ë‹¤ë¦„
    # í™•ë¥  ë¶„í¬ê°€ ì§‘ì¤‘ë˜ì–´ ìˆìœ¼ë©´ ì ì€ ìˆ˜, ê³ ë¥´ë©´ ë§ì€ ìˆ˜

nucleus_sampling_example()
```

### Top-k vs Top-p ë¹„êµ

```python
def compare_top_k_vs_top_p():
    """Top-kì™€ Top-p ìƒ˜í”Œë§ ë¹„êµ"""
    
    # ì‹œë‚˜ë¦¬ì˜¤ 1: í™•ë¥ ì´ ì§‘ì¤‘ëœ ê²½ìš°
    concentrated_dist = [0.5, 0.3, 0.1, 0.05, 0.03, 0.02]
    
    print("=== í™•ë¥  ì§‘ì¤‘ëœ ìƒí™© ===")
    print("ë¶„í¬:", concentrated_dist)
    
    # Top-k=3: í•­ìƒ 3ê°œ
    print("Top-k=3: ìƒìœ„ 3ê°œ ì„ íƒ (0.5, 0.3, 0.1)")
    
    # Top-p=0.9: í™•ë¥  90%ê¹Œì§€
    cumul = 0
    for i, p in enumerate(concentrated_dist):
        cumul += p
        if cumul >= 0.9:
            print(f"Top-p=0.9: ìƒìœ„ {i+1}ê°œ ì„ íƒ (ëˆ„ì  {cumul:.1f})")
            break
    
    print("\n=== í™•ë¥  ë¶„ì‚°ëœ ìƒí™© ===")
    # ì‹œë‚˜ë¦¬ì˜¤ 2: í™•ë¥ ì´ ë¶„ì‚°ëœ ê²½ìš°  
    distributed_dist = [0.2, 0.18, 0.15, 0.12, 0.1, 0.08, 0.07, 0.05, 0.03, 0.02]
    print("ë¶„í¬:", distributed_dist)
    
    print("Top-k=3: ì—¬ì „íˆ 3ê°œë§Œ ì„ íƒ")
    
    cumul = 0
    for i, p in enumerate(distributed_dist):
        cumul += p
        if cumul >= 0.9:
            print(f"Top-p=0.9: ìƒìœ„ {i+1}ê°œ ì„ íƒ (ë” ë§ì€ ì„ íƒì§€)")
            break

compare_top_k_vs_top_p()
```

### Top-p êµ¬í˜„ ì„¸ë¶€ì‚¬í•­

```python
def top_p_sampling(logits, p=0.9):
    """Top-p (Nucleus) ìƒ˜í”Œë§ êµ¬í˜„"""
    
    # 1. í™•ë¥ ë¡œ ë³€í™˜
    probs = F.softmax(logits, dim=-1)
    
    # 2. ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    sorted_probs, sorted_indices = torch.sort(probs, descending=True)
    
    # 3. ëˆ„ì  í™•ë¥  ê³„ì‚°
    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
    
    # 4. p ì„ê³„ê°’ì„ ë„˜ëŠ” ë¶€ë¶„ ë§ˆìŠ¤í‚¹
    # cumulative_probs > pì¸ ìœ„ì¹˜ë¥¼ ì°¾ë˜, ì²« ë²ˆì§¸ëŠ” í•­ìƒ í¬í•¨
    sorted_indices_to_remove = cumulative_probs > p
    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
    sorted_indices_to_remove[..., 0] = 0  # ì²« ë²ˆì§¸ëŠ” í•­ìƒ ìœ ì§€
    
    # 5. ì›ë˜ ìˆœì„œë¡œ ë˜ëŒë¦¬ê¸°
    indices_to_remove = sorted_indices[sorted_indices_to_remove]
    probs[indices_to_remove] = 0
    
    # 6. ì¬ì •ê·œí™”
    probs = probs / probs.sum()
    
    # 7. ìƒ˜í”Œë§
    next_token = torch.multinomial(probs, num_samples=1)
    
    return next_token
```

---

## 5. Beam Search: ë” ë‚˜ì€ í’ˆì§ˆì„ ì°¾ì•„ì„œ

### Greedy vs Beam Search

#### Greedy Decodingì˜ í•œê³„

```python
# Greedy: ê° ë‹¨ê³„ì—ì„œ ê°€ì¥ í™•ë¥  ë†’ì€ ë‹¨ì–´ë§Œ ì„ íƒ

Step 1: "ROMEO:" â†’ "But" (0.3)
Step 2: "ROMEO: But" â†’ "soft" (0.4)  
Step 3: "ROMEO: But soft" â†’ "what" (0.35)

# ì „ì²´ í™•ë¥ : 0.3 Ã— 0.4 Ã— 0.35 = 0.042

# ë¬¸ì œ: êµ­ì†Œ ìµœì í•´ì— ë¹ ì§ˆ ìˆ˜ ìˆìŒ
# ë” ë‚˜ì€ ì „ì²´ ê²½ë¡œê°€ ìˆì„ ìˆ˜ë„ ìˆìŒ:
Alternative path:
Step 1: "ROMEO:" â†’ "O" (0.2)  # ë‚®ì€ ì‹œì‘
Step 2: "ROMEO: O" â†’ "Juliet" (0.8)  # ë†’ì€ í™•ë¥ 
Step 3: "ROMEO: O Juliet" â†’ "my" (0.7)  # ë†’ì€ í™•ë¥ 

# ì „ì²´ í™•ë¥ : 0.2 Ã— 0.8 Ã— 0.7 = 0.112 (ë” ë†’ìŒ!)
```

#### Beam Searchì˜ í•´ê²°ì±…

```python
def beam_search_example(beam_size=3):
    """Beam Search ë™ì‘ ê³¼ì • ì‹œì—°"""
    
    # ì´ˆê¸° ìƒíƒœ: "ROMEO:"
    beams = [
        {"sequence": ["ROMEO:"], "score": 0.0}  # log í™•ë¥  ì‚¬ìš©
    ]
    
    print("=== Beam Search ê³¼ì • ===")
    print(f"Beam Size: {beam_size}")
    
    # Step 1: ì²« ë²ˆì§¸ ë‹¨ì–´ ìƒì„±
    candidates = []
    for beam in beams:
        # ê° beamì—ì„œ ê°€ëŠ¥í•œ ë‹¤ìŒ ë‹¨ì–´ë“¤
        next_words = [
            ("But", -1.20),   # log(0.3)
            ("O", -1.61),     # log(0.2)  
            ("What", -1.90),  # log(0.15)
            ("Soft", -2.30),  # log(0.1)
        ]
        
        for word, log_prob in next_words:
            new_sequence = beam["sequence"] + [word]
            new_score = beam["score"] + log_prob
            candidates.append({
                "sequence": new_sequence,
                "score": new_score
            })
    
    # ìƒìœ„ beam_sizeê°œë§Œ ìœ ì§€
    beams = sorted(candidates, key=lambda x: x["score"], reverse=True)[:beam_size]
    
    print("Step 1 ê²°ê³¼:")
    for i, beam in enumerate(beams):
        print(f"Beam {i+1}: {' '.join(beam['sequence'])} (ì ìˆ˜: {beam['score']:.2f})")
    
    # Step 2: ë‘ ë²ˆì§¸ ë‹¨ì–´ ìƒì„± (ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì§„í–‰)
    print("\nStep 2 í›„ ìƒìœ„ beamë“¤:")
    # ... ì‹¤ì œë¡œëŠ” ê°™ì€ ê³¼ì • ë°˜ë³µ

beam_search_example()
```

### Beam Searchì˜ ì¥ë‹¨ì 

```python
# ì¥ì :
âœ… ë” ë‚˜ì€ ì „ì²´ í’ˆì§ˆ: êµ­ì†Œ ìµœì í•´ íšŒí”¼
âœ… ì¼ê´€ì„± ìˆëŠ” í…ìŠ¤íŠ¸: ë” ë…¼ë¦¬ì ì¸ êµ¬ì¡°
âœ… ì˜ˆì¸¡ ê°€ëŠ¥í•œ ê²°ê³¼: ê°™ì€ ì…ë ¥ì— ëŒ€í•´ í•­ìƒ ê°™ì€ ì¶œë ¥

# ë‹¨ì :
âŒ ê³„ì‚° ë¹„ìš©: beam_sizeë°° ë§Œí¼ ì—°ì‚°ëŸ‰ ì¦ê°€  
âŒ ë‹¤ì–‘ì„± ë¶€ì¡±: ì•ˆì „í•œ ì„ íƒ ìœ„ì£¼
âŒ ë°˜ë³µ ë¬¸ì œ: ê°™ì€ íŒ¨í„´ ë°˜ë³µ ê²½í–¥
âŒ ê¸¸ì´ í¸í–¥: ì§§ì€ ë¬¸ì¥ ì„ í˜¸ (í™•ë¥  ê³±ì´ë¯€ë¡œ)
```

### Beam Search ê°œì„  ê¸°ë²•ë“¤

```python
def improved_beam_search():
    """ê°œì„ ëœ Beam Search ê¸°ë²•ë“¤"""
    
    improvements = {
        "Length Normalization": {
            "ë¬¸ì œ": "ê¸´ ë¬¸ì¥ì¼ìˆ˜ë¡ í™•ë¥ ì´ ë‚®ì•„ì§ (ê³±ì…ˆ ë•Œë¬¸ì—)",
            "í•´ê²°": "ì ìˆ˜ë¥¼ ê¸¸ì´ë¡œ ë‚˜ëˆ„ê±°ë‚˜ ê¸¸ì´ íŒ¨ë„í‹° ì ìš©",
            "ê³µì‹": "score / (length ** alpha)"
        },
        
        "Coverage Penalty": {
            "ë¬¸ì œ": "ê°™ì€ ë‚´ìš© ë°˜ë³µ",
            "í•´ê²°": "ì´ë¯¸ ì–¸ê¸‰ëœ ë‚´ìš©ì— íŒ¨ë„í‹°",
            "íš¨ê³¼": "ë” ë‹¤ì–‘í•œ ë‚´ìš© ìƒì„±"
        },
        
        "Diverse Beam Search": {
            "ë¬¸ì œ": "ë¹„ìŠ·í•œ beamë“¤ë§Œ ìƒì„±",  
            "í•´ê²°": "beamë“¤ ê°„ì˜ ë‹¤ì–‘ì„± ê°•ì œ",
            "ë°©ë²•": "ê·¸ë£¹ë³„ë¡œ ë‹¤ë¥¸ ë°©í–¥ íƒìƒ‰"
        }
    }
    
    for technique, details in improvements.items():
        print(f"\n=== {technique} ===")
        for aspect, description in details.items():
            print(f"{aspect}: {description}")

improved_beam_search()
```

---

## 6. ì‹¤ì œ ìƒì„± ê³¼ì • ì™„ì „ ë¶„ì„

### ìš°ë¦¬ êµ¬í˜„ì˜ generate() í•¨ìˆ˜ ì‹¬ì¸µ ë¶„ì„

```python
def analyze_generation_step_by_step():
    """ì‹¤ì œ ìƒì„± ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ ë¶„ì„"""
    
    # 1. ì…ë ¥ ì¤€ë¹„
    prompt = "ROMEO:"
    tokens = enc.encode(prompt)  # [15496, 11]
    input_ids = torch.tensor([tokens])
    
    print("=== ìƒì„± ê³¼ì • ë‹¨ê³„ë³„ ë¶„ì„ ===")
    print(f"ì´ˆê¸° ì…ë ¥: {prompt} â†’ {tokens}")
    
    # 2. ëª¨ë¸ ì„¤ì •
    model.eval()
    max_new_tokens = 10
    temperature = 0.8
    top_k = 40
    
    current_sequence = input_ids.clone()
    
    for step in range(max_new_tokens):
        print(f"\n--- Step {step + 1} ---")
        
        # 2.1. ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ í™•ì¸
        seq_len = current_sequence.size(1)
        if seq_len > model.config.block_size:
            context = current_sequence[:, -model.config.block_size:]
            print(f"ì»¨í…ìŠ¤íŠ¸ ì˜ë¦¼: {seq_len} â†’ {model.config.block_size}")
        else:
            context = current_sequence
            print(f"ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {seq_len}")
        
        # 2.2. ëª¨ë¸ ìˆœì „íŒŒ
        with torch.no_grad():
            logits, _ = model(context)
            next_token_logits = logits[0, -1, :]  # ë§ˆì§€ë§‰ ìœ„ì¹˜
        
        # 2.3. Temperature ì ìš©
        scaled_logits = next_token_logits / temperature
        print(f"Temperature {temperature} ì ìš©")
        
        # 2.4. Top-k í•„í„°ë§
        if top_k is not None:
            values, indices = torch.topk(scaled_logits, top_k)
            filtered_logits = torch.full_like(scaled_logits, float('-inf'))
            filtered_logits[indices] = values
            print(f"Top-{top_k} í•„í„°ë§ ì ìš©")
        else:
            filtered_logits = scaled_logits
        
        # 2.5. í™•ë¥  ë³€í™˜ ë° ìƒ˜í”Œë§
        probs = F.softmax(filtered_logits, dim=-1)
        next_token = torch.multinomial(probs, num_samples=1)
        
        # 2.6. ê²°ê³¼ ì¶œë ¥
        next_word = enc.decode([next_token.item()])
        probability = probs[next_token].item()
        
        print(f"ì„ íƒëœ í† í°: {next_token.item()} â†’ '{next_word}'")
        print(f"ì„ íƒ í™•ë¥ : {probability:.3f}")
        
        # 2.7. ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸
        current_sequence = torch.cat([current_sequence, next_token.unsqueeze(0)], dim=1)
        
        # í˜„ì¬ê¹Œì§€ ìƒì„±ëœ í…ìŠ¤íŠ¸
        current_text = enc.decode(current_sequence[0].tolist())
        print(f"í˜„ì¬ ì‹œí€€ìŠ¤: {current_text}")
    
    return current_text

# ì‹¤í–‰
final_text = analyze_generation_step_by_step()
print(f"\nìµœì¢… ê²°ê³¼: {final_text}")
```

### ìƒì„± í’ˆì§ˆì— ì˜í–¥ì„ ì£¼ëŠ” ìš”ì†Œë“¤

```python
def generation_quality_factors():
    """ìƒì„± í’ˆì§ˆì— ì˜í–¥ì„ ì£¼ëŠ” ì£¼ìš” ìš”ì†Œë“¤"""
    
    factors = {
        "ëª¨ë¸ í¬ê¸°": {
            "ì˜í–¥": "í° ëª¨ë¸ì¼ìˆ˜ë¡ ë” í’ë¶€í•œ í‘œí˜„ê³¼ ì¼ê´€ì„±",
            "ì˜ˆì‹œ": "GPT-2 Small vs XLì˜ í’ˆì§ˆ ì°¨ì´",
            "ê¶Œì¥": "ê°€ëŠ¥í•œ ë²”ìœ„ì—ì„œ í° ëª¨ë¸ ì‚¬ìš©"
        },
        
        "í”„ë¡¬í”„íŠ¸ í’ˆì§ˆ": {
            "ì˜í–¥": "ì¢‹ì€ ì‹œì‘ì´ ì¢‹ì€ ê²°ê³¼ë¥¼ ë§Œë“¦",
            "ì˜ˆì‹œ": "'Write a story' vs 'In the misty mountains where dragons dwell'",
            "ê¶Œì¥": "êµ¬ì²´ì ì´ê³  í¥ë¯¸ë¡œìš´ í”„ë¡¬í”„íŠ¸ ì‘ì„±"
        },
        
        "í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©": {
            "ì˜í–¥": "Temperature, Top-k, Top-pì˜ ê· í˜•",
            "ì˜ˆì‹œ": "ì°½ì˜ì  ê¸€ì“°ê¸° vs ê¸°ìˆ  ë¬¸ì„œì˜ ë‹¤ë¥¸ ì„¤ì •",
            "ê¶Œì¥": "ìš©ë„ì— ë§ëŠ” íŒŒë¼ë¯¸í„° íŠœë‹"
        },
        
        "í•™ìŠµ ë°ì´í„° í’ˆì§ˆ": {
            "ì˜í–¥": "ëª¨ë¸ì´ í•™ìŠµí•œ ë°ì´í„°ì˜ í’ˆì§ˆì´ ìƒì„±ì— ì§ì ‘ ì˜í–¥",
            "ì˜ˆì‹œ": "ì…°ìµìŠ¤í”¼ì–´ ë°ì´í„°ë¡œ í•™ìŠµ â†’ ì…°ìµìŠ¤í”¼ì–´ ìŠ¤íƒ€ì¼ ìƒì„±",
            "ê¶Œì¥": "ê³ í’ˆì§ˆ, ë‹¤ì–‘í•œ í•™ìŠµ ë°ì´í„° ì‚¬ìš©"
        }
    }
    
    print("=== ìƒì„± í’ˆì§ˆ ì˜í–¥ ìš”ì†Œ ===")
    for factor, details in factors.items():
        print(f"\n{factor}:")
        for aspect, description in details.items():
            print(f"  {aspect}: {description}")

generation_quality_factors()
```

---

## 7. ìƒì„± í’ˆì§ˆ í‰ê°€ ë°©ë²•ë“¤

### ìë™ í‰ê°€ ì§€í‘œë“¤

```python
def automatic_evaluation_metrics():
    """ìë™ ìƒì„± í…ìŠ¤íŠ¸ í‰ê°€ ì§€í‘œë“¤"""
    
    metrics = {
        "Perplexity": {
            "ì •ì˜": "ëª¨ë¸ì´ í…ìŠ¤íŠ¸ë¥¼ ì–¼ë§ˆë‚˜ 'ì˜ˆìƒ'í–ˆëŠ”ì§€ ì¸¡ì •",
            "ê³„ì‚°": "exp(í‰ê·  êµì°¨ì—”íŠ¸ë¡œí”¼ ì†ì‹¤)",
            "í•´ì„": "ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ (ë” ì˜ˆì¸¡ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸)",
            "í•œê³„": "ì˜ˆì¸¡ ê°€ëŠ¥ != ì¢‹ì€ í’ˆì§ˆ"
        },
        
        "BLEU Score": {
            "ì •ì˜": "ìƒì„± í…ìŠ¤íŠ¸ì™€ ì°¸ì¡° í…ìŠ¤íŠ¸ì˜ n-gram ì¼ì¹˜ë„",
            "ê³„ì‚°": "1-gram~4-gram ì •ë°€ë„ì˜ ê¸°í•˜í‰ê· ",
            "í•´ì„": "0~1, ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ",
            "í•œê³„": "ì°½ì˜ì  í…ìŠ¤íŠ¸ì—ëŠ” ë¶€ì í•©"
        },
        
        "Diversity Metrics": {
            "ì •ì˜": "ìƒì„±ëœ í…ìŠ¤íŠ¸ì˜ ë‹¤ì–‘ì„± ì¸¡ì •",
            "ì¢…ë¥˜": "Distinct-1, Distinct-2 (ê³ ìœ  n-gram ë¹„ìœ¨)",
            "í•´ì„": "ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘í•˜ê³  ë°˜ë³µì ì´ì§€ ì•ŠìŒ",
            "ì¤‘ìš”ì„±": "ë°˜ë³µ ë¬¸ì œ ê°ì§€"
        },
        
        "Semantic Coherence": {
            "ì •ì˜": "ë¬¸ì¥ ê°„ ì˜ë¯¸ì  ì¼ê´€ì„±",
            "ì¸¡ì •": "ë¬¸ì¥ ì„ë² ë”© ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„",
            "í•´ì„": "ì ì ˆí•œ ë²”ìœ„ì˜ ì¼ê´€ì„± í•„ìš”",
            "ë„êµ¬": "BERT, Sentence-BERT ë“± í™œìš©"
        }
    }
    
    print("=== ìë™ í‰ê°€ ì§€í‘œ ===")
    for metric, details in metrics.items():
        print(f"\n{metric}:")
        for aspect, description in details.items():
            print(f"  {aspect}: {description}")

automatic_evaluation_metrics()
```

### ì¸ê°„ í‰ê°€ ê¸°ì¤€ë“¤

```python
def human_evaluation_criteria():
    """ì¸ê°„ì´ í‰ê°€í•˜ëŠ” í…ìŠ¤íŠ¸ í’ˆì§ˆ ê¸°ì¤€"""
    
    criteria = {
        "Fluency (ìœ ì°½ì„±)": {
            "ì§ˆë¬¸": "ë¬¸ë²•ì ìœ¼ë¡œ ì˜¬ë°”ë¥´ê³  ìì—°ìŠ¤ëŸ¬ìš´ê°€?",
            "í‰ê°€": "1~5ì  ì²™ë„",
            "ì˜ˆì‹œ": {
                5: "ì™„ë²½í•œ ë¬¸ë²•ê³¼ ìì—°ìŠ¤ëŸ¬ìš´ íë¦„",
                3: "ì•½ê°„ì˜ ì–´ìƒ‰í•¨ì´ ìˆì§€ë§Œ ì´í•´ ê°€ëŠ¥",
                1: "ë¬¸ë²• ì˜¤ë¥˜ê°€ ë§ê³  ì´í•´í•˜ê¸° ì–´ë ¤ì›€"
            }
        },
        
        "Coherence (ì¼ê´€ì„±)": {
            "ì§ˆë¬¸": "ë…¼ë¦¬ì ìœ¼ë¡œ ì¼ê´€ë˜ê³  ì£¼ì œê°€ ëª…í™•í•œê°€?",
            "í‰ê°€": "1~5ì  ì²™ë„", 
            "ì˜ˆì‹œ": {
                5: "ëª…í™•í•œ ì£¼ì œì™€ ë…¼ë¦¬ì  ì „ê°œ",
                3: "ëŒ€ì²´ë¡œ ì¼ê´€ë˜ì§€ë§Œ ì¼ë¶€ í˜¼ë€",
                1: "ì£¼ì œê°€ ë¶ˆë¶„ëª…í•˜ê³  ë…¼ë¦¬ì  ì—°ê²° ë¶€ì¡±"
            }
        },
        
        "Creativity (ì°½ì˜ì„±)": {
            "ì§ˆë¬¸": "ë…ì°½ì ì´ê³  í¥ë¯¸ë¡œìš´ ë‚´ìš©ì¸ê°€?",
            "í‰ê°€": "1~5ì  ì²™ë„",
            "ì£¼ì˜": "ì°½ì˜ì„±ê³¼ ì¼ê´€ì„±ì˜ ê· í˜• í•„ìš”"
        },
        
        "Relevance (ê´€ë ¨ì„±)": {
            "ì§ˆë¬¸": "ì£¼ì–´ì§„ í”„ë¡¬í”„íŠ¸ì™€ ì–¼ë§ˆë‚˜ ê´€ë ¨ ìˆëŠ”ê°€?",
            "í‰ê°€": "1~5ì  ì²™ë„",
            "ì¤‘ìš”ì„±": "í”„ë¡¬í”„íŠ¸ ì˜ë„ íŒŒì•… ëŠ¥ë ¥ ì¸¡ì •"
        }
    }
    
    print("=== ì¸ê°„ í‰ê°€ ê¸°ì¤€ ===")
    for criterion, details in criteria.items():
        print(f"\n{criterion}:")
        for aspect, description in details.items():
            if isinstance(description, dict):
                print(f"  {aspect}:")
                for score, example in description.items():
                    print(f"    {score}: {example}")
            else:
                print(f"  {aspect}: {description}")

human_evaluation_criteria()
```

---

## 8. ì‹¤ìŠµ: ê³ ê¸‰ ìƒì„± ê¸°ë²• êµ¬í˜„

### ì‹¤ìŠµ 1: ë‹¤ì¤‘ ìƒ˜í”Œë§ ë°©ë²• ë¹„êµ

```python
def compare_sampling_methods():
    """ë‹¤ì–‘í•œ ìƒ˜í”Œë§ ë°©ë²• ë¹„êµ ì‹¤í—˜"""
    
    prompt = "ROMEO:"
    max_tokens = 30
    
    methods = {
        "greedy": {"temperature": 0.0, "top_k": 1},
        "low_temp": {"temperature": 0.3, "top_k": None},
        "balanced": {"temperature": 0.8, "top_k": 40},
        "creative": {"temperature": 1.2, "top_k": 100},
        "wild": {"temperature": 2.0, "top_k": None}
    }
    
    print("=== ìƒ˜í”Œë§ ë°©ë²•ë³„ ìƒì„± ê²°ê³¼ ë¹„êµ ===")
    
    for method_name, params in methods.items():
        print(f"\n=== {method_name.upper()} ===")
        print(f"ì„¤ì •: Temperature={params['temperature']}, Top-k={params['top_k']}")
        
        # ê°™ì€ í”„ë¡¬í”„íŠ¸ë¡œ 3ë²ˆ ìƒì„±
        for trial in range(3):
            tokens = enc.encode(prompt)
            input_ids = torch.tensor([tokens])
            
            with torch.no_grad():
                generated = model.generate(
                    input_ids,
                    max_new_tokens=max_tokens,
                    temperature=params['temperature'],
                    top_k=params['top_k']
                )
            
            result = enc.decode(generated[0].tolist())
            print(f"ì‹œë„ {trial + 1}: {result}")
        
        # íŠ¹ì„± ë¶„ì„
        if params['temperature'] < 0.5:
            print("â†’ íŠ¹ì„±: ì˜ˆì¸¡ ê°€ëŠ¥, ì¼ê´€ì„± ë†’ìŒ, ì°½ì˜ì„± ë‚®ìŒ")
        elif params['temperature'] < 1.0:
            print("â†’ íŠ¹ì„±: ê· í˜•ì¡íŒ í’ˆì§ˆê³¼ ë‹¤ì–‘ì„±")
        else:
            print("â†’ íŠ¹ì„±: ë†’ì€ ì°½ì˜ì„±, ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥ì„±")

compare_sampling_methods()
```

### ì‹¤ìŠµ 2: ì»¤ìŠ¤í…€ ìƒ˜í”Œë§ ì „ëµ

```python
def custom_sampling_strategy():
    """ì»¤ìŠ¤í…€ ìƒ˜í”Œë§ ì „ëµ êµ¬í˜„"""
    
    def adaptive_temperature_sampling(logits, position, sequence_length):
        """ìœ„ì¹˜ì— ë”°ë¼ ì ì‘ì ìœ¼ë¡œ temperature ì¡°ì ˆ"""
        
        # ì‹œì‘ ë¶€ë¶„: ë³´ìˆ˜ì  (ì¼ê´€ì„± ì¤‘ìš”)
        if position < sequence_length * 0.3:
            temp = 0.6
        # ì¤‘ê°„ ë¶€ë¶„: ê· í˜•ì  (ì°½ì˜ì„±ê³¼ ì¼ê´€ì„±)
        elif position < sequence_length * 0.7:
            temp = 0.9
        # ë§ˆì§€ë§‰ ë¶€ë¶„: ì°½ì˜ì  (ë§ˆë¬´ë¦¬ì˜ ì„íŒ©íŠ¸)
        else:
            temp = 1.2
            
        return logits / temp
    
    def repetition_penalty_sampling(logits, generated_tokens, penalty=1.2):
        """ë°˜ë³µ ë°©ì§€ë¥¼ ìœ„í•œ í˜ë„í‹° ì ìš©"""
        
        # ì´ë¯¸ ìƒì„±ëœ í† í°ë“¤ì— í˜ë„í‹°
        for token in set(generated_tokens):
            logits[token] /= penalty
            
        return logits
    
    def combined_sampling(prompt, max_tokens=50):
        """ì—¬ëŸ¬ ì „ëµì„ ì¡°í•©í•œ ìƒ˜í”Œë§"""
        
        tokens = enc.encode(prompt)
        generated_tokens = tokens.copy()
        
        for position in range(max_tokens):
            # í˜„ì¬ ì»¨í…ìŠ¤íŠ¸ë¡œ ì˜ˆì¸¡
            input_ids = torch.tensor([generated_tokens[-model.config.block_size:]])
            
            with torch.no_grad():
                logits, _ = model(input_ids)
                next_logits = logits[0, -1, :].clone()
            
            # 1. ì ì‘ì  temperature ì ìš©
            next_logits = adaptive_temperature_sampling(
                next_logits, position, max_tokens
            )
            
            # 2. ë°˜ë³µ í˜ë„í‹° ì ìš©
            next_logits = repetition_penalty_sampling(
                next_logits, generated_tokens
            )
            
            # 3. Top-k í•„í„°ë§
            top_k = 50
            if top_k > 0:
                values, indices = torch.topk(next_logits, top_k)
                filtered_logits = torch.full_like(next_logits, float('-inf'))
                filtered_logits[indices] = values
                next_logits = filtered_logits
            
            # 4. ìƒ˜í”Œë§
            probs = F.softmax(next_logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1).item()
            
            generated_tokens.append(next_token)
            
            # ì¢…ë£Œ ì¡°ê±´ (ë¬¸ì¥ ë)
            if next_token == enc.encode('.')[0]:
                break
        
        return enc.decode(generated_tokens)
    
    # í…ŒìŠ¤íŠ¸
    print("=== ì»¤ìŠ¤í…€ ìƒ˜í”Œë§ ê²°ê³¼ ===")
    for i in range(3):
        result = combined_sampling("ROMEO:")
        print(f"\nìƒì„± {i+1}: {result}")

custom_sampling_strategy()
```

### ì‹¤ìŠµ 3: ìƒì„± í’ˆì§ˆ ìë™ í‰ê°€

```python
def automatic_quality_assessment():
    """ìƒì„±ëœ í…ìŠ¤íŠ¸ì˜ í’ˆì§ˆ ìë™ í‰ê°€"""
    
    def calculate_perplexity(text, model):
        """Perplexity ê³„ì‚°"""
        tokens = enc.encode(text)
        if len(tokens) < 2:
            return float('inf')
        
        input_ids = torch.tensor([tokens[:-1]])
        target_ids = torch.tensor([tokens[1:]])
        
        with torch.no_grad():
            logits, loss = model(input_ids, target_ids)
        
        return torch.exp(loss).item()
    
    def calculate_diversity(text):
        """í…ìŠ¤íŠ¸ ë‹¤ì–‘ì„± ê³„ì‚°"""
        words = text.lower().split()
        
        if len(words) < 2:
            return 0.0
        
        # Distinct-2: ê³ ìœ í•œ 2-gram ë¹„ìœ¨
        bigrams = [f"{words[i]}_{words[i+1]}" for i in range(len(words)-1)]
        distinct_2 = len(set(bigrams)) / len(bigrams) if bigrams else 0
        
        return distinct_2
    
    def calculate_repetition_rate(text):
        """ë°˜ë³µë¥  ê³„ì‚°"""
        words = text.lower().split()
        
        if len(words) < 4:
            return 0.0
        
        # 4-gram ë°˜ë³µ ê²€ì‚¬
        fourgrams = [' '.join(words[i:i+4]) for i in range(len(words)-3)]
        repeated = len(fourgrams) - len(set(fourgrams))
        
        return repeated / len(fourgrams) if fourgrams else 0
    
    # ë‹¤ì–‘í•œ ì„¤ì •ìœ¼ë¡œ í…ìŠ¤íŠ¸ ìƒì„± ë° í‰ê°€
    test_configs = [
        {"name": "Conservative", "temp": 0.3, "top_k": 20},
        {"name": "Balanced", "temp": 0.8, "top_k": 40},
        {"name": "Creative", "temp": 1.2, "top_k": 100}
    ]
    
    print("=== ìƒì„± í’ˆì§ˆ ìë™ í‰ê°€ ===")
    
    for config in test_configs:
        print(f"\n--- {config['name']} ì„¤ì • ---")
        
        # í…ìŠ¤íŠ¸ ìƒì„±
        tokens = enc.encode("ROMEO:")
        input_ids = torch.tensor([tokens])
        
        with torch.no_grad():
            generated = model.generate(
                input_ids,
                max_new_tokens=50,
                temperature=config['temp'],
                top_k=config['top_k']
            )
        
        text = enc.decode(generated[0].tolist())
        print(f"ìƒì„± í…ìŠ¤íŠ¸: {text}")
        
        # í’ˆì§ˆ ì§€í‘œ ê³„ì‚°
        perplexity = calculate_perplexity(text, model)
        diversity = calculate_diversity(text)
        repetition = calculate_repetition_rate(text)
        
        print(f"Perplexity: {perplexity:.2f} (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)")
        print(f"Diversity: {diversity:.3f} (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)")
        print(f"Repetition Rate: {repetition:.3f} (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)")
        
        # ì¢…í•© ì ìˆ˜ (ê°„ë‹¨í•œ ê°€ì¤‘í•©)
        score = (1/perplexity) * diversity * (1-repetition) * 100
        print(f"ì¢…í•© ì ìˆ˜: {score:.2f}")

automatic_quality_assessment()
```

---

## ë§ˆë¬´ë¦¬: ì°½ì‘í•˜ëŠ” AIì˜ ë¹„ë°€ì„ ë§ˆìŠ¤í„°í–ˆìŠµë‹ˆë‹¤

### ì˜¤ëŠ˜ ë°°ìš´ í•µì‹¬ ë‚´ìš©

1. **Autoregressive Generation**: í•œ ë‹¨ì–´ì”© ìˆœì°¨ì ìœ¼ë¡œ ìƒì„±í•˜ëŠ” ì›ë¦¬
2. **Temperature Sampling**: ì°½ì˜ì„±ê³¼ ì¼ê´€ì„±ì˜ ê· í˜• ì¡°ì ˆ ê¸°ë²•
3. **Top-k Sampling**: í˜„ì‹¤ì ì¸ ì„ íƒì§€ë¡œ ì œí•œí•˜ì—¬ í’ˆì§ˆ ë³´ì¥
4. **Top-p (Nucleus) Sampling**: ë™ì ìœ¼ë¡œ ì„ íƒì§€ë¥¼ ì¡°ì ˆí•˜ëŠ” í˜ì‹ 
5. **Beam Search**: ë” ë‚˜ì€ ì „ì²´ í’ˆì§ˆì„ ìœ„í•œ íƒìƒ‰ ì „ëµ

### ìƒì„± ì „ëµì˜ ì™„ì „í•œ ì´í•´

```
ì¢‹ì€ í…ìŠ¤íŠ¸ ìƒì„± = ì ì ˆí•œ ìƒ˜í”Œë§ ì „ëµ + í’ˆì§ˆ í‰ê°€ + ë°˜ë³µ ê°œì„ 

í•µì‹¬ ì›ì¹™:
1. ìš©ë„ì— ë§ëŠ” íŒŒë¼ë¯¸í„° ì„ íƒ
2. ì°½ì˜ì„±ê³¼ ì¼ê´€ì„±ì˜ ê· í˜•
3. ë°˜ë³µê³¼ ì´ìƒí•œ í‘œí˜„ ë°©ì§€
4. ì§€ì†ì ì¸ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§
```

### ì‹¤ë¬´ ì ìš© ê°€ì´ë“œë¼ì¸

```python
# ìš©ë„ë³„ ì¶”ì²œ ì„¤ì •

ì°½ì‘ ê¸€ì“°ê¸°:
- Temperature: 0.9~1.1
- Top-k: 40~60  
- Top-p: 0.8~0.95

ê¸°ìˆ  ë¬¸ì„œ:
- Temperature: 0.3~0.5
- Top-k: 10~20
- Top-p: 0.7~0.8

ëŒ€í™”í˜• ì±—ë´‡:
- Temperature: 0.7~0.9
- Top-k: 30~50
- Top-p: 0.85~0.95

ì½”ë“œ ìƒì„±:
- Temperature: 0.2~0.4
- Top-k: 5~15
- Beam Search ê³ ë ¤
```

### ë‹¤ìŒ í¸ ì˜ˆê³ : ì‹¤ì „ êµ¬í˜„ì˜ ëª¨ë“  ê²ƒ

ë‹¤ìŒ í¸ì—ì„œëŠ” ì§€ê¸ˆê¹Œì§€ ë°°ìš´ ëª¨ë“  ì§€ì‹ì„ ì‹¤ì „ì— ì ìš©í•˜ëŠ” **ê³ ê¸‰ êµ¬í˜„ ê¸°ë²•ë“¤**ì„ ë‹¤ë£¹ë‹ˆë‹¤:

- **ë©”ëª¨ë¦¬ ìµœì í™”**: GPU ë©”ëª¨ë¦¬ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•
- **ì†ë„ ìµœì í™”**: ì¶”ë¡  ì†ë„ë¥¼ ê·¹ëŒ€í™”í•˜ëŠ” ê¸°ìˆ ë“¤
- **ë°°ì¹˜ ì²˜ë¦¬**: ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ ë™ì‹œì— íš¨ìœ¨ì ìœ¼ë¡œ ìƒì„±
- **ëª¨ë¸ ì••ì¶•**: ì„±ëŠ¥ ìœ ì§€í•˜ë©´ì„œ í¬ê¸° ì¤„ì´ê¸°
- **ì‹¤ì „ ë””ë²„ê¹…**: ë¬¸ì œ ìƒí™© ì§„ë‹¨ê³¼ í•´ê²°
- **ë°°í¬ ìµœì í™”**: ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ì—”ì§€ë‹ˆì–´ë§

**ë¯¸ë¦¬ ìƒê°í•´ë³¼ ì§ˆë¬¸:**
ì§€ê¸ˆê¹Œì§€ ë°°ìš´ ì•„ë¦„ë‹¤ìš´ ì´ë¡ ë“¤ì„ ì‹¤ì œ ì„œë¹„ìŠ¤ì— ì ìš©í•  ë•Œ ì–´ë–¤ ì‹¤ë¬´ì  ê³ ë ¤ì‚¬í•­ë“¤ì´ ìˆì„ê¹Œìš”? ì„±ëŠ¥ê³¼ í’ˆì§ˆ, ë¹„ìš©ì˜ ê· í˜•ì„ ì–´ë–»ê²Œ ë§ì¶œê¹Œìš”?

### ì‹¤ìŠµ ê³¼ì œ

ë‹¤ìŒ í¸ê¹Œì§€ í•´ë³¼ ê³¼ì œ:

1. **ìƒ˜í”Œë§ ì „ëµ ì‹¤í—˜**: ë‹¤ì–‘í•œ Temperatureì™€ Top-k ì¡°í•©ìœ¼ë¡œ ìµœì  ì„¤ì • ì°¾ê¸°
2. **í’ˆì§ˆ í‰ê°€ ë„êµ¬ ê°œë°œ**: ìƒì„±ëœ í…ìŠ¤íŠ¸ë¥¼ ìë™ìœ¼ë¡œ í‰ê°€í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
3. **ì°½ì˜ì  í”„ë¡¬í”„íŠ¸ ì‹¤í—˜**: í¥ë¯¸ë¡œìš´ ì‹œì‘ ë¬¸êµ¬ë“¤ë¡œ ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸

ì´ì œ GPTì˜ ëª¨ë“  ì´ë¡ ì„ ì™„ì „íˆ ë§ˆìŠ¤í„°í–ˆìœ¼ë‹ˆ, ì‹¤ì „ì—ì„œ í™œìš©í•˜ëŠ” ì—”ì§€ë‹ˆì–´ë§ ë…¸í•˜ìš°ë¥¼ ë°°ì›Œë´…ì‹œë‹¤! ğŸš€

---

**ì´ì „ í¸**: [6í¸: í•™ìŠµì˜ ê³¼í•™ - ëª¨ë¸ì´ "ë°°ìš°ëŠ”" ê³¼ì •](ë§í¬)  
**ë‹¤ìŒ í¸**: [8í¸: ì‹¤ì „ êµ¬í˜„ ë¶„ì„ - ì½”ë“œ í•œ ì¤„ì”© ì™„ì „ ë¶„í•´](ë§í¬)  
**ì‹œë¦¬ì¦ˆ ì „ì²´**: [GPT-2 êµ¬í˜„ìœ¼ë¡œ ë°°ìš°ëŠ” Transformer ì™„ì „ ì •ë³µ ì‹œë¦¬ì¦ˆ](ë§í¬)

