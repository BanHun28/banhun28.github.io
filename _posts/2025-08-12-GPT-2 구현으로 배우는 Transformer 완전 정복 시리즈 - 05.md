---
title: GPT-2 êµ¬í˜„ìœ¼ë¡œ ë°°ìš°ëŠ” Transformer ì™„ì „ ì •ë³µ ì‹œë¦¬ì¦ˆ - 05
date: 2025-08-12 08:10:24 +0900
categories: [machine learning, GPT]
tags: [machine learning, GPT, Transformer]       # TAG names should always be lowercase
---

# GPT ì™„ì „ ì •ë³µ 5í¸: GPT ëª¨ë¸ ì „ì²´ êµ¬ì¡° - í¼ì¦ì˜ ì™„ì„±

> **ì´ì „ í¸ ìš”ì•½**: 4í¸ì—ì„œëŠ” Transformer Blockì˜ ë‚´ë¶€ êµ¬ì¡°ë¥¼ ì™„ì „íˆ ì´í•´í–ˆìŠµë‹ˆë‹¤. ì´ì œ ëª¨ë“  í¼ì¦ ì¡°ê°ì„ ë§ì¶°ì„œ ì™„ì „í•œ GPT ëª¨ë¸ì˜ ì „ì²´ ê·¸ë¦¼ì„ ê·¸ë ¤ë³´ê² ìŠµë‹ˆë‹¤.

---

## ë“¤ì–´ê°€ë©°: êµí–¥ì•…ë‹¨ì˜ ì™„ì„±

ì§€ê¸ˆê¹Œì§€ ìš°ë¦¬ëŠ” ê°œë³„ ì•…ê¸°ë“¤ì„ ë°°ì› ìŠµë‹ˆë‹¤:

```
1í¸: GPTì˜ ì² í•™ (ì§€íœ˜ìì˜ ë¹„ì „)
2í¸: í† í°í™” (ì•…ë³´ë¥¼ ì½ëŠ” ë°©ë²•)  
3í¸: Attention (ì•…ê¸°ë“¤ ê°„ì˜ ì¡°í™”)
4í¸: Transformer Block (ê° ì•…ê¸°ì˜ ì—°ì£¼ë²•)
```

**ì´ì œ ëª¨ë“  ì•…ê¸°ê°€ í•¨ê»˜ ì—°ì£¼í•˜ëŠ” ì™„ì „í•œ êµí–¥ê³¡ì„ ê°ìƒí•  ì‹œê°„ì…ë‹ˆë‹¤.**

í•˜ë‚˜ì˜ GPT ëª¨ë¸ì´ ì–´ë–»ê²Œ "ROMEO:"ë¼ëŠ” ì…ë ¥ìœ¼ë¡œë¶€í„° ì…°ìµìŠ¤í”¼ì–´ ìŠ¤íƒ€ì¼ì˜ ì™„ì „í•œ ëŒ€ì‚¬ë¥¼ ë§Œë“¤ì–´ë‚´ëŠ”ì§€, ê·¸ ì „ì²´ ê³¼ì •ì„ ì²˜ìŒë¶€í„° ëê¹Œì§€ ì¶”ì í•´ë³´ê² ìŠµë‹ˆë‹¤.

---

## 1. GPT í´ë˜ìŠ¤ ì „ì²´ êµ¬ì¡°: ë§ˆìŠ¤í„° ì•„í‚¤í…íŠ¸

### ìš°ë¦¬ êµ¬í˜„ì˜ í•µì‹¬ ì½”ë“œ

[ë ˆí¬ì§€í† ë¦¬](https://github.com/BanHun28/gpt2_study)ì˜ `main.py`ì—ì„œ:

```python
class GPT(nn.Module):
    """
    GPT(Generative Pre-trained Transformer) ë©”ì¸ ëª¨ë¸ í´ë˜ìŠ¤
    ì „ì²´ ëª¨ë¸ì„ í†µí•©í•˜ì—¬ í…ìŠ¤íŠ¸ ìƒì„± ê¸°ëŠ¥ì„ ì œê³µ
    """
    def __init__(self, config):
        super().__init__()
        self.config = config  # ì„¤ì • ì €ì¥ (generate ë“±ì—ì„œ í•„ìš”)
        
        # ã€ModuleDictë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ ã€‘
        # - êµ¬ì„±ìš”ì†Œë“¤ì„ ì²´ê³„ì ìœ¼ë¡œ ê´€ë¦¬
        # - state_dict ì €ì¥/ë¡œë“œ ì‹œ ìë™ ì²˜ë¦¬  
        # - ë„¤ì„ìŠ¤í˜ì´ìŠ¤ êµ¬ë¶„ìœ¼ë¡œ ë””ë²„ê¹… ìš©ì´
        self.transformer = nn.ModuleDict(dict(
            
            # ã€ì…ë ¥ì¸µã€‘ í† í°ì„ ë²¡í„°ë¡œ ë³€í™˜
            wte=nn.Embedding(config.vocab_size, config.n_embd),
            
            # ã€ìœ„ì¹˜ì¸µã€‘ ìˆœì„œ ì •ë³´ ì¶”ê°€
            wpe=nn.Embedding(config.block_size, config.n_embd),
            
            # ã€ì²˜ë¦¬ì¸µã€‘ 12ê°œì˜ Transformer Block
            h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
            
            # ã€ì •ê·œí™”ì¸µã€‘ ìµœì¢… ì¶œë ¥ ì•ˆì •í™”
            ln_f=nn.LayerNorm(config.n_embd, eps=1e-5),
        ))
        
        # ã€ì¶œë ¥ì¸µã€‘ ë²¡í„°ë¥¼ í™•ë¥ ë¡œ ë³€í™˜
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        
        # ã€í•µì‹¬ í˜ì‹ ã€‘ ì…ì¶œë ¥ ê°€ì¤‘ì¹˜ ê³µìœ 
        self.transformer.wte.weight = self.lm_head.weight
        
        # ã€ì´ˆê¸°í™”ã€‘ í•™ìŠµ ì‹œì‘ì  ì„¤ì •
        self.apply(self._init_weights)
```

### ì „ì²´ ì•„í‚¤í…ì²˜ ì‹œê°í™”

```
                    GPT ëª¨ë¸ ì „ì²´ êµ¬ì¡°

ì…ë ¥: "ROMEO:" â†’ [15496, 11] (í† í° ID)
                        â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚              Token Embedding            â”‚
        â”‚   [15496] â†’ [0.1, 0.5, -0.2, ...]     â”‚
        â”‚   [11] â†’ [0.3, -0.1, 0.8, ...]        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚            Position Embedding           â”‚
        â”‚   ìœ„ì¹˜0 â†’ [0.1, 0.0, 0.2, ...]        â”‚
        â”‚   ìœ„ì¹˜1 â†’ [0.0, 0.1, -0.1, ...]       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“ (ë”í•˜ê¸°)
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚              Block 1                    â”‚
        â”‚     ln â†’ attention â†’ +x                 â”‚
        â”‚     ln â†’ mlp â†’ +x                       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚              Block 2                    â”‚
        â”‚     ...ë™ì¼í•œ êµ¬ì¡°...                   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
                     ... Ã— 12 ...
                        â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚            Final LayerNorm              â”‚
        â”‚        ì¶œë ¥ ë²¡í„° ì •ê·œí™”                  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚            Language Model Head          â”‚
        â”‚   ë²¡í„° â†’ 50257ê°œ ë‹¨ì–´ë³„ í™•ë¥              â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
ì¶œë ¥: [0.001, 0.234, 0.156, ...] â†’ "But" (ê°€ì¥ ë†’ì€ í™•ë¥ )
```

---

## 2. ë°ì´í„° í”Œë¡œìš°: í•œ í† í°ì˜ ì—¬í–‰

### "ROMEO:"ì—ì„œ "But"ê¹Œì§€ì˜ ì™„ì „í•œ ì—¬ì •

#### ë‹¨ê³„ 0: í† í°í™” (2í¸ì—ì„œ ë°°ìš´ ë‚´ìš©)

```python
# ì…ë ¥ í…ìŠ¤íŠ¸
text = "ROMEO:"

# tiktokenìœ¼ë¡œ í† í°í™”
import tiktoken
enc = tiktoken.get_encoding("gpt2")
tokens = enc.encode(text)  # [15496, 11]

print(f"'{text}' â†’ {tokens}")
# 'ROMEO:' â†’ [15496, 11]
```

#### ë‹¨ê³„ 1: ì„ë² ë”© ë³€í™˜

```python
def forward(self, idx, targets=None):
    device = idx.device
    b, t = idx.size()  # batch_size=1, seq_len=2
    
    # ìœ„ì¹˜ ì¸ë±ìŠ¤ ìƒì„±
    pos = torch.arange(0, t, dtype=torch.long, device=device)  # [0, 1]
    
    # í† í° ì„ë² ë”©: ID â†’ 768ì°¨ì› ë²¡í„°
    tok_emb = self.transformer.wte(idx)  # (1, 2, 768)
    # [15496] â†’ [0.12, -0.45, 0.33, ..., 0.67]  # 768ê°œ ìˆ«ì
    # [11]    â†’ [0.89, 0.23, -0.11, ..., -0.34] # 768ê°œ ìˆ«ì
    
    # ìœ„ì¹˜ ì„ë² ë”©: ìœ„ì¹˜ â†’ 768ì°¨ì› ë²¡í„°  
    pos_emb = self.transformer.wpe(pos)  # (2, 768)
    # ìœ„ì¹˜0 â†’ [0.05, 0.12, -0.03, ..., 0.21]
    # ìœ„ì¹˜1 â†’ [-0.08, 0.07, 0.15, ..., -0.09]
    
    # ë‘ ì„ë² ë”©ì„ ë”í•´ì„œ ìµœì¢… ì…ë ¥ ìƒì„±
    x = tok_emb + pos_emb  # (1, 2, 768)
```

#### ë‹¨ê³„ 2-13: 12ê°œ Block ìˆœì°¨ ì²˜ë¦¬

```python
# 12ê°œ Transformer Blockì„ ìˆœì°¨ì ìœ¼ë¡œ í†µê³¼
for block in self.transformer.h:
    x = block(x)  # ê° Blockì—ì„œ ê´€ê³„ íŒŒì•… + ì˜ë¯¸ ë³€í™˜
    
# Blockë³„ ì ì§„ì  ë³€í™” (ê°œë…ì  í‘œí˜„)
x_after_block_1  = x + attention_relations_basic + mlp_transform_basic
x_after_block_2  = x + attention_relations_syntax + mlp_transform_syntax  
x_after_block_3  = x + attention_relations_semantic + mlp_transform_semantic
# ...
x_after_block_12 = x + attention_relations_abstract + mlp_transform_abstract
```

#### ë‹¨ê³„ 14: ìµœì¢… ì •ê·œí™”

```python
# 12ê°œ Blockì„ ê±°ì¹œ í›„ ìµœì¢… Layer Normalization
x = self.transformer.ln_f(x)  # (1, 2, 768)

# íš¨ê³¼: ì¶œë ¥ì¸µ ì…ë ¥ ì•ˆì •í™”
# - 12ê°œ Blockì„ ê±°ì¹˜ë©´ì„œ ê°’ì˜ í¬ê¸°ê°€ ë³€í–ˆì„ ìˆ˜ ìˆìŒ
# - softmax ì „ì— ì ì ˆí•œ ìŠ¤ì¼€ì¼ë¡œ ì¡°ì •
```

#### ë‹¨ê³„ 15: í™•ë¥  ì˜ˆì¸¡

```python
if targets is not None:
    # í›ˆë ¨ ëª¨ë“œ: ëª¨ë“  ìœ„ì¹˜ì—ì„œ ì˜ˆì¸¡
    logits = self.lm_head(x)  # (1, 2, 50257)
else:
    # ì¶”ë¡  ëª¨ë“œ: ë§ˆì§€ë§‰ ìœ„ì¹˜ë§Œ ì˜ˆì¸¡ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
    logits = self.lm_head(x[:, [-1], :])  # (1, 1, 50257)

# logits: ê° ë‹¨ì–´ì— ëŒ€í•œ "ì ìˆ˜" (í™•ë¥  ì•„ë‹˜)
# ì˜ˆì‹œ: [1.2, 3.4, 0.8, 2.1, ...] (50257ê°œ)
```

#### ë‹¨ê³„ 16: ë‹¤ìŒ ë‹¨ì–´ ì„ íƒ

```python
# logitsë¥¼ í™•ë¥ ë¡œ ë³€í™˜
probs = F.softmax(logits, dim=-1)
# [0.001, 0.234, 0.003, 0.156, ...] (í•©ì´ 1)

# ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í† í° ì„ íƒ
next_token = torch.argmax(probs, dim=-1)  # ì˜ˆ: 284 ("But")

# ë˜ëŠ” í™•ë¥ ì  ìƒ˜í”Œë§
next_token = torch.multinomial(probs, num_samples=1)
```

---

## 3. Weight Sharing: ì…ì¶œë ¥ ê°€ì¤‘ì¹˜ ê³µìœ ì˜ í˜ì‹ 

### ì™œ ê°€ì¤‘ì¹˜ë¥¼ ê³µìœ í• ê¹Œ?

```python
# ã€í•µì‹¬ í˜ì‹ ã€‘ ì…ì¶œë ¥ ê°€ì¤‘ì¹˜ ê³µìœ 
self.transformer.wte.weight = self.lm_head.weight
```

#### 1. ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±

```python
# ê°€ì¤‘ì¹˜ ê³µìœ  ì—†ì´
wte_params = 50257 * 768 = 38,597,376  # ì•½ 38M íŒŒë¼ë¯¸í„°
lm_head_params = 768 * 50257 = 38,597,376  # ì•½ 38M íŒŒë¼ë¯¸í„°
total = 77,194,752  # ì•½ 77M íŒŒë¼ë¯¸í„°

# ê°€ì¤‘ì¹˜ ê³µìœ  í›„
shared_params = 50257 * 768 = 38,597,376  # ì•½ 38M íŒŒë¼ë¯¸í„°ë§Œ!
ì ˆì•½ëœ_ë©”ëª¨ë¦¬ = 38M * 4ë°”ì´íŠ¸ = 152MB  # float32 ê¸°ì¤€
```

#### 2. ì˜ë¯¸ì  ì¼ê´€ì„±

```python
# ì…ë ¥ ì„ë² ë”©: í† í° ID â†’ ì˜ë¯¸ ë²¡í„°
"cat" (ID: 7163) â†’ [0.2, -0.5, 1.3, 0.8, ...]

# ì¶œë ¥ ì˜ˆì¸¡: ì˜ë¯¸ ë²¡í„°ì™€ ê° í† í°ì˜ ìœ ì‚¬ë„
output_vector = [0.1, -0.3, 1.1, 0.9, ...]

# ê°€ì¤‘ì¹˜ ê³µìœ ë¡œ "cat" í† í°ê³¼ì˜ ë‚´ì  ê³„ì‚°
similarity = output_vector @ cat_embedding  # ë†’ì€ ê°’ â†’ "cat" ì„ íƒ í™•ë¥  ë†’ìŒ

# ì§ê´€: ì¶œë ¥ ë²¡í„°ê°€ "cat"ì˜ ì˜ë¯¸ì— ê°€ê¹Œìš°ë©´ "cat"ì„ ì˜ˆì¸¡
```

#### 3. í•™ìŠµ íš¨ìœ¨ì„±

```python
# ì…ì¶œë ¥ ëª¨ë‘ì—ì„œ ê°™ì€ ì„ë² ë”©ì´ ì—…ë°ì´íŠ¸ë¨
# â†’ ê°™ì€ ë‹¨ì–´ì— ëŒ€í•´ ë‘ ë²ˆì˜ í•™ìŠµ ì‹ í˜¸

ì…ë ¥ì—ì„œì˜ í•™ìŠµ: "cat"ì„ ë³´ê³  ì˜ë¯¸ ë²¡í„° í•™ìŠµ
ì¶œë ¥ì—ì„œì˜ í•™ìŠµ: "cat"ì„ ì˜ˆì¸¡í•˜ë©° ì˜ë¯¸ ë²¡í„° ì •ì œ

ê²°ê³¼: ë” ë¹ ë¥´ê³  ì¼ê´€ëœ í•™ìŠµ
```

### Weight Sharing êµ¬í˜„ ì„¸ë¶€ì‚¬í•­

```python
def __init__(self, config):
    # ... ë‹¤ë¥¸ ì´ˆê¸°í™” ...
    
    # ì„ë² ë”©ê³¼ ì¶œë ¥ì¸µ ìƒì„±
    self.transformer.wte = nn.Embedding(config.vocab_size, config.n_embd)
    self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
    
    # ã€í•µì‹¬ã€‘ ê°™ì€ ê°€ì¤‘ì¹˜ë¥¼ ì°¸ì¡°í•˜ë„ë¡ ì„¤ì •
    self.transformer.wte.weight = self.lm_head.weight
    
    # ì´ì œ ë‘˜ì€ ì™„ì „íˆ ê°™ì€ ë©”ëª¨ë¦¬ë¥¼ ê³µìœ 
    # wte.weightë¥¼ ì—…ë°ì´íŠ¸ â†’ lm_head.weightë„ ìë™ ì—…ë°ì´íŠ¸
    # lm_head.weightë¥¼ ì—…ë°ì´íŠ¸ â†’ wte.weightë„ ìë™ ì—…ë°ì´íŠ¸
```

---

## 4. ëª¨ë¸ í¬ê¸°ì™€ ì„±ëŠ¥: ìŠ¤ì¼€ì¼ë§ ë²•ì¹™

### GPT ëª¨ë¸ í¬ê¸° ë¹„êµ

```python
# ìš°ë¦¬ êµ¬í˜„ (GPT-2 Small)
our_config = GPTConfig(
    n_layer=12,    # 12ê°œ Block  
    n_head=12,     # 12ê°œ Attention Head
    n_embd=768,    # 768ì°¨ì› ì„ë² ë”©
)

# íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
def count_parameters(config):
    # Token + Position Embedding
    embedding_params = config.vocab_size * config.n_embd + config.block_size * config.n_embd
    
    # ê° Blockì˜ íŒŒë¼ë¯¸í„°
    # Attention: 3 * n_embd^2 (Q,K,V) + n_embd^2 (projection)
    # MLP: n_embd * 4*n_embd + 4*n_embd * n_embd  
    # LayerNorm: 2 * n_embd (ê° Blockì— 2ê°œ)
    block_params = (4 * config.n_embd**2 + 8 * config.n_embd**2 + 2 * config.n_embd) * config.n_layer
    
    # Final LayerNorm
    final_ln_params = config.n_embd
    
    total = embedding_params + block_params + final_ln_params
    return total

print(f"ìš°ë¦¬ ëª¨ë¸: {count_parameters(our_config)/1e6:.1f}M íŒŒë¼ë¯¸í„°")
```

#### ê³µì‹ GPT ëª¨ë¸ í¬ê¸°ë“¤

```python
# GPT-2 ëª¨ë¸ ê³„ì—´
configs = {
    "GPT-2 Small":  {"n_layer": 12, "n_head": 12, "n_embd": 768},   # 117M
    "GPT-2 Medium": {"n_layer": 24, "n_head": 16, "n_embd": 1024},  # 345M  
    "GPT-2 Large":  {"n_layer": 36, "n_head": 20, "n_embd": 1280},  # 774M
    "GPT-2 XL":     {"n_layer": 48, "n_head": 25, "n_embd": 1600},  # 1.5B
}

# GPT-3 ê³„ì—´ (ì¶”ì •ì¹˜)
gpt3_configs = {
    "GPT-3 Small":    {"n_layer": 12, "n_head": 12, "n_embd": 768},    # 125M
    "GPT-3 Medium":   {"n_layer": 24, "n_head": 16, "n_embd": 1024},   # 350M
    "GPT-3 Large":    {"n_layer": 24, "n_head": 16, "n_embd": 1536},   # 760M  
    "GPT-3 XL":       {"n_layer": 24, "n_head": 24, "n_embd": 2048},   # 1.3B
    "GPT-3":          {"n_layer": 96, "n_head": 96, "n_embd": 12288},  # 175B
}
```

### ìŠ¤ì¼€ì¼ë§ ë²•ì¹™ì˜ ë°œê²¬

#### 1. ë” í° ëª¨ë¸ = ë” ì¢‹ì€ ì„±ëŠ¥

```python
# ì—°êµ¬ ê²°ê³¼: íŒŒë¼ë¯¸í„° ìˆ˜ê°€ 10ë°° ì¦ê°€í•˜ë©´
# ì†ì‹¤(perplexity)ì´ ì¼ì • ë¹„ìœ¨ë¡œ ê°ì†Œ

ëª¨ë¸ í¬ê¸°     | ì†ì‹¤  | ì„±ëŠ¥
117M (Small)  | 3.2   | ê¸°ë³¸
345M (Medium) | 2.8   | í–¥ìƒ  
774M (Large)  | 2.5   | ë” í–¥ìƒ
1.5B (XL)     | 2.3   | í›¨ì”¬ í–¥ìƒ
175B (GPT-3)  | 1.8   | ë†€ë¼ìš´ ì„±ëŠ¥
```

#### 2. ì°½ë°œì  ëŠ¥ë ¥ (Emergent Abilities)

```python
# ëª¨ë¸ì´ ì»¤ì§€ë©´ì„œ ì˜ˆìƒì¹˜ ëª»í•œ ëŠ¥ë ¥ë“¤ì´ ë‚˜íƒ€ë‚¨

Small ëª¨ë¸ (117M):
- ê¸°ë³¸ì ì¸ í…ìŠ¤íŠ¸ ìƒì„±
- ë‹¨ìˆœí•œ íŒ¨í„´ í•™ìŠµ

Large ëª¨ë¸ (774M):  
- ë” ìì—°ìŠ¤ëŸ¬ìš´ í…ìŠ¤íŠ¸
- ê¸°ë³¸ì ì¸ ì¶”ë¡ 

XL ëª¨ë¸ (1.5B):
- ë¬¸ë§¥ ìœ ì§€ ëŠ¥ë ¥ í–¥ìƒ
- ê°„ë‹¨í•œ ì§ˆì˜ì‘ë‹µ

GPT-3 (175B):
- ë³µì¡í•œ ì¶”ë¡ 
- ì½”ë”© ëŠ¥ë ¥  
- ìˆ˜í•™ ë¬¸ì œ í•´ê²°
- ì°½ì‘ ëŠ¥ë ¥
```

---

## 5. ì „ì²´ ëª¨ë¸ì˜ í•™ìŠµê³¼ ì¶”ë¡ 

### í•™ìŠµ ëª¨ë“œ vs ì¶”ë¡  ëª¨ë“œ

#### í•™ìŠµ ëª¨ë“œ: ë³‘ë ¬ ì²˜ë¦¬

```python
# í›ˆë ¨ ì¤‘: ëª¨ë“  ìœ„ì¹˜ì—ì„œ ë™ì‹œì— ì˜ˆì¸¡
input_sequence = ["ë‚˜ëŠ”", "í•™êµì—", "ê°„ë‹¤"]
target_sequence = ["í•™êµì—", "ê°„ë‹¤", "<END>"]

# ëª¨ë“  ìœ„ì¹˜ì˜ ì˜ˆì¸¡ì„ í•œ ë²ˆì— ê³„ì‚°
logits = model(input_tokens)  # (batch, seq_len, vocab_size)

# ê° ìœ„ì¹˜ì—ì„œì˜ ì†ì‹¤ ê³„ì‚°
position_0: "ë‚˜ëŠ”" ë‹¤ìŒì— "í•™êµì—" ì˜ˆì¸¡í–ˆëŠ”ê°€?
position_1: "ë‚˜ëŠ” í•™êµì—" ë‹¤ìŒì— "ê°„ë‹¤" ì˜ˆì¸¡í–ˆëŠ”ê°€?  
position_2: "ë‚˜ëŠ” í•™êµì— ê°„ë‹¤" ë‹¤ìŒì— "<END>" ì˜ˆì¸¡í–ˆëŠ”ê°€?

# ëª¨ë“  ìœ„ì¹˜ì˜ ì†ì‹¤ì„ í‰ê· í•˜ì—¬ í•™ìŠµ
total_loss = mean(all_position_losses)
```

#### ì¶”ë¡  ëª¨ë“œ: ìˆœì°¨ì  ìƒì„±

```python
@torch.no_grad()
def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):
    """í…ìŠ¤íŠ¸ ìƒì„± ë©”ì„œë“œ"""
    for _ in range(max_new_tokens):
        # 1. ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
        idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]
        
        # 2. ë‹¤ìŒ í† í° ì˜ˆì¸¡
        logits, _ = self(idx_cond)
        logits = logits[:, -1, :] / temperature  # ë§ˆì§€ë§‰ ìœ„ì¹˜ë§Œ ì‚¬ìš©
        
        # 3. Top-k í•„í„°ë§
        if top_k is not None:
            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
            logits[logits < v[:, [-1]]] = -float('Inf')
        
        # 4. í™•ë¥ ì  ìƒ˜í”Œë§
        probs = F.softmax(logits, dim=-1)
        idx_next = torch.multinomial(probs, num_samples=1)
        
        # 5. ê¸°ì¡´ ì‹œí€€ìŠ¤ì— ì¶”ê°€
        idx = torch.cat((idx, idx_next), dim=1)
    
    return idx
```

### ìƒì„± ê³¼ì •ì˜ ë‹¨ê³„ë³„ ì¶”ì 

```python
# ì˜ˆì‹œ: "ROMEO:" â†’ "ROMEO: But soft, what light"

step_0: "ROMEO:" [15496, 11]
        â†“ model forward
        ì˜ˆì¸¡: "But" (í™•ë¥  23.4%)
        
step_1: "ROMEO: But" [15496, 11, 284]  
        â†“ model forward
        ì˜ˆì¸¡: "soft" (í™•ë¥  18.7%)
        
step_2: "ROMEO: But soft" [15496, 11, 284, 2705]
        â†“ model forward  
        ì˜ˆì¸¡: "," (í™•ë¥  31.2%)

step_3: "ROMEO: But soft," [15496, 11, 284, 2705, 11]
        â†“ model forward
        ì˜ˆì¸¡: "what" (í™•ë¥  27.8%)

# ì´ëŸ° ì‹ìœ¼ë¡œ ê³„ì† ì§„í–‰...
```

---

## 6. ëª¨ë¸ ì„±ëŠ¥ ìµœì í™” ê¸°ë²•ë“¤

### 1. Mixed Precision Training

```python
# main.pyì—ì„œ ì‚¬ìš©ë˜ëŠ” ìµœì í™”
scaler = torch.cuda.amp.GradScaler(enabled=(device=='cuda'))

# í›ˆë ¨ ë£¨í”„ì—ì„œ
with torch.cuda.amp.autocast(enabled=(device=='cuda')):
    logits, loss = model(xb, yb)  # ìˆœì „íŒŒëŠ” float16ìœ¼ë¡œ

scaler.scale(loss).backward()  # ì—­ì „íŒŒëŠ” ìŠ¤ì¼€ì¼ë§í•˜ì—¬ ì•ˆì •ì„± í™•ë³´
```

#### ë©”ëª¨ë¦¬ì™€ ì†ë„ í–¥ìƒ

```python
# Float32 vs Float16 ë¹„êµ
memory_float32 = model_params * 4  # bytes
memory_float16 = model_params * 2  # bytes  
memory_saving = 50%  # ë©”ëª¨ë¦¬ ì ˆì•½

speed_improvement = 1.5~2.0ë°°  # GPU ì¢…ë¥˜ì— ë”°ë¼
```

### 2. Gradient Clipping

```python
# ê·¸ë˜ë””ì–¸íŠ¸ í­ë°œ ë°©ì§€
torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)

# íš¨ê³¼: ê·¸ë˜ë””ì–¸íŠ¸ ë…¸ë¦„ì´ grad_clipì„ ë„˜ìœ¼ë©´ ìŠ¤ì¼€ì¼ ë‹¤ìš´
# ì•ˆì •ì ì¸ í•™ìŠµ ë³´ì¥
```

### 3. Learning Rate Scheduling

```python
# Warm-up + Decay ìŠ¤ì¼€ì¤„ë§
if iter_num < 100:  # ì²˜ìŒ 100 ì´í„°ì—ì„œëŠ” ì›Œë°ì—…
    lr = learning_rate * iter_num / 100
else:
    lr = learning_rate  # ì´í›„ ê³ ì • (ë˜ëŠ” decay ì ìš© ê°€ëŠ¥)
```

---

## 7. ì‹¤ì „ ë¶„ì„: ì „ì²´ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •

### ëª¨ë¸ í¬ê¸°ë³„ ì„±ëŠ¥ ë¹„êµ

```python
def benchmark_model_sizes():
    """ë‹¤ì–‘í•œ ëª¨ë¸ í¬ê¸°ì˜ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    
    configs = [
        {"name": "Tiny", "n_layer": 4, "n_head": 4, "n_embd": 128},
        {"name": "Small", "n_layer": 6, "n_head": 6, "n_embd": 384},  # ìš°ë¦¬ ì„¤ì •
        {"name": "Medium", "n_layer": 8, "n_head": 8, "n_embd": 512},
    ]
    
    results = []
    
    for config_dict in configs:
        config = GPTConfig(**{k: v for k, v in config_dict.items() if k != "name"})
        model = GPT(config)
        
        # íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
        total_params = sum(p.numel() for p in model.parameters())
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
        model_size_mb = total_params * 4 / (1024**2)  # float32 ê¸°ì¤€
        
        # ì¶”ë¡  ì†ë„ ì¸¡ì •
        test_input = torch.randint(0, 1000, (1, 100))
        
        import time
        start_time = time.time()
        with torch.no_grad():
            for _ in range(10):
                _ = model(test_input)
        inference_time = (time.time() - start_time) / 10
        
        results.append({
            "name": config_dict["name"],
            "params": f"{total_params/1e6:.1f}M",
            "size_mb": f"{model_size_mb:.1f}MB",
            "inference_ms": f"{inference_time*1000:.1f}ms"
        })
    
    # ê²°ê³¼ ì¶œë ¥
    print("ëª¨ë¸ í¬ê¸°ë³„ ì„±ëŠ¥ ë¹„êµ:")
    for result in results:
        print(f"{result['name']:6} | {result['params']:6} | {result['size_mb']:8} | {result['inference_ms']:8}")

benchmark_model_sizes()
```

### ìƒì„± í’ˆì§ˆ í‰ê°€

```python
def evaluate_generation_quality():
    """ìƒì„±ëœ í…ìŠ¤íŠ¸ì˜ í’ˆì§ˆ í‰ê°€"""
    
    model.eval()
    start_text = "ROMEO:"
    
    # ë‹¤ì–‘í•œ ìƒì„± ì„¤ì •ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
    settings = [
        {"temperature": 0.8, "top_k": 40, "name": "Creative"},
        {"temperature": 0.5, "top_k": 20, "name": "Balanced"},  
        {"temperature": 0.2, "top_k": 10, "name": "Conservative"},
    ]
    
    for setting in settings:
        print(f"\n=== {setting['name']} ì„¤ì • ===")
        
        tokens = enc.encode(start_text)
        input_ids = torch.tensor([tokens])
        
        with torch.no_grad():
            generated = model.generate(
                input_ids, 
                max_new_tokens=50,
                temperature=setting['temperature'],
                top_k=setting['top_k']
            )
        
        generated_text = enc.decode(generated[0].tolist())
        print(f"ìƒì„± ê²°ê³¼: {generated_text}")
        
        # ê°„ë‹¨í•œ í’ˆì§ˆ ì§€í‘œë“¤
        lines = generated_text.split('\n')
        avg_line_length = sum(len(line) for line in lines) / len(lines)
        unique_words = len(set(generated_text.lower().split()))
        
        print(f"í‰ê·  ì¤„ ê¸¸ì´: {avg_line_length:.1f}")
        print(f"ê³ ìœ  ë‹¨ì–´ ìˆ˜: {unique_words}")

evaluate_generation_quality()
```

---

## 8. ì‹¤ìŠµ: ì™„ì „í•œ GPT ëª¨ë¸ ì»¤ìŠ¤í„°ë§ˆì´ì§•

### ì‹¤ìŠµ 1: ì„¤ì • ë³€ê²½ ì‹¤í—˜

```python
# ë‹¤ì–‘í•œ ì„¤ì •ìœ¼ë¡œ ëª¨ë¸ ì‹¤í—˜
experimental_configs = [
    # ì›ë³¸ ì„¤ì •
    {"name": "Original", "n_layer": 6, "n_head": 6, "n_embd": 384},
    
    # ë” ê¹Šê³  ì¢ì€ ëª¨ë¸
    {"name": "Deep_Narrow", "n_layer": 12, "n_head": 4, "n_embd": 256},
    
    # ë” ì–•ê³  ë„“ì€ ëª¨ë¸  
    {"name": "Shallow_Wide", "n_layer": 3, "n_head": 8, "n_embd": 512},
    
    # í—¤ë“œ ìˆ˜ ë³€ê²½
    {"name": "More_Heads", "n_layer": 6, "n_head": 12, "n_embd": 384},
]

def compare_configurations():
    for config_dict in experimental_configs:
        print(f"\n=== {config_dict['name']} ëª¨ë¸ ===")
        
        # ì„¤ì • ì ìš©
        config = GPTConfig(**{k: v for k, v in config_dict.items() if k != "name"})
        model = GPT(config)
        
        # íŒŒë¼ë¯¸í„° ìˆ˜ ì¶œë ¥
        total_params = sum(p.numel() for p in model.parameters())
        print(f"íŒŒë¼ë¯¸í„° ìˆ˜: {total_params/1e6:.2f}M")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        memory_mb = total_params * 4 / (1024**2)
        print(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_mb:.1f}MB")
        
        # í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸
        test_input = torch.tensor([[15496, 11]])  # "ROMEO:"
        with torch.no_grad():
            output = model.generate(test_input, max_new_tokens=10)
        
        generated_text = enc.decode(output[0].tolist())
        print(f"ìƒì„± ì˜ˆì‹œ: {generated_text[:50]}...")

compare_configurations()
```

### ì‹¤ìŠµ 2: ì»¤ìŠ¤í…€ ìƒì„± í•¨ìˆ˜

```python
def advanced_generate(model, prompt, max_length=100, **kwargs):
    """ê³ ê¸‰ í…ìŠ¤íŠ¸ ìƒì„± í•¨ìˆ˜"""
    
    # ê¸°ë³¸ ì„¤ì •
    settings = {
        'temperature': 0.8,
        'top_k': 40,
        'top_p': 0.9,  # nucleus sampling
        'repetition_penalty': 1.1,
    }
    settings.update(kwargs)
    
    model.eval()
    tokens = enc.encode(prompt)
    input_ids = torch.tensor([tokens])
    
    generated_tokens = tokens.copy()
    
    with torch.no_grad():
        for _ in range(max_length):
            # í˜„ì¬ ì‹œí€€ìŠ¤ë¡œ ì˜ˆì¸¡
            current_input = torch.tensor([generated_tokens[-model.config.block_size:]])
            logits, _ = model(current_input)
            logits = logits[0, -1, :]  # ë§ˆì§€ë§‰ ìœ„ì¹˜
            
            # Repetition penalty ì ìš©
            for token in set(generated_tokens):
                logits[token] /= settings['repetition_penalty']
            
            # Temperature ì ìš©
            logits = logits / settings['temperature']
            
            # Top-k í•„í„°ë§
            if settings['top_k'] > 0:
                indices_to_remove = logits < torch.topk(logits, settings['top_k'])[0][..., -1, None]
                logits[indices_to_remove] = -float('Inf')
            
            # Top-p í•„í„°ë§ (nucleus sampling)
            if settings['top_p'] < 1.0:
                sorted_logits, sorted_indices = torch.sort(logits, descending=True)
                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
                
                sorted_indices_to_remove = cumulative_probs > settings['top_p']
                sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()
                sorted_indices_to_remove[0] = 0
                
                indices_to_remove = sorted_indices[sorted_indices_to_remove]
                logits[indices_to_remove] = -float('Inf')
            
            # ìƒ˜í”Œë§
            probs = F.softmax(logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1).item()
            
            generated_tokens.append(next_token)
            
            # ì¢…ë£Œ ì¡°ê±´ í™•ì¸
            if next_token == enc.encode('\n')[0]:  # ì¤„ë°”ê¿ˆìœ¼ë¡œ ì¢…ë£Œ
                break
    
    return enc.decode(generated_tokens)

# ì‚¬ìš© ì˜ˆì‹œ
result = advanced_generate(
    model, 
    "ROMEO:", 
    max_length=50,
    temperature=0.7,
    top_k=30,
    top_p=0.8,
    repetition_penalty=1.2
)
print("ê³ ê¸‰ ìƒì„± ê²°ê³¼:", result)
```

### ì‹¤ìŠµ 3: ëª¨ë¸ ë¶„ì„ ë„êµ¬

```python
def analyze_model_behavior():
    """ëª¨ë¸ì˜ ë‚´ë¶€ ë™ì‘ ë¶„ì„"""
    
    # 1. Attention íŒ¨í„´ ì‹œê°í™”
    def get_attention_patterns(text):
        tokens = enc.encode(text)
        input_ids = torch.tensor([tokens])
        
        # Hookìœ¼ë¡œ attention weights ìˆ˜ì§‘
        attention_weights = []
        
        def hook_fn(module, input, output):
            if hasattr(output, 'attn_weights'):
                attention_weights.append(output.attn_weights.detach())
        
        # ëª¨ë“  attention ë ˆì´ì–´ì— hook ë“±ë¡
        hooks = []
        for block in model.transformer.h:
            hook = block.attn.register_forward_hook(hook_fn)
            hooks.append(hook)
        
        with torch.no_grad():
            _ = model(input_ids)
        
        # Hook ì œê±°
        for hook in hooks:
            hook.remove()
        
        return attention_weights, [enc.decode([token]) for token in tokens]
    
    # 2. ì„ë² ë”© ìœ ì‚¬ë„ ë¶„ì„
    def embedding_similarity(word1, word2):
        token1 = enc.encode(word1)[0]
        token2 = enc.encode(word2)[0]
        
        emb1 = model.transformer.wte.weight[token1]
        emb2 = model.transformer.wte.weight[token2]
        
        similarity = F.cosine_similarity(emb1, emb2, dim=0)
        return similarity.item()
    
    # í…ŒìŠ¤íŠ¸
    print("=== ì„ë² ë”© ìœ ì‚¬ë„ ë¶„ì„ ===")
    word_pairs = [("king", "queen"), ("love", "hate"), ("Romeo", "Juliet")]
    
    for word1, word2 in word_pairs:
        sim = embedding_similarity(word1, word2)
        print(f"{word1} - {word2}: {sim:.3f}")
    
    # 3. ë ˆì´ì–´ë³„ í‘œí˜„ ë³€í™”
    def layer_representations(text):
        tokens = enc.encode(text)
        input_ids = torch.tensor([tokens])
        
        representations = []
        
        def hook_fn(module, input, output):
            representations.append(output.detach())
        
        # ê° ë ˆì´ì–´ ì¶œë ¥ì— hook
        hooks = []
        for i, block in enumerate(model.transformer.h):
            hook = block.register_forward_hook(hook_fn)
            hooks.append(hook)
        
        with torch.no_grad():
            _ = model(input_ids)
        
        for hook in hooks:
            hook.remove()
        
        return representations
    
    print("\n=== ë ˆì´ì–´ë³„ í‘œí˜„ ë³€í™” ===")
    reps = layer_representations("Romeo loves Juliet")
    
    for i, rep in enumerate(reps[:3]):  # ì²˜ìŒ 3ê°œ ë ˆì´ì–´ë§Œ
        mean_norm = rep.norm(dim=-1).mean().item()
        print(f"ë ˆì´ì–´ {i+1}: í‰ê·  ë…¸ë¦„ = {mean_norm:.3f}")

analyze_model_behavior()
```

---

## ë§ˆë¬´ë¦¬: í¼ì¦ì˜ ì™„ì„±

### ì „ì²´ ì—¬ì • ìš”ì•½

ì§€ê¸ˆê¹Œì§€ ìš°ë¦¬ëŠ” ì™„ì „í•œ GPT ëª¨ë¸ì˜ ëª¨ë“  êµ¬ì„±ìš”ì†Œë¥¼ ë§ˆìŠ¤í„°í–ˆìŠµë‹ˆë‹¤:

```
1í¸: GPTì˜ í˜ì‹ ì  ì•„ì´ë””ì–´ â†’ "ë‹¤ìŒ ë‹¨ì–´ ë§íˆê¸°"ì˜ ì² í•™
2í¸: í† í°í™”ì™€ ì„ë² ë”© â†’ í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ë³€í™˜í•˜ëŠ” ê¸°ìˆ   
3í¸: Attention ë©”ì»¤ë‹ˆì¦˜ â†’ ë‹¨ì–´ë“¤ì´ ì†Œí†µí•˜ëŠ” ë°©ë²•
4í¸: Transformer Block â†’ ì§€ëŠ¥ì˜ ê¸°ë³¸ êµ¬ì¡°
5í¸: ì „ì²´ ëª¨ë¸ í†µí•© â†’ ëª¨ë“  í¼ì¦ ì¡°ê°ì˜ ì™„ì„±
```

### GPTì˜ ì „ì²´ ë™ì‘ ì›ë¦¬

```
ì…ë ¥ "ROMEO:" 
    â†“ í† í°í™”
[15496, 11]
    â†“ ì„ë² ë”© (ë‹¨ì–´ + ìœ„ì¹˜)
768ì°¨ì› ë²¡í„°ë“¤
    â†“ 12ê°œ Transformer Block 
ì ì§„ì  ì´í•´ ì‹¬í™” (ë¬¸ë²• â†’ ì˜ë¯¸ â†’ ì¶”ë¡ )
    â†“ ì¶œë ¥ì¸µ
50257ê°œ ë‹¨ì–´ë³„ í™•ë¥ 
    â†“ ìƒ˜í”Œë§
"But" (ë‹¤ìŒ ë‹¨ì–´)
    â†“ ë°˜ë³µ
"ROMEO: But soft, what light through yonder window breaks?"
```

### ìŠ¤ì¼€ì¼ë§ì˜ ë§ˆë²•

```
ì‘ì€ ëª¨ë¸ (ìš°ë¦¬ êµ¬í˜„): ê¸°ë³¸ì ì¸ íŒ¨í„´ í•™ìŠµ
ì¤‘ê°„ ëª¨ë¸: ë” ìì—°ìŠ¤ëŸ¬ìš´ í…ìŠ¤íŠ¸  
í° ëª¨ë¸ (GPT-3): ì¶”ë¡ , ì½”ë”©, ì°½ì‘ê¹Œì§€

í•µì‹¬: ê°™ì€ êµ¬ì¡°, ë‹¤ë¥¸ í¬ê¸° â†’ ì§ˆì ìœ¼ë¡œ ë‹¤ë¥¸ ëŠ¥ë ¥
```

### ë‹¤ìŒ í¸ ì˜ˆê³ : í•™ìŠµì˜ ê³¼í•™

ë‹¤ìŒ í¸ì—ì„œëŠ” ì´ ëª¨ë“  êµ¬ì¡°ê°€ ì–´ë–»ê²Œ **í•™ìŠµì„ í†µí•´ ì§€ëŠ¥ì„ íšë“**í•˜ëŠ”ì§€ ë°°ì›ë‹ˆë‹¤:

- **Cross-Entropy Loss**: ì˜ˆì¸¡ ì˜¤ì°¨ë¥¼ ì¸¡ì •í•˜ëŠ” ë°©ë²•
- **Backpropagation**: ì˜¤ì°¨ë¥¼ ì—­ì‚°í•˜ì—¬ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸  
- **AdamW Optimizer**: íš¨ìœ¨ì ì¸ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜
- **Learning Rate Scheduling**: ìµœì ì˜ í•™ìŠµ ì†ë„ ì¡°ì ˆ
- **ì •ê·œí™” ê¸°ë²•ë“¤**: ê³¼ì í•© ë°©ì§€ì™€ ì¼ë°˜í™”

**ë¯¸ë¦¬ ìƒê°í•´ë³¼ ì§ˆë¬¸:**
"ROMEO:"ì—ì„œ "But"ì„ ì˜ëª» ì˜ˆì¸¡í–ˆì„ ë•Œ, ëª¨ë¸ì€ ì–´ë–»ê²Œ ìì‹ ì˜ ì‹¤ìˆ˜ë¥¼ ê¹¨ë‹«ê³  ë‹¤ìŒì—ëŠ” ë” ë‚˜ì€ ì˜ˆì¸¡ì„ í•  ìˆ˜ ìˆì„ê¹Œìš”?

### ì‹¤ìŠµ ê³¼ì œ

ë‹¤ìŒ í¸ê¹Œì§€ í•´ë³¼ ê³¼ì œ:

1. **ëª¨ë¸ í¬ê¸° ì‹¤í—˜**: ë‹¤ì–‘í•œ ì„¤ì •ìœ¼ë¡œ ëª¨ë¸ í¬ê¸°ì™€ ì„±ëŠ¥ ê´€ê³„ í™•ì¸
2. **ìƒì„± í’ˆì§ˆ ë¹„êµ**: temperature, top_k ë“± ì„¤ì • ë³€ê²½ íš¨ê³¼ ê´€ì°°
3. **ì „ì²´ ì½”ë“œ ì‹¤í–‰**: `python main.py`ë¡œ ì™„ì „í•œ í•™ìŠµ-ìƒì„± ê³¼ì • ì²´í—˜

ì´ì œ GPTì˜ ì „ì²´ êµ¬ì¡°ë¥¼ ì™„ì „íˆ ì´í•´í–ˆìœ¼ë‹ˆ, ì–´ë–»ê²Œ ì´ ëª¨ë“  ê²ƒì´ "í•™ìŠµ"ì„ í†µí•´ ì§€ëŠ¥ì„ ì–»ëŠ”ì§€ ì•Œì•„ë´…ì‹œë‹¤! ğŸ§ 

---

**ì´ì „ í¸**: [4í¸: Transformer Block í•´ë¶€í•™ - ì§€ëŠ¥ì˜ êµ¬ì¡°](https://github.com/BanHun28/gpt2_study)  
**ë‹¤ìŒ í¸**: [6í¸: í•™ìŠµì˜ ê³¼í•™ - ëª¨ë¸ì´ "ë°°ìš°ëŠ”" ê³¼ì •](https://github.com/BanHun28/gpt2_study)  
**ì‹œë¦¬ì¦ˆ ì „ì²´**: [GPT-2 êµ¬í˜„ìœ¼ë¡œ ë°°ìš°ëŠ” Transformer ì™„ì „ ì •ë³µ ì‹œë¦¬ì¦ˆ](https://github.com/BanHun28/gpt2_study)  

