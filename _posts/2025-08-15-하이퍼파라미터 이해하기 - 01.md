---
title: í•˜ì´í¼íŒŒë¼ë¯¸í„° ì´í•´í•˜ê¸° - 01
date: 2025-08-15 10:57:50 +0900
categories: [artificial intelligence, machine learning]
tags: [machine learning, deep learning, hyperparameters, finetuning, llm, llamafactory, pytorch, optimization, training, warmup, ai]     # TAG names should always be lowercase
---

# Warmup ì´í•´í•˜ê¸°. 

## ğŸ“š ê°œë… ì´í•´

### Warmup?

**Warmup**ì€ í›ˆë ¨ ì´ˆê¸°ì— í•™ìŠµë¥ ì„ ì ì§„ì ìœ¼ë¡œ ì¦ê°€ì‹œí‚¤ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤. ì²˜ìŒì—ëŠ” ë§¤ìš° ì‘ì€ í•™ìŠµë¥ ë¡œ ì‹œì‘í•´ì„œ ì§€ì •ëœ ìŠ¤í… ìˆ˜ì— ê±¸ì³ ëª©í‘œ í•™ìŠµë¥ ê¹Œì§€ ì„œì„œíˆ ì˜¬ë ¤ê°‘ë‹ˆë‹¤.

```python
# Warmup ê³¼ì • ì˜ˆì‹œ
ì´ˆê¸° í•™ìŠµë¥ : 0 (ë˜ëŠ” ë§¤ìš° ì‘ì€ ê°’)
ëª©í‘œ í•™ìŠµë¥ : 5e-5
warmup_steps: 100

Step 1:   í•™ìŠµë¥  = 5e-7  (ëª©í‘œì˜ 1%)
Step 50:  í•™ìŠµë¥  = 2.5e-5 (ëª©í‘œì˜ 50%)  
Step 100: í•™ìŠµë¥  = 5e-5   (ëª©í‘œì˜ 100%)
Step 101+: ìŠ¤ì¼€ì¤„ëŸ¬ì— ë”°ë¼ ê°ì†Œ/ìœ ì§€
```

## ğŸ¯ Warmupì´ í•„ìš”í•œ ì´ìœ 

### 1. ê·¸ë˜ë””ì–¸íŠ¸ í­ë°œ ë°©ì§€

**ë¬¸ì œìƒí™©**:
```python
# í›ˆë ¨ ì‹œì‘ ì‹œ í° í•™ìŠµë¥ ë¡œ ì¸í•œ ë¬¸ì œ
ì´ˆê¸° ê°€ì¤‘ì¹˜: ë¬´ì‘ìœ„ ì´ˆê¸°í™” ìƒíƒœ
í° í•™ìŠµë¥ : 5e-4
ê²°ê³¼: ê·¸ë˜ë””ì–¸íŠ¸ê°€ í­ë°œí•˜ì—¬ lossê°€ ë°œì‚°
```

**Warmup í•´ê²°ì±…**:
```python
# ì ì§„ì  í•™ìŠµë¥  ì¦ê°€ë¡œ ì•ˆì •í™”
Step 1-100: ì‘ì€ í•™ìŠµë¥ ë¡œ ì•ˆì •ì  ì´ˆê¸° í•™ìŠµ
Step 101+: ë³¸ê²©ì ì¸ í•™ìŠµë¥ ë¡œ íš¨ìœ¨ì  í•™ìŠµ
```

### 2. Adam ì˜µí‹°ë§ˆì´ì €ì˜ í¸í–¥ ë¬¸ì œ

Adam ì˜µí‹°ë§ˆì´ì €ëŠ” ì´ˆê¸°ì— ëª¨ë©˜í…€ ì¶”ì •ì¹˜ê°€ 0ì— í¸í–¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤:

```python
# Adamì˜ ë‚´ë¶€ ìƒíƒœ
m_t = 0  # 1ì°¨ ëª¨ë©˜í…€ (ì´ˆê¸°ê°’)
v_t = 0  # 2ì°¨ ëª¨ë©˜í…€ (ì´ˆê¸°ê°’)

# í¸í–¥ ë³´ì •ì´ í•„ìš”í•œ ì´ˆê¸° ë‹¨ê³„
m_hat = m_t / (1 - beta1^t)  # í¸í–¥ ë³´ì •
v_hat = v_t / (1 - beta2^t)  # í¸í–¥ ë³´ì •
```

Warmupì€ ì´ í¸í–¥ì´ ìì—°ìŠ¤ëŸ½ê²Œ í•´ê²°ë  ì‹œê°„ì„ ì œê³µí•©ë‹ˆë‹¤.

### 3. ì‚¬ì „í›ˆë ¨ëœ ëª¨ë¸ì˜ ì•ˆì •ì„±

íŒŒì¸íŠœë‹ ì‹œ ì‚¬ì „í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ë¥¼ ê¸‰ê²©íˆ ë³€í™”ì‹œí‚¤ì§€ ì•Šë„ë¡ í•©ë‹ˆë‹¤:

```python
# ì‚¬ì „í›ˆë ¨ëœ ëª¨ë¸ì˜ íŠ¹ì„±
- ì´ë¯¸ ì¢‹ì€ í‘œí˜„ì„ í•™ìŠµí•œ ìƒíƒœ
- ê¸‰ê²©í•œ ë³€í™”ëŠ” ê¸°ì¡´ ì§€ì‹ì„ íŒŒê´´í•  ìˆ˜ ìˆìŒ
- ì ì§„ì  ì ì‘ì´ í•„ìš”
```

## âš™ï¸ LLaMA Factoryì—ì„œì˜ warmup_steps ì„¤ì •

### ê¸°ë³¸ ì„¤ì •
```yaml
warmup_steps: 100  # ê¸°ë³¸ê°’: 0 (warmup ì—†ìŒ)
```

### ë‹¤ì–‘í•œ ì„¤ì • ë°©ë²•

#### 1. ì ˆëŒ€ê°’ìœ¼ë¡œ ì„¤ì •
```yaml
warmup_steps: 500  # ì •í™•íˆ 500 ìŠ¤í… ë™ì•ˆ warmup
```

#### 2. ë¹„ìœ¨ë¡œ ì„¤ì • (ì¼ë¶€ í”„ë ˆì„ì›Œí¬)
```yaml
warmup_ratio: 0.1  # ì „ì²´ í›ˆë ¨ ìŠ¤í…ì˜ 10%
```

### ì‹¤ì œ ê³„ì‚° ì˜ˆì‹œ

```python
# ì„¤ì • ì˜ˆì‹œ
total_samples = 10000
per_device_batch_size = 8
gradient_accumulation_steps = 4
num_epochs = 3
num_gpus = 2

# ê³„ì‚°
effective_batch_size = per_device_batch_size * gradient_accumulation_steps * num_gpus
# = 8 * 4 * 2 = 64

steps_per_epoch = total_samples / effective_batch_size
# = 10000 / 64 = 156.25 â‰ˆ 156

total_steps = steps_per_epoch * num_epochs
# = 156 * 3 = 468

# Warmup ì„¤ì • ê¶Œì¥ì‚¬í•­
warmup_steps = total_steps * 0.1  # ì „ì²´ì˜ 10%
# = 468 * 0.1 = 46.8 â‰ˆ 47
```

## ğŸ“Š ì ì ˆí•œ warmup_steps ê°’ ì„ íƒ

### ë°ì´í„°ì…‹ í¬ê¸°ë³„ ê¶Œì¥ê°’

#### ì†Œê·œëª¨ ë°ì´í„°ì…‹ (< 1,000 ìƒ˜í”Œ)
```yaml
warmup_steps: 10-50
# ì´ìœ : ì „ì²´ í›ˆë ¨ì´ ì§§ì•„ì„œ ê¸´ warmupì€ ë¹„íš¨ìœ¨ì 
```

#### ì¤‘ê°„ ê·œëª¨ ë°ì´í„°ì…‹ (1,000-10,000 ìƒ˜í”Œ)
```yaml
warmup_steps: 50-200
# ì´ìœ : ì ë‹¹í•œ ì•ˆì •í™” ê¸°ê°„ í•„ìš”
```

#### ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ (> 10,000 ìƒ˜í”Œ)
```yaml
warmup_steps: 200-1000
# ì´ìœ : ì¶©ë¶„í•œ ì•ˆì •í™”ë¡œ ì „ì²´ ì„±ëŠ¥ í–¥ìƒ
```

### ëª¨ë¸ í¬ê¸°ë³„ ê¶Œì¥ê°’

#### ì‘ì€ ëª¨ë¸ (< 1B íŒŒë¼ë¯¸í„°)
```yaml
warmup_steps: 50-100
learning_rate: 1e-4
# ì‘ì€ ëª¨ë¸ì€ ìƒëŒ€ì ìœ¼ë¡œ ì•ˆì •ì 
```

#### ì¤‘ê°„ ëª¨ë¸ (1B-10B íŒŒë¼ë¯¸í„°)  
```yaml
warmup_steps: 100-500
learning_rate: 5e-5
# ê· í˜•ì¡íŒ ì ‘ê·¼
```

#### í° ëª¨ë¸ (> 10B íŒŒë¼ë¯¸í„°)
```yaml
warmup_steps: 500-1000
learning_rate: 1e-5
# í° ëª¨ë¸ì¼ìˆ˜ë¡ ì‹ ì¤‘í•œ warmup í•„ìš”
```

### íŒŒì¸íŠœë‹ ë°©ë²•ë³„ ê¶Œì¥ê°’

#### LoRA íŒŒì¸íŠœë‹
```yaml
warmup_steps: 50-200
learning_rate: 1e-4
# LoRAëŠ” ìƒëŒ€ì ìœ¼ë¡œ ì•ˆì •ì ì´ë¼ ì§§ì€ warmup
```

#### Full Parameter íŒŒì¸íŠœë‹
```yaml
warmup_steps: 200-1000
learning_rate: 5e-6
# ì „ì²´ ê°€ì¤‘ì¹˜ ë³€ê²½ìœ¼ë¡œ ê¸´ warmup í•„ìš”
```

#### QLoRA íŒŒì¸íŠœë‹
```yaml
warmup_steps: 100-300
learning_rate: 2e-4
# ì–‘ìí™”ë¡œ ì¸í•œ ë¶ˆì•ˆì •ì„± ê³ ë ¤
```

## ğŸ”¬ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ì™€ì˜ ìƒí˜¸ì‘ìš©

### Linear Scheduler + Warmup
```python
# ìŠ¤ì¼€ì¤„ ì˜ˆì‹œ (1000 total steps, 100 warmup steps)
Step 1-100:   0 â†’ target_lr (linear increase)
Step 101-1000: target_lr â†’ 0 (linear decrease)
```

```yaml
warmup_steps: 100
lr_scheduler_type: linear
learning_rate: 5e-5
num_train_epochs: 3
```

### Cosine Scheduler + Warmup
```python
# ìŠ¤ì¼€ì¤„ ì˜ˆì‹œ
Step 1-100:   0 â†’ target_lr (linear warmup)
Step 101-1000: target_lr â†’ min_lr (cosine decay)
```

```yaml
warmup_steps: 100
lr_scheduler_type: cosine
learning_rate: 5e-5
num_train_epochs: 3
```

### Constant Scheduler + Warmup
```python
# ìŠ¤ì¼€ì¤„ ì˜ˆì‹œ
Step 1-100:   0 â†’ target_lr (linear warmup)
Step 101-1000: target_lr (constant)
```

```yaml
warmup_steps: 100
lr_scheduler_type: constant
learning_rate: 5e-5
```

## ğŸ“ˆ ì‹¤ë¬´ ì ìš© ì‚¬ë¡€

### ì‚¬ë¡€ 1: ê³ ê° ì§€ì› ì±—ë´‡ ê°œë°œ

```yaml
# í™˜ê²½: í˜¸ìŠ¤íŒ… ì—…ê³„ ê³ ê° ë¬¸ì˜ ë°ì´í„°
# ë°ì´í„°: 5,000ê°œ ê³ ê° ë¬¸ì˜-ì‘ë‹µ ìŒ
# ëª©í‘œ: ì•ˆì •ì ì´ê³  ì¼ê´€ëœ ì‘ë‹µ ìƒì„±

model_name_or_path: Qwen/Qwen2.5-7B-Instruct
template: qwen
stage: sft
finetuning_type: lora

# ë°ì´í„°ì…‹
dataset: customer_support_ko
cutoff_len: 1024
max_samples: 5000

# í•™ìŠµ ì„¤ì •
learning_rate: 5e-5
num_train_epochs: 3
per_device_train_batch_size: 4
gradient_accumulation_steps: 8

# Warmup ì„¤ì • (ì‹ ì¤‘í•œ ì ‘ê·¼)
warmup_steps: 200
lr_scheduler_type: cosine

# ê³„ì‚°:
# total_steps â‰ˆ 5000/(4*8) * 3 = 468 steps
# warmup_steps = 200 (ì „ì²´ì˜ ~43%)
# ì´ìœ : ê³ ê° ì‘ë‹µì˜ ì¼ê´€ì„±ì´ ì¤‘ìš”í•˜ë¯€ë¡œ ì¶©ë¶„í•œ warmup
```

### ì‚¬ë¡€ 2: ê¸°ìˆ  ë¬¸ì„œ ìš”ì•½ ëª¨ë¸

```yaml
# í™˜ê²½: ì„œë²„ ê´€ë¦¬ ë¬¸ì„œ ìš”ì•½
# ë°ì´í„°: 50,000ê°œ ê¸°ìˆ  ë¬¸ì„œ
# ëª©í‘œ: ì •í™•í•˜ê³  ê°„ê²°í•œ ìš”ì•½

model_name_or_path: meta-llama/Meta-Llama-3-8B-Instruct
template: llama3
stage: sft
finetuning_type: lora

# ë°ì´í„°ì…‹  
dataset: tech_docs_summary
cutoff_len: 2048
max_samples: 50000

# í•™ìŠµ ì„¤ì •
learning_rate: 3e-5
num_train_epochs: 2
per_device_train_batch_size: 8
gradient_accumulation_steps: 4

# Warmup ì„¤ì • (í‘œì¤€ ì ‘ê·¼)
warmup_steps: 300
lr_scheduler_type: cosine

# ê³„ì‚°:
# total_steps â‰ˆ 50000/(8*4) * 2 = 3125 steps  
# warmup_steps = 300 (ì „ì²´ì˜ ~10%)
# ì´ìœ : ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ìœ¼ë¡œ í‘œì¤€ì ì¸ warmup ë¹„ìœ¨
```

### ì‚¬ë¡€ 3: ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘

```yaml
# í™˜ê²½: ë¹ ë¥¸ ê°œë… ê²€ì¦
# ë°ì´í„°: 1,000ê°œ ìƒ˜í”Œ
# ëª©í‘œ: ìµœëŒ€í•œ ë¹ ë¥¸ ê²°ê³¼ í™•ì¸

model_name_or_path: microsoft/Phi-3-mini-4k-instruct
template: phi
stage: sft
finetuning_type: qlora
quantization_bit: 4

# ë°ì´í„°ì…‹
dataset: alpaca_en_demo
cutoff_len: 512
max_samples: 1000

# í•™ìŠµ ì„¤ì •
learning_rate: 1e-4
num_train_epochs: 1
per_device_train_batch_size: 2
gradient_accumulation_steps: 8

# Warmup ì„¤ì • (ìµœì†Œí•œ)
warmup_steps: 20
lr_scheduler_type: linear

# ê³„ì‚°:
# total_steps â‰ˆ 1000/(2*8) * 1 = 62.5 steps
# warmup_steps = 20 (ì „ì²´ì˜ ~32%)
# ì´ìœ : ì§§ì€ í›ˆë ¨ì´ì§€ë§Œ ìµœì†Œí•œì˜ ì•ˆì •ì„± í™•ë³´
```

## ğŸš¨ ì¼ë°˜ì ì¸ ì‹¤ìˆ˜ì™€ í•´ê²°ë°©ë²•

### ì‹¤ìˆ˜ 1: Warmupì´ ë„ˆë¬´ ê¸¸ ë•Œ

**ë¬¸ì œ**:
```yaml
total_steps: 100
warmup_steps: 80  # ì „ì²´ì˜ 80%!
```

**ì¦ìƒ**:
- ì‹¤ì œ í•™ìŠµ ì‹œê°„ì´ ë„ˆë¬´ ì§§ìŒ
- ëª©í‘œ í•™ìŠµë¥ ì— ë„ë‹¬í•˜ìë§ˆì í›ˆë ¨ ì¢…ë£Œ
- ì„±ëŠ¥ í–¥ìƒì´ ë¯¸ë¯¸í•¨

**í•´ê²°**:
```yaml
total_steps: 100
warmup_steps: 10  # ì „ì²´ì˜ 10%
```

### ì‹¤ìˆ˜ 2: Warmupì´ ë„ˆë¬´ ì§§ì„ ë•Œ

**ë¬¸ì œ**:
```yaml
learning_rate: 1e-3  # ë§¤ìš° í° í•™ìŠµë¥ 
warmup_steps: 5      # ë§¤ìš° ì§§ì€ warmup
```

**ì¦ìƒ**:
- ì´ˆê¸° lossê°€ ë°œì‚°í•˜ê±°ë‚˜ ë¶ˆì•ˆì •
- NaN ê°’ ë°œìƒ
- í›ˆë ¨ì´ ì‹¤íŒ¨

**í•´ê²°**:
```yaml
learning_rate: 1e-4  # í•™ìŠµë¥  ê°ì†Œ
warmup_steps: 50     # warmup ì¦ê°€
```

### ì‹¤ìˆ˜ 3: í° ëª¨ë¸ì— ë¶€ì ì ˆí•œ ì„¤ì •

**ë¬¸ì œ**:
```yaml
model_name_or_path: meta-llama/Meta-Llama-3-70B-Instruct
finetuning_type: full
learning_rate: 1e-4  # ë„ˆë¬´ í° í•™ìŠµë¥ 
warmup_steps: 10     # ë„ˆë¬´ ì§§ì€ warmup
```

**í•´ê²°**:
```yaml
model_name_or_path: meta-llama/Meta-Llama-3-70B-Instruct
finetuning_type: lora  # ë” ì•ˆì „í•œ ë°©ë²•
learning_rate: 1e-5   # ë” ì‘ì€ í•™ìŠµë¥ 
warmup_steps: 500     # ì¶©ë¶„í•œ warmup
```

## ğŸ”§ ë””ë²„ê¹… ë° ëª¨ë‹ˆí„°ë§

### Loss ê·¸ë˜í”„ ë¶„ì„

#### ì •ìƒì ì¸ Warmup íŒ¨í„´
```python
# Loss ë³€í™” íŒ¨í„´
Step 1-100 (warmup):
  - Lossê°€ ë¹ ë¥´ê²Œ ê°ì†Œ (ì‘ì€ í•™ìŠµë¥ ì´ì§€ë§Œ íš¨ê³¼ì )
  - ì•ˆì •ì ì¸ ê°ì†Œ ê³¡ì„ 
  
Step 101+ (main training):
  - ê³„ì†í•´ì„œ ì•ˆì •ì  ê°ì†Œ
  - ê°€ë” ì‘ì€ fluctuationì€ ì •ìƒ
```

#### ë¬¸ì œê°€ ìˆëŠ” íŒ¨í„´ë“¤

**Warmupì´ ë¶€ì¡±í•œ ê²½ìš°**:
```python
Step 1-10: Lossê°€ ê¸‰ê²©íˆ ë³€ë™í•˜ê±°ë‚˜ ì¦ê°€
Step 11+: ë¶ˆì•ˆì •í•œ í•™ìŠµ íŒ¨í„´
```

**Warmupì´ ê³¼ë„í•œ ê²½ìš°**:
```python
Step 1-500: ë§¤ìš° ëŠë¦° Loss ê°ì†Œ
Step 501+: ì§§ì€ êµ¬ê°„ì—ì„œ ê¸‰ê²©í•œ ë³€í™” ì‹œë„
```

### ë¡œê¹…ì„ í†µí•œ ëª¨ë‹ˆí„°ë§

```yaml
# ìƒì„¸ ëª¨ë‹ˆí„°ë§ ì„¤ì •
logging_steps: 10  # ìì£¼ ë¡œê¹…
save_steps: 100
eval_steps: 100

# ì‹œê°í™” ë„êµ¬ ì‚¬ìš©
report_to: tensorboard
use_swanlab: true
swanlab_project: warmup_analysis
```

### í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ í™•ì¸ ì½”ë“œ

```python
# í•™ìŠµë¥  ë³€í™” ì‹œê°í™” (ë””ë²„ê¹…ìš©)
import matplotlib.pyplot as plt
import numpy as np

def plot_lr_schedule(total_steps, warmup_steps, target_lr, scheduler_type="cosine"):
    steps = np.arange(1, total_steps + 1)
    lrs = []
    
    for step in steps:
        if step <= warmup_steps:
            # Warmup phase
            lr = target_lr * (step / warmup_steps)
        else:
            # Main training phase
            if scheduler_type == "cosine":
                progress = (step - warmup_steps) / (total_steps - warmup_steps)
                lr = target_lr * 0.5 * (1 + np.cos(np.pi * progress))
            elif scheduler_type == "linear":
                lr = target_lr * (total_steps - step) / (total_steps - warmup_steps)
            else:  # constant
                lr = target_lr
        
        lrs.append(lr)
    
    plt.figure(figsize=(10, 6))
    plt.plot(steps, lrs)
    plt.axvline(x=warmup_steps, color='red', linestyle='--', label='Warmup End')
    plt.xlabel('Training Steps')
    plt.ylabel('Learning Rate')
    plt.title(f'Learning Rate Schedule (warmup_steps={warmup_steps})')
    plt.legend()
    plt.grid(True)
    plt.show()

# ì‚¬ìš© ì˜ˆì‹œ
plot_lr_schedule(1000, 100, 5e-5, "cosine")
```

## ğŸ’¡ ê³ ê¸‰ í™œìš© íŒ

### 1. ë™ì  Warmup ì¡°ì •

ëŒ€ìš©ëŸ‰ ëª¨ë¸ì´ë‚˜ ë¶ˆì•ˆì •í•œ ë°ì´í„°ì…‹ì˜ ê²½ìš°:

```yaml
# 1ë‹¨ê³„: ê¸´ warmupìœ¼ë¡œ ì‹œì‘
warmup_steps: 1000
learning_rate: 1e-5
num_train_epochs: 1

# 2ë‹¨ê³„: ì•ˆì •í™” í™•ì¸ í›„ ì§§ì€ warmupìœ¼ë¡œ ê³„ì†
warmup_steps: 100  
learning_rate: 5e-5
num_train_epochs: 2
```

### 2. ë‹¤ì¤‘ ë‹¨ê³„ Warmup

```yaml
# ë§¤ìš° í° ëª¨ë¸ì˜ ê²½ìš° ì ì§„ì  ì ‘ê·¼
# Phase 1: ë§¤ìš° ë³´ìˆ˜ì 
warmup_steps: 500
learning_rate: 1e-6

# Phase 2: ì¤‘ê°„ ì •ë„
warmup_steps: 200
learning_rate: 5e-6

# Phase 3: ëª©í‘œ í•™ìŠµë¥ 
warmup_steps: 100
learning_rate: 1e-5
```

### 3. ë„ë©”ì¸ë³„ ìµœì í™”

#### ì½”ë“œ ìƒì„± ëª¨ë¸
```yaml
# ì½”ë“œì˜ ì •í™•ì„±ì´ ì¤‘ìš” â†’ ì‹ ì¤‘í•œ warmup
warmup_steps: 300
learning_rate: 3e-5
lr_scheduler_type: cosine
```

#### ì°½ì‘ ëª¨ë¸
```yaml
# ì°½ì˜ì„± ì¤‘ì‹œ â†’ ìƒëŒ€ì ìœ¼ë¡œ ì§§ì€ warmup
warmup_steps: 100
learning_rate: 5e-5
lr_scheduler_type: constant
```

## ğŸ“‹ ìµœì¢… ê¶Œì¥ì‚¬í•­

### ì¼ë°˜ì ì¸ ê°€ì´ë“œë¼ì¸

1. **ê¸°ë³¸ê°’**: ì „ì²´ í›ˆë ¨ ìŠ¤í…ì˜ 10%
2. **ìµœì†Œê°’**: 50 steps
3. **ìµœëŒ€ê°’**: ì „ì²´ ìŠ¤í…ì˜ 20%

### ìƒí™©ë³„ ì¶”ì²œ

#### ì´ˆì‹¬ììš© ì•ˆì „ ì„¤ì •
```yaml
warmup_steps: 200
learning_rate: 3e-5
lr_scheduler_type: cosine
```

#### ì‹¤í—˜ì  ì„¤ì •
```yaml
warmup_steps: 50
learning_rate: 1e-4
lr_scheduler_type: linear
```

#### í”„ë¡œë•ì…˜ ì•ˆì •ì„± ì¤‘ì‹œ
```yaml
warmup_steps: 500
learning_rate: 1e-5
lr_scheduler_type: cosine
```

Warmup stepsëŠ” ëª¨ë¸ í›ˆë ¨ì˜ ì•ˆì •ì„±ê³¼ ì„±ëŠ¥ì— í° ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ì¤‘ìš”í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤. ì ì ˆí•œ ì„¤ì •ì„ í†µí•´ ë” ì•ˆì •ì ì´ê³  íš¨ê³¼ì ì¸ íŒŒì¸íŠœë‹ì„ ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

