---
title: AI가 팩맨을 정복하다- Deep Convolutional Q-Network로 게임 마스터하기
date: 2025-08-06 10:55:29 +0900
categories: [ai, deep learning]
tags: [ai, deep learning, DCQN]     # TAG names should always be lowercase
---

# 🎮 AI가 팩맨을 정복하다: Deep Convolutional Q-Network로 게임 마스터하기

> "게임 화면만 보고 스스로 팩맨을 배우는 AI, 과연 얼마나 똑똑할까요?"

## 들어가며: 인간처럼 게임을 배우는 AI

2013년 DeepMind가 발표한 DQN(Deep Q-Network)은 AI 역사의 한 획을 그었습니다. 화면의 픽셀만 보고 아타리 게임을 인간 수준으로 플레이하는 AI였죠. 오늘은 그 기술의 핵심인 **Deep Convolutional Q-Network(DCQN)**를 직접 구현해서 팩맨 게임을 정복하는 과정을 함께 살펴보겠습니다.

## 🧠 강화학습의 핵심: Q-Learning의 진화

### Q-Learning이란?

강화학습의 기본 아이디어는 간단합니다:
- **상태(State)**: 현재 게임 화면
- **행동(Action)**: 상하좌우 이동
- **보상(Reward)**: 점수 획득 또는 게임 오버
- **정책(Policy)**: 어떤 상황에서 어떤 행동을 할지 결정하는 전략

Q-Learning은 각 상태에서 각 행동의 "가치"를 학습합니다. 이 가치를 Q-값이라고 부르죠.

```
Q(상태, 행동) = 현재 보상 + 할인된 미래 보상의 기댓값
```

### Deep Learning과의 만남

하지만 팩맨 게임처럼 복잡한 환경에서는 무수히 많은 상태가 존재합니다. 84x84 픽셀의 그레이스케일 이미지만 해도 256^(84×84) 가지 경우의 수가 있죠. 

여기서 **Deep Learning**이 등장합니다. 신경망이 게임 화면(픽셀)을 입력받아 각 행동의 Q-값을 예측하는 것이죠.

## 🏗️ DCQN 아키텍처: CNN이 게임을 이해하는 방법

### 입력 처리: 게임 화면을 AI 친화적으로

원본 게임 화면을 AI가 효율적으로 처리할 수 있도록 전처리 과정을 거칩니다:

1. **그레이스케일 변환**: 컬러 정보 제거로 연산량 감소
2. **크기 조정**: 210x160 → 84x84로 리사이징
3. **정규화**: 0-255 → 0-1 범위로 정규화
4. **프레임 스택**: 최근 4프레임을 쌓아서 움직임 정보 제공

### CNN 아키텍처: 시각적 특징 추출

프로젝트에서 사용하는 4개의 컨볼루션 레이어는 다음과 같은 역할을 합니다:

- **첫 번째 층**: 기본적인 엣지와 코너 감지 (미로의 벽)
- **두 번째 층**: 더 복잡한 패턴 인식 (경로와 교차점)
- **세 번째 층**: 게임 객체 인식 (팩맨, 유령, 점)
- **네 번째 층**: 고수준 게임 상황 이해

## 🎯 핵심 기법들: 안정적인 학습을 위한 노하우

### 1. Experience Replay: 과거에서 배우기

인간이 실수를 돌이켜보며 배우듯, AI도 과거 경험을 반복 학습합니다. 프로젝트에서는 **리플레이 버퍼 크기 10,000**을 사용하여 충분한 경험의 다양성을 확보합니다.

**왜 중요한가?**
- **상관관계 제거**: 연속된 프레임들의 유사성으로 인한 과적합 방지
- **효율적 학습**: 중요한 경험들을 여러 번 학습
- **안정성**: 급격한 정책 변화 방지

### 2. Target Network: 움직이는 목표 문제 해결

Q-Learning의 목표값이 같은 네트워크로 계산되면 "움직이는 목표"를 쫓는 문제가 발생합니다. 이를 해결하기 위해 별도의 타겟 네트워크를 사용하여 학습 안정성을 확보합니다.

### 3. Epsilon-Greedy: 탐험과 활용의 균형

학습 초기에는 높은 확률로 랜덤 행동을 취하고, 점차 학습된 정책을 활용하는 비율을 늘려갑니다. 이를 통해 새로운 전략 발견과 기존 전략 활용의 균형을 맞춥니다.

## 📊 학습 설정: 최적의 하이퍼파라미터

프로젝트에서 사용하는 핵심 설정값들:

```
학습률: 5e-4          # 안정적인 학습을 위한 적절한 값
배치 크기: 64         # 리소스 효율성과 학습 안정성의 균형
할인 계수: 0.99       # 장기 보상을 충분히 고려
리플레이 버퍼: 10,000 # 경험 다양성 확보
에피소드: 2000        # 충분한 학습 기회 제공
```

이러한 설정은 팩맨 게임에 특화되어 최적화되었으며, **목표 평균 점수 500점 이상**을 달성하기 위해 조정되었습니다.

## 🚀 실행해보기: 당신도 AI 트레이너가 되어보세요

### 1. 환경 설정

프로젝트는 사용자 편의를 위해 자동 설정 스크립트를 제공합니다:

```bash
# 1. 프로젝트 클론
git clone https://github.com/BanHun28/dcqn_study.git
cd dcqn_study

# 2. 자동 설정 스크립트 실행 (권장)
chmod +x setup_and_run.sh
./setup_and_run.sh
```

이 스크립트는 다음 작업을 자동으로 수행합니다:
- Python 가상환경 생성
- 필요한 패키지 설치 (requirements.txt 기반)
- 환경 테스트
- 학습 실행 옵션 제공

### 2. 수동 설정 (선택사항)

```bash
# 가상환경 생성
python3 -m venv pacman_dcqn_env

# 가상환경 활성화
source pacman_dcqn_env/bin/activate  # macOS/Linux
# pacman_dcqn_env\Scripts\activate   # Windows

# 패키지 설치
pip install -r requirements.txt

# 메인 코드 실행
python deep_convolutional_q_learning_for_pac_man_complete_code.py
```

### 3. 시스템 요구사항

- **Python**: 3.8 이상
- **운영체제**: macOS, Linux, Windows
- **메모리**: 최소 4GB RAM
- **GPU**: 선택사항 (CUDA 지원 시 학습 속도 향상)

### 4. 결과 확인

학습이 완료되면 다음 파일들이 생성됩니다:
- **checkpoint.pth**: 학습된 모델 가중치
- **video.mp4**: 학습된 에이전트의 게임플레이 영상

## 📈 학습 과정 분석: AI는 어떻게 발전하는가?

### 학습 진행 모니터링

학습 중에는 콘솔에서 실시간으로 다음 정보를 확인할 수 있습니다:
- 현재 에피소드 번호
- 최근 100 에피소드 평균 점수
- 목표 달성 여부 (평균 500점 이상)

### 단계별 학습 패턴

**1단계 (초기 학습)**: 기본 이동 학습
- 벽 충돌 감소
- 기본적인 점 수집 행동

**2단계 (중기 학습)**: 전략적 움직임
- 유령 회피 기초 습득
- 체계적인 점 수집 패턴

**3단계 (고급 학습)**: 최적화된 플레이
- 파워펠릿 활용
- 효율적인 경로 계획
- 목표 점수 500점 달성

## 🎮 실제 성능: 무엇을 기대할 수 있을까?

### 학습된 AI의 특징

성공적으로 학습된 에이전트는 다음과 같은 능력을 보여줍니다:

1. **효율적인 탐색**: 미로를 체계적으로 탐색하여 모든 점을 수집
2. **유령 회피**: 위험한 상황에서 적절한 회피 기동
3. **파워펠릿 활용**: 전략적 타이밍으로 파워펠릿 사용
4. **경로 최적화**: 불필요한 움직임 최소화

### 한계점

현재 구현에서 관찰되는 제한사항들:
- 복잡한 상황에서의 의사결정 지연
- 가끔 발생하는 비효율적인 경로 선택
- 장기적 전략보다는 즉각적 보상에 치중

## 💡 고급 기법으로의 확장

### Double DQN
과대평가 문제를 해결하여 더 안정적인 학습이 가능합니다.

### Dueling Network
가치 함수와 어드밴티지 함수를 분리하여 학습 효율성을 높일 수 있습니다.

### Prioritized Experience Replay
중요한 경험을 우선적으로 학습하여 샘플 효율성을 개선할 수 있습니다.

## 🌟 실무 적용 가능성

### 게임 AI 개발
- NPC 행동 패턴 설계
- 동적 난이도 조절 시스템
- 플레이어 맞춤형 게임 경험

### 로봇 제어
- 자율 내비게이션
- 장애물 회피
- 경로 계획

### 자율 시스템
- 드론 제어
- 자율주행 보조 시스템
- 스마트 팩토리 자동화

## 🎯 마무리: AI 학습의 여정

이 프로젝트는 복잡해 보이는 강화학습을 **실용적이고 접근 가능한 형태**로 구현한 훌륭한 예시입니다. 단순한 게임 환경에서 시작하지만, 여기서 배운 원리들은 훨씬 복잡한 실제 문제에도 적용할 수 있습니다.

### 핵심 학습 포인트

1. **단순한 원리, 강력한 결과**: Q-Learning + CNN = 놀라운 성능
2. **경험의 중요성**: Experience Replay를 통한 효율적 학습
3. **안정성의 가치**: Target Network와 적절한 하이퍼파라미터
4. **인내의 필요성**: 강화학습은 시간이 걸리지만 확실한 성과

### 다음 도전 과제

- **다른 아타리 게임**: Breakout, Space Invaders 등으로 확장
- **고급 DQN 기법**: Rainbow DQN, A3C, PPO 등 최신 알고리즘
- **실제 문제 적용**: 로봇 제어, 자율주행 등 실무 프로젝트

### 리소스

**프로젝트**: [GitHub - BanHun28/dcqn_study](https://github.com/BanHun28/dcqn_study)

**참고 논문**:
- [Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602) - 원조 DQN
- [Human-level control through deep reinforcement learning](https://www.nature.com/articles/nature14236) - Nature 논문

---

*"가장 복잡한 AI도 결국 간단한 원리들의 조합입니다. 중요한 것은 첫 걸음을 떼는 것이죠."*

이제 당신도 AI가 스스로 게임을 배우는 마법을 직접 경험해보세요. 코드를 실행하고, 하이퍼파라미터를 조정하고, AI의 학습 과정을 관찰해보세요. 강화학습의 세계가 여러분을 기다리고 있습니다! 🚀
